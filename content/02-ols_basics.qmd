---
title: "OLS Basics"
format: 
  html:
    fig-width: 10
execute:
  message: false
  warning: false
bibliography: references.bib
---

## Slides

```{=html}
<iframe class="slide-deck" src="../content/slides/02-ols_basics.html" width = "100%" height = 600px></iframe>
```

## Packages

```{r}
library(tidyverse)
library(broom)
library(ggdist)
library(poliscidata)
library(modelsummary)
```

```{r}
set.seed(1234)
```

## Regression models

This week we are going to continue to look at how we can model the distribution of our outcome of interest using a single predictor.

Here's the general formula for a linear regression model with a single predictor:

$$
Y = \beta_0 + \beta_1X_1 + \epsilon
$$

Let's work backwards to help build our intuition for this model. Let's start with a modeled relationship between some outcome, $Y$, and a predictor, $X_1$:

$$
Y = 10 + 20X_1 + \epsilon
$$

Think of this as the *true relationship* between $Y$ and $X_1$.

::: callout-note
Out there in the real world we use modeling to attempt to find this *true relationship*. If only we had access to it!
:::

### Random error

This equation has two unknown variables: $X_1$ and $\epsilon$. You need both to be able to calculate the corresponding values for $Y$.

$X_1$ will in many ways be more intuitive for you. Generally, it represents some **vector** of values that representing something tangible: perhaps it's a person's age, a country's GDP, or a binary value telling you whether or not a country is a democracy. The error term can be a bit more abstract, so let's take a minute to look at it more closely.

::: callout-note
A quick note on notation! Generally, we will use capital letters (for example, $Y$ and $X_1$) to denote vectors and lower case letters (for example, $y$ and $x_1$) to denote single values (usually with another subscript telling us the observation to which that single value belongs).

What is a vector? Here, I mean a series of values. In other words, **it is a fancy term for some variable.** The term comes from matrix algebra. A matrix with only one row or column is referred to as a vector. Think of it simply as one column (variable) or one row (unit of observation) of a dataframe.

For example, if our outcome of interest is whether or not an individual will vote in the next election, we might use data on whether or not survey respondents voted in the last election to study the predictors of voting. In this case, our vector $Y$ will be a series of `1`s and `0`s describing whether or not each survey respondent voted in the last election.
:::

The error term; on the other hand, is a set of random values. It captures all of the random things that inevitably muddy the relationship between our outcomes of interest and our predictors in the real world.

We can learn some useful things about the random error effecting our outcome of interest. For example, let's say that the error in this true relationship is normally distributed with a mean of zero and a standard deviation of 50. In other words, there is some random noise in our data: $Y$ does not always precisely equal $10 + 20X_1$. Some random value will always be added to $10 + 20X_1$. That value will be drawn randomly from this distribution:

```{r}
ggplot(tibble(error = rnorm(1e6, mean = 0, sd = 50)), aes(x = error)) + 
  geom_density(fill = "#A2E3C4") + 
  theme_minimal()
```

The coloured area under that curve represents the probability that the corresponding error value (along the $x$-axis) will be drawn. This means that each error value randomly drawn is most likely to be a number around zero (where the curve peaks). It is very unlikely to be a number bigger than around 150 or smaller than around -150.

Notably, this error value is **completely independent of** $X_1$ **and** $Y$ (you don't need these values to work out your error value). To demonstrate, let's draw some of those random error values from this distribution:

```{r}
rnorm(1, mean = 0, sd = 50)
```

```{r}
rnorm(1, mean = 0, sd = 50)
```

```{r}
rnorm(1, mean = 0, sd = 50)
```

::: callout-note
This semester we will talk a lot about the assumptions you are required to make when you fit a linear regression model. These include some assumptions about error, including its shape and uniformity.
:::

## Fitting our regression model

Let's make $X_1$ equal all the whole numbers between one and 100. We can take this vector and our random error and find 100 values of $Y$:

```{r}
x_1 <- 1:100
error <- rnorm(100, mean = 0, sd = 50)

df <- tibble(x_1 = x_1,
             y = 10 + 20*x_1 + error)

df
```

Let's plot those:

```{r}
ggplot(df, aes(x = x_1, y = y)) + 
  geom_point() + 
  theme_minimal()
```

What is the line-of-best-fit? In other words, what is the line that minimizes the distance between itself and all of these points?

```{r}
m <- lm(y ~ x_1, data = df)

summary(m)
```

Hmm, it's different from the true relationship. It's this:

$$
Y = 8.09 + 20.08X_1 + \epsilon
$$

Instead of this:

$$
Y = 10 + 20X_1 + \epsilon
$$

This is despite the fact that we set everything up according to the true relationship between $Y$ and $X_1$!

Why is it different? The long and short answer is: that pesky random error!

Random error will always exist. That's okay. That's why we built it into our true relationship. We have some statistical tools to help us deal out.

Looking back at our model's output we can see that we are provided with a lot of information about its uncertainty in those coefficient estimates.

```{r}
tidy(m)
```

In addition to our coefficient estimates (`estimate`), we also have access to each estimate's standard error (`std.error`), t-statistic (`statistic`), and p-value (`p.value`). Let's focus on the standard error. We can use this to work out all of the plausible alternative coefficient estimates that our model could have found if that random error had been different. Let's take a look at those plausible alternatives to see whether the true relationship ($\beta_0 = 10$ and $\beta_1 = 20$) is sitting in there.

Let's start with the intercept. Centering our plausible set of intercept estimates on our model's best guess - `r tidy(m) |> filter(term == "(Intercept)") |> pull(estimate) |> round(2)` - let's simulate those alternative estimates using our uncertainty (the intercept coefficient's standard error) and see where the "true" intercept estimate (represented by the black line on the graph below) sits within this context:

```{r}
intercept_est <- tidy(m) |> 
  filter(term == "(Intercept)") |> 
  pull(estimate)

intercept_est

intercept_se <- tidy(m) |> 
  filter(term == "(Intercept)") |> 
  pull(std.error)

intercept_se
```

```{r}
tibble(intercept = rnorm(1e6, mean = intercept_est, sd = intercept_se)) |> 
  ggplot(aes(x = intercept)) + 
  stat_halfeye(.width = c(0.025, 0.975), fill = "#A2E3C4") +
  geom_vline(xintercept = 10) + 
  theme_minimal()
```

::: callout-note
I expect these plausible alternative coefficients to be normally distributed. For a simulated proof of why we can assume this, please check out the [Regression notes from GVPT622](https://hgoers.github.io/GVPT622/content/11-regression.html#uncertainty-around-this-relationship).
:::

We can do the same for our model's best guess of $\beta_1$:

```{r}
beta_1_est <- tidy(m) |> 
  filter(term == "x_1") |> 
  pull(estimate)

beta_1_est

beta_1_se <- tidy(m) |> 
  filter(term == "x_1") |> 
  pull(std.error)

beta_1_se
```

```{r}
tibble(beta_1 = rnorm(1e6, mean = beta_1_est, sd = beta_1_se)) |> 
  ggplot(aes(x = beta_1)) + 
  stat_halfeye(.width = c(0.025, 0.975), fill = "#3095A2") +
  geom_vline(xintercept = 20) + 
  theme_minimal()
```

Often in political science we are required to accept that the true estimates sit somewhere with 95 percent of these alternatives. Happily for us, the true values for the intercept (10) and $\beta_1$ (20) sit within this range (shown by the horizontal black lines on the graphs).

## Statistical significance

Traditionally, we are required to accept that 95 percent of all alternative coefficient estimates are plausible. Sadly for us, zero is included within this range of plausible values for the intercept coefficient. This coefficient would not; therefore, be considered to be statistically significant at the 95 percent confidence level.

We can learn that more quickly using the intercept coefficient's p-value (which is less than 0.05, or the 5 percent risk we are willing to take on that we believe a null relationship):

```{r}
tidy(m)
```

::: callout-note
The p-value tells us how likely we would be to observe the coefficient estimate that we did if it were actually equal to zero. It is calculated using the coefficient estimate's t-statistic (`statistic`), which is a transformation of the coefficient estimate into its place along a standard distribution: the t-distribution.
:::

What if we had more certainty around our estimates? In other words, what if there was less error? Let's reduce the spread of our random error by reducing its standard deviation from 50 to 5:

```{r}
error_smaller <- rnorm(100, mean = 0, sd = 5)

df_smaller <- tibble(x_1 = x_1,
                     y = 10 + 20*x_1 + error_smaller)

df_smaller
```

Let's plot that:

```{r}
ggplot(df_smaller, aes(x = x_1, y = y)) + 
  geom_point() + 
  theme_minimal()
```

Those data points are much more tightly placed around the line-of-best-fit. Let's find that line:

```{r}
m_smaller <- lm(y ~ x_1, data = df_smaller)

summary(m_smaller)
```

Or:

$$
Y = 9.2587 + 20.0112X_1 + \epsilon
$$

That's closer to the true relationship! But - of course - there is still uncertainty in those estimates. Let's look at the plausible range of intercept estimates:

```{r}
intercept_est_smaller <- tidy(m_smaller) |> 
  filter(term == "(Intercept)") |> 
  pull(estimate)

intercept_est_smaller

intercept_se_smaller <- tidy(m_smaller) |> 
  filter(term == "(Intercept)") |> 
  pull(std.error)

intercept_se_smaller
```

```{r}
tibble(intercept = rnorm(1e6, mean = intercept_est_smaller, sd = intercept_se_smaller)) |> 
  ggplot(aes(x = intercept)) + 
  stat_halfeye(.width = c(0.025, 0.975), fill = "#A2E3C4") +
  geom_vline(xintercept = 10) + 
  theme_minimal()
```

No zero in sight! The range of values included within this plausible range is much smaller and it still includes the true intercept (10). By reducing our uncertainty, we gained more confident insight into where the true intercept coefficient lies.

::: callout-tip
This is an important point to think about: we lost the zero because the true intercept is different from zero. Sometimes you will theorize that changes to a predictor are associated with changes to your outcome of interest. Sometimes the truth is that there is no meaningful relationship between the two. We can learn a lot from strong null results. You should always aim to reduce your uncertainty, even if it leads to a stronger zero.
:::

## Interpreting your regression models

We use regression models to provide empirical evidence of the relationship between our outcome of interest and a set of predictors that we theorize to be important drivers of that outcome.

Regression models *cannot* prove causality. This can make them difficult or awkward to interpret. You need to be very careful with the language you use to describe your model to avoid being misleading.

Let's step through interpreting your model results. To do this, we will look at the (highly precise) relationship between an individual's feelings towards President Obama and their dog ownership.

The National Election Survey (NES) asked respondents both their feelings towards President Obama (rating between zero and 100, with higher values indicating more support) and whether or not they own a dog. Let's fit a linear regression model against their responses to these two questions:

```{r}
m <- lm(obama_therm ~ own_dog, data = nes)

modelsummary(m,
             statistic = "[{conf.low}, {conf.high}]",
             stars = T,
             coef_rename = c("own_dogYes" = "Owns a dog"))
```

Our regression model is as follows:

$$
Obama\ thermometer = 74.3047 - 9.2858*Owns\ a\ dog + \epsilon
$$

It is tempting to state that the estimated effect of dog ownership on an individual's feelings towards Obama is a decrease of 9.28 points, on average and holding all else constant. However, this suggests an effect for which we really have no proof. According to @gelman2020, an effect is usually thought of as "the change associated with some treatment, or intervention" (pg. 84). Here, we would be suggesting that if we gave someone a dog, their support for Obama would drop by 9.28 points. That's not actually what we have found. Rather, we have observed that, on average, people who were surveyed who had a dog had lower opinions of Obama than those who did not own a dog.

Another way of thinking about this is to acknowledge that regression models using observational data only allow us to make comparisons between our units of observation. Here, we can make comparisons between respondents to the NES. We cannot, however, use this model to make statements about changes to any individual respondent. The NES did not give someone a dog and then ask them how their feelings towards Obama changed. We cannot suggest that we have insight into the effects of such an intervention (as cute as it would be).

**When you are interpreting a regression model, you should use comparative language.** For example, we can say that, under the fitted model, the average difference in feelings towards President Obama between a person with a dog and a person without a dog is 9.28 points.

::: callout-tip
This care with your language will become even more important when we move on to looking at multiple predictors.
:::

Remember: regression is just fancy averaging. This binary predictor makes this point easy to illustrate. For example, let's look at how we can interpret the intercept. Individuals who do not own a dog (`own_dog` $= 0$) are predicted to report feeling `r tidy(m) |> filter(term == "(Intercept)") |> pull(estimate) |> round(2)` points towards President Obama, on average and holding all else constant.

Where did this come from, you may ask? *It's just fancy averaging!*

```{r}
avg_responses <- nes |> 
  drop_na(own_dog) |> 
  group_by(own_dog) |> 
  summarise(avg_obama_therm = mean(obama_therm, na.rm = T))

avg_responses
```

There it is! It's just the average response provided by those who do not own a dog. How cool is that?!

You may also have noticed that the difference between the average response provided by dog owners and non-dog owners is roughly 9 points. Hmm...

```{r}
mutate(avg_responses, diff = avg_obama_therm - lag(avg_obama_therm))
```

That's our coefficient estimate! It's just the difference in the average response provided by dog owners and non-dog owners.

This is the beauty of linear regression: we can gain so much insight into the important predictors of the things that we really care about by applying simple and clear statistical processes to our data.

## Making predictions using your regression model

This leads us neatly to another great use for our regression model: predictions. We can use our model to predict our outcome of interest.

For example, imagine that I pulled someone randomly from the US voting population and asked them their feelings towards President Obama on a 100-point scale. What would be your best guess of their response?

We have access to the NES, which is a representative sample of the US voting population. In other words, they pulled `r nrow(nes) |> scales::comma()` people randomly from the US voting population and asked them this very question. We can look at the first 10 respondents' answers:

```{r}
head(nes$obama_therm, n = 10)
```

Imagine that the only information I provide to you is these `r nrow(nes) |> scales::comma()` individuals' responses. Your educated best guess may then be the average of their response:

```{r}
mean(nes$obama_therm, na.rm = T)
```
::: {.callout-tip collapse="true"}
It's just fancy averaging!

```{r}
lm(obama_therm ~ 1, data = nes)
```
:::

Now I might ask you what other piece of information you would like to know about this random individual that might improve your guess. You may respond that you would like to know whether or not the random individual identifies as a Democrat. You suspect that Democrats will have warmer feelings towards Obama than non-Democrats. 

You adopt the same approach (your best guess is the average response), but you can now look at Democrats and non-Democrats separately. 

```{r}
obama_therm_dem <- nes |> 
  group_by(dem) |> 
  summarise(avg_obama_therm = mean(obama_therm, na.rm = T))

obama_therm_dem
```

Now your best guess may be `r obama_therm_dem |> filter(dem == 1) |> pull(avg_obama_therm) |> round(2)` if the random person identifies as a Democrat and `r obama_therm_dem |> filter(dem == 0) |> pull(avg_obama_therm) |> round(2)` if they do not.

::: {.callout-tip collapse="true"}
It's just fancy averaging!

```{r}
lm(obama_therm ~ dem, data = nes)
```

Remember, the coefficient estimate for `dem` is the estimated difference between individuals who do not identify as Democrats (`dem` $= 0$) and individuals who identify as Democrats (`dem` $= 1$). In other words, add those two values together and you will get the average response provided by Democrats. 
:::

Have you improved your guess? How might we evaluate this? One approach would be to look at how far our predicted values were from the observed values.

Using our most simple approach, we will always predict the average feeling thermometer value provided by respondents to the NES. Using our slightly more sophisticated approach, we will predict the average response provided by Democrats if the individual is a Democrat, or the average response provided by non-Democrats if they do not identify as a Democrat. Let's map those predictions against the actual responses provided by NES participants:

```{r}
pred <- nes |> 
  transmute(dem, 
            obama_therm,
            pred_simple = mean(nes$obama_therm, na.rm = T)) |> 
  left_join(obama_therm_dem) |> 
  rename(pred_party_id = avg_obama_therm)

pred |> 
  select(dem, obama_therm, pred_simple, pred_party_id) |> 
  head()
```

Here, we have stored our simple prediction in the `pred_simple` variable and our approach that accounts for party identification in the `pred_party_id` column.

How does each approach perform?

```{r}
pred |> 
  mutate(resid_simple = pred_simple - obama_therm,
         resid_party_id = pred_party_id - obama_therm) |> 
  summarise(r_2_simple = sum(resid_simple^2, na.rm = T),
            r_2_party_id = sum(resid_party_id^2, na.rm = T))
```

The simple approach predicts values further from the observed values than our approach that accounts for party ID. Great! It is worthwhile asking our random individual to which party they belong.

Soon, we will look at including more than one predictor in our models. This will help us improve our predictive power even further. 
