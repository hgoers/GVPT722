[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "content/03-statistical_inference.html",
    "href": "content/03-statistical_inference.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "library(tidyverse)\nlibrary(poliscidata)\nlibrary(modelsummary)\nlibrary(broom)\nlibrary(ggdist)\n\n\nset.seed(222)"
  },
  {
    "objectID": "content/03-statistical_inference.html#packages",
    "href": "content/03-statistical_inference.html#packages",
    "title": "Statistical Inference",
    "section": "",
    "text": "library(tidyverse)\nlibrary(poliscidata)\nlibrary(modelsummary)\nlibrary(broom)\nlibrary(ggdist)\n\n\nset.seed(222)"
  },
  {
    "objectID": "content/03-statistical_inference.html#consistency-and-bias",
    "href": "content/03-statistical_inference.html#consistency-and-bias",
    "title": "Statistical Inference",
    "section": "Consistency and bias",
    "text": "Consistency and bias\nTo illustrate these concepts, we are going to look at the relationship between an individual’s feelings towards President Obama and whether or not they identify as a Democrat. We will use responses to the NES, which can be accessed here:\n\npoliscidata::nes\n\n\nConsistency\nLet’s take a small, pure random sample from the NES. We will start with a sample of 100 respondents, or 2% of the survey respondents.\n\nnes_100 &lt;- nes |&gt; \n  sample_n(100) |&gt; \n  select(caseid, obama_therm, dem)\n\nglimpse(nes_100)\n\nRows: 100\nColumns: 3\n$ caseid      &lt;dbl&gt; 405, 5672, 372, 3522, 3882, 1437, 6178, 1148, 5379, 5953, …\n$ obama_therm &lt;dbl&gt; 100, 85, 40, 100, 100, 100, NA, 60, 50, 100, 70, 100, NA, …\n$ dem         &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0…\n\n\nLet’s fit a model using these 100 people:\n\nm &lt;- lm(obama_therm ~ dem, data = nes_100)\n\nmodelsummary(m, \n             coef_rename = c(dem = \"Democrat\"), \n             statistic = \"conf.int\", \n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n47.268***\n\n\n\n[40.450, 54.085]\n\n\nDemocrat\n44.192***\n\n\n\n[33.383, 55.000]\n\n\nNum.Obs.\n93\n\n\nR2\n0.420\n\n\nR2 Adj.\n0.414\n\n\nAIC\n871.6\n\n\nBIC\n879.2\n\n\nLog.Lik.\n−432.813\n\n\nF\n65.961\n\n\nRMSE\n25.41\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nOur estimate for the party ID coefficient is 44.192. In other words, we found that individuals who identify as Democrats feel, on average, 44.2 points better about President Obama than individuals who do not identify as Democrats.\nBut we know that if we take a different pure random sample of 100 people from the NES, we will get a different estimate. For example:\n\nnes_100 &lt;- nes |&gt; \n  sample_n(100) |&gt; \n  select(caseid, obama_therm, dem)\n\nglimpse(nes_100)\n\nRows: 100\nColumns: 3\n$ caseid      &lt;dbl&gt; 6111, 148, 1913, 6246, 6640, 3983, 6779, 3650, 1452, 4943,…\n$ obama_therm &lt;dbl&gt; 85, 100, 80, NA, 50, 100, NA, 70, 100, 50, 70, 70, 60, 85,…\n$ dem         &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0…\n\n\nLet’s fit a new model using these 100 people:\n\nm &lt;- lm(obama_therm ~ dem, data = nes_100)\n\nmodelsummary(m, \n             coef_rename = c(dem = \"Democrat\"), \n             statistic = \"conf.int\", \n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n39.057***\n\n\n\n[31.878, 46.235]\n\n\nDemocrat\n42.548***\n\n\n\n[31.822, 53.274]\n\n\nNum.Obs.\n96\n\n\nR2\n0.398\n\n\nR2 Adj.\n0.391\n\n\nAIC\n904.3\n\n\nBIC\n912.0\n\n\nLog.Lik.\n−449.166\n\n\nF\n62.030\n\n\nRMSE\n26.05\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nLet’s do this many times and see how varied our estimates are:\n\nrandom_sample_nes &lt;- function(n) {\n  \n  nes_sample &lt;- nes |&gt; \n    sample_n(n) |&gt; \n    select(caseid, obama_therm, dem)\n  \n  m &lt;- lm(obama_therm ~ dem, data = nes_sample)\n  \n  dem_estimate &lt;- tidy(m) |&gt; \n    filter(term == \"dem\") |&gt; \n    pull(estimate)\n  \n  dem_estimate\n  \n}\n\nnes_100_dem_estimates &lt;- map(rep(100, 1000), random_sample_nes)\n\nggplot(tibble(beta_dem = unlist(nes_100_dem_estimates)), \n       aes(x = beta_dem)) + \n  stat_halfeye() + \n  theme_minimal()\n\n\n\n\nI now have 1,000 different estimates for dem drawn from 1,000 different models fit from 1,000 different random samples pulled from the same population. The only reason these estimates are different is random chance.\nThis is okay! But remember, we always need to be upfront about this uncertainty. Generally, we need to accept that 95% of these possible values of our coefficient could be the true coefficient.\n\n\n\n\n\n\nNote\n\n\n\nThe average estimate of the dem coefficient created from these repeated random samples will be the true estimate because this sample is not biased.1 We will talk about that more shortly.\n\n\nWe cannot take an infinite number of random samples of our population to calculate these possible values. In fact, we often cannot take more than one random sample of our population. So, we need a strategy that maximizes the chance that the single random sample we draw will produce an estimate of our coefficient that is as close to the true coefficient as possible. To do this, we need to tighten the spread of these possible values around their mean (our best guess at the true coefficient). In other words, we want to more consistent estimates.\nHow do we do this? We increase our sample size! In other words, we increase the amount of information we are using to build our understanding of the relationship between individuals’ feelings towards Obama and their political affiliation.\nTo demonstrate, let’s first look at repeated samples of 300 people, or 5% of the survey respondents. Then let’s also look at repeated samples of 1,000 people, or 17% of the survey respondents.\n\nnes_300_dem_estimates &lt;- map(rep(300, 1000), random_sample_nes)\n\nnes_1000_dem_estimates &lt;- map(rep(1000, 1000), random_sample_nes)\n\n\ntibble(sample_size = 100, \n       beta_dem = unlist(nes_100_dem_estimates)) |&gt; \n  bind_rows(\n    tibble(\n      sample_size = 300,\n      beta_dem = unlist(nes_300_dem_estimates)\n    )\n  ) |&gt; \n  bind_rows(\n    tibble(\n      sample_size = 1000,\n      beta_dem = unlist(nes_1000_dem_estimates)\n    )\n  ) |&gt; \n  ggplot(aes(x = beta_dem, fill = factor(sample_size))) + \n  stat_halfeye(alpha = 0.5) + \n  theme_minimal() + \n  theme(legend.position = \"bottom\") + \n  labs(fill = \"Sample size\")\n\n\n\n\nThe larger our sample size, the narrower the range of plausible alternative values of the dem coefficient are. They are more consistent.\nThe more consistently we produce estimates of our coefficient of interest, the more confident we can be that any single random sample from our population will tell us something close to the truth about the relationship between our dependent and independent variables.\n\n\nBias\nAt the end of the day, we are interested in that coefficient. We want to get as close as possible to the true coefficient (or the true relationship between our predictors and outcome of interest). Therefore, we want to avoid bias.\nTo illustrate, let’s see what happens to our understanding of the relationship between an individual’s feelings towards Obama and their party affiliation if we only look at male respondents.\n\nnes_men &lt;- nes |&gt; \n  filter(gender == \"Male\") |&gt; \n  select(caseid, obama_therm, dem, gender)\n\nglimpse(nes_men)\n\nRows: 2,847\nColumns: 4\n$ caseid      &lt;dbl&gt; 408, 3282, 1942, 118, 5533, 5880, 1651, 6687, 5903, 629, 1…\n$ obama_therm &lt;dbl&gt; 15, 100, 70, 30, 70, 45, 50, 60, 15, 100, NA, 0, 45, 30, 4…\n$ dem         &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ gender      &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male…\n\n\nLet’s take a random sample of 1,000 people from this biased sample of our population and estimate the relationship between their feelings towards Obama and their party affiliation.\n\nnes_1000_men &lt;- sample_n(nes_men, 1000)\n\nm &lt;- lm(obama_therm ~ dem, data = nes_1000_men)\n\nmodelsummary(m, \n             coef_rename = c(dem = \"Democrat\"), \n             statistic = \"conf.int\", \n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n43.575***\n\n\n\n[41.303, 45.847]\n\n\nDemocrat\n41.641***\n\n\n\n[37.722, 45.559]\n\n\nNum.Obs.\n925\n\n\nR2\n0.320\n\n\nR2 Adj.\n0.320\n\n\nAIC\n8838.3\n\n\nBIC\n8852.8\n\n\nLog.Lik.\n−4416.169\n\n\nF\n435.015\n\n\nRMSE\n28.65\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nOur estimate is consistent: we demonstrated above that a 1,000 person sample is large enough to give us confidence that this single sample’s coefficient is probably close to the average we would draw from an infinite number of random samples of our population.\nBut, I am not necessarily confident that this estimate is close to the true coefficient. That’s because we have a biased sample. We are interested in learning what the relationship is between individuals’ feelings towards Obama and their political affiliation across the whole US voting population. But we have only used information on people who identified as male to learn about that relationship.\nTo illustrate, here is the estimate produced by a 1,000 people random sample of the whole pool of respondents:\n\nnes_1000 &lt;- sample_n(nes, 1000)\n\nm &lt;- lm(obama_therm ~ dem, data = nes_1000)\n\nmodelsummary(m, \n             coef_rename = c(dem = \"Democrat\"), \n             statistic = \"conf.int\", \n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n43.102***\n\n\n\n[40.732, 45.473]\n\n\nDemocrat\n41.505***\n\n\n\n[37.724, 45.287]\n\n\nNum.Obs.\n934\n\n\nR2\n0.332\n\n\nR2 Adj.\n0.332\n\n\nAIC\n8929.3\n\n\nBIC\n8943.8\n\n\nLog.Lik.\n−4461.647\n\n\nF\n463.948\n\n\nRMSE\n28.73\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nWhen we include people who do not identify as male in our population from which we draw our random sample, we get a smaller estimate of the association between an individual’s feelings towards Obama and their party affiliation.\nOur coefficient estimate was artificially high because of our biased sample. This could meaningfully effect our understanding of the substantive significance of party affiliation on feelings towards Obama."
  },
  {
    "objectID": "content/03-statistical_inference.html#heteroscedasticity",
    "href": "content/03-statistical_inference.html#heteroscedasticity",
    "title": "Statistical Inference",
    "section": "Heteroscedasticity",
    "text": "Heteroscedasticity\nOne of the things you need to assume is true when you use linear regression to learn about the relationship between your outcome of interest and predictors is that your random error varies consistently across all values of your predictors.\nTo illustrate, let’s return to our made up model from last week.\n\\[\nY = 10 + 20X_1 + \\epsilon\n\\]\nRemember, that \\(y_i\\) will rarely equal exactly \\(10 + 20x_i\\). There will always be a little random error, \\(\\epsilon\\), sprinkled in there. If all is going well, this random error is independent of \\(X\\). In other words, you don’t need to know what the value of \\(x_i\\) is to work out what the random error is.\nLet’s find \\(Y\\) in a world in which our error is normally distributed with a standard deviation of 50.\n\n\n\n\n\n\nNote\n\n\n\nRemember, this means we are just randomly drawing a value from this distribution and adding it to \\(10+20x_1i\\) to find \\(y_i\\):\n\nggplot(tibble(error = rnorm(1e6, mean = 0, sd = 50)), aes(x = error)) + \n  geom_density(fill = \"#A2E3C4\") + \n  theme_minimal()\n\n\n\n\n\n\n\nerror &lt;- rnorm(100, mean = 0, sd = 50)\n\ndf &lt;- tibble(\n  x = 1:100,\n  error = error, \n  y = 10 + 20*x + error\n)\n\ndf\n\n# A tibble: 100 × 3\n       x error     y\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 -15.6  14.4\n 2     2 -38.9  11.1\n 3     3  36.3 106. \n 4     4  22.9 113. \n 5     5  57.8 168. \n 6     6 -28.4 102. \n 7     7 -46.1 104. \n 8     8 -35.3 135. \n 9     9 -50.3 140. \n10    10  25.0 235. \n# ℹ 90 more rows\n\n\nFor any given value of \\(x_i\\), the random error has an equal chance of being any normally distributed value with a standard error of 50. In other words, 95 percent of the time, the random error is going to be some value between -98 and 98 (or \\(1.96*50\\), remember back to confidence intervals).\nWe can plot this to get a better look at what consistent variation in the error term looks like:\n\nggplot(df, aes(x = x, y = y)) + \n  geom_ribbon(aes(ymin = 10 + 20*x - 95, ymax = 10 + 20*x + 95),\n              fill = \"grey\", alpha = 0.75) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nWhen this is the case, we have homoscedasticity.\nWhat happens if the variance in the error term changes across \\(x_i\\)?\n\ndf &lt;- tibble(x = 1:100) |&gt; \n  rowwise() |&gt; \n  mutate(error = rnorm(1, sd = x*3),\n         y = 10 + 20*x + error)\n\ndf\n\n# A tibble: 100 × 3\n# Rowwise: \n       x  error     y\n   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1     1   3.03  33.0\n 2     2 -19.9   30.1\n 3     3  -1.46  68.5\n 4     4 -14.6   75.4\n 5     5   2.35 112. \n 6     6 -16.1  114. \n 7     7  20.2  170. \n 8     8   3.91 174. \n 9     9 -14.4  176. \n10    10 -19.8  190. \n# ℹ 90 more rows\n\n\nHere, our error term is not independent of \\(X\\). We need to know \\(x_i\\) before we can work out the error value. Here, it is a random value drawn from a normal distribution that has a standard deviation equal to three times the given value of \\(x_i\\).\nLet’s visualize that:\n\ndf |&gt;\n  mutate(ymin = 10 + 20*x - x * 3 * 1.96,\n         ymax = 10 + 20*x + x * 3 * 1.96) |&gt; \n  ggplot(aes(x = x, y = y, ymin = ymin, ymax = ymax)) + \n  geom_ribbon(fill = \"grey\", alpha = 0.75) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal()\n\n\n\n\nHere, we have heteroscadasticity. When this is the case, we will have biased standard errors on our coefficient estimates. We cannot tell which way this bias will lean: we could be over or underestimating our confidence in our estimates."
  },
  {
    "objectID": "content/03-statistical_inference.html#footnotes",
    "href": "content/03-statistical_inference.html#footnotes",
    "title": "Statistical Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nActually, it won’t be because we have not taken an infinite number of random samples of 100 people (we have only taken 1,000 random samples). If we were able to take an infinite number of random samples (which we can only in theory), the average of those estimated coefficients would be the true coefficient.↩︎"
  },
  {
    "objectID": "content/slides/04-panel_data.html#todays-class",
    "href": "content/slides/04-panel_data.html#todays-class",
    "title": "Working with Panel Data",
    "section": "Today’s class",
    "text": "Today’s class\n\nWhat are panel data?\nIntroduce intuition behind panel data\nIntroduce random effects\nWorked example of fitting and interpreting a multi-level model"
  },
  {
    "objectID": "content/slides/04-panel_data.html#looking-at-the-same-observations-over-time",
    "href": "content/slides/04-panel_data.html#looking-at-the-same-observations-over-time",
    "title": "Working with Panel Data",
    "section": "Looking at the same observations over time",
    "text": "Looking at the same observations over time\n\nCountries’ voting behavior in the UN in the post-Cold War period;\nCountries’ propensity to go to war with one another;\nDemocratic backsliding across regions or countries;\nA politician’s voting behavior;\nAn individual’s turnout behavior over all elections in which they are eligible to vote."
  },
  {
    "objectID": "content/slides/04-panel_data.html#relationship-between-health-and-wealth",
    "href": "content/slides/04-panel_data.html#relationship-between-health-and-wealth",
    "title": "Working with Panel Data",
    "section": "Relationship between health and wealth",
    "text": "Relationship between health and wealth\nWhat is the relationship between people’s health and wealth?"
  },
  {
    "objectID": "content/slides/04-panel_data.html#countries-over-time",
    "href": "content/slides/04-panel_data.html#countries-over-time",
    "title": "Working with Panel Data",
    "section": "Countries over time",
    "text": "Countries over time\n\n\n# A tibble: 4,557 × 7\n   iso3c country  region              year gdp_per_cap log_gdp life_exp\n   &lt;chr&gt; &lt;chr&gt;    &lt;ord&gt;              &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 ZWE   Zimbabwe Sub-Saharan Africa     0        565.    6.34     44.7\n 2 ZWE   Zimbabwe Sub-Saharan Africa     1        569.    6.34     42.0\n 3 ZWE   Zimbabwe Sub-Saharan Africa     2        529.    6.27     44.6\n 4 ZWE   Zimbabwe Sub-Saharan Africa     3        474.    6.16     43.4\n 5 ZWE   Zimbabwe Sub-Saharan Africa     4        477.    6.17     44.5\n 6 ZWE   Zimbabwe Sub-Saharan Africa     5        471.    6.15     44.8\n 7 ZWE   Zimbabwe Sub-Saharan Africa     6        441.    6.09     45.4\n 8 ZWE   Zimbabwe Sub-Saharan Africa     7        425.    6.05     45.6\n 9 ZWE   Zimbabwe Sub-Saharan Africa     8        352.    5.86     46.7\n10 ZWE   Zimbabwe Sub-Saharan Africa     9        762.    6.64     48.1\n# ℹ 4,547 more rows"
  },
  {
    "objectID": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-1",
    "href": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-1",
    "title": "Working with Panel Data",
    "section": "Relationship between health and wealth",
    "text": "Relationship between health and wealth"
  },
  {
    "objectID": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-2",
    "href": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-2",
    "title": "Working with Panel Data",
    "section": "Relationship between health and wealth",
    "text": "Relationship between health and wealth"
  },
  {
    "objectID": "content/slides/04-panel_data.html#modelling-life-expectancy-over-time",
    "href": "content/slides/04-panel_data.html#modelling-life-expectancy-over-time",
    "title": "Working with Panel Data",
    "section": "Modelling life expectancy over time",
    "text": "Modelling life expectancy over time\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1Year + \\epsilon\n\\]\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n67.516***\n\n\n\n(0.253)\n\n\nYear\n0.289***\n\n\n\n(0.022)\n\n\nNum.Obs.\n4409\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "content/slides/04-panel_data.html#modelling-life-expectancy-over-time-1",
    "href": "content/slides/04-panel_data.html#modelling-life-expectancy-over-time-1",
    "title": "Working with Panel Data",
    "section": "Modelling life expectancy over time",
    "text": "Modelling life expectancy over time"
  },
  {
    "objectID": "content/slides/04-panel_data.html#accounting-for-the-structure-of-our-data",
    "href": "content/slides/04-panel_data.html#accounting-for-the-structure-of-our-data",
    "title": "Working with Panel Data",
    "section": "Accounting for the structure of our data",
    "text": "Accounting for the structure of our data"
  },
  {
    "objectID": "content/slides/04-panel_data.html#region-level-intercepts",
    "href": "content/slides/04-panel_data.html#region-level-intercepts",
    "title": "Working with Panel Data",
    "section": "Region-level intercepts",
    "text": "Region-level intercepts\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1Year + \\beta_2Region + \\epsilon\n\\]"
  },
  {
    "objectID": "content/slides/04-panel_data.html#region-level-intercepts-1",
    "href": "content/slides/04-panel_data.html#region-level-intercepts-1",
    "title": "Working with Panel Data",
    "section": "Region-level intercepts",
    "text": "Region-level intercepts"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-level-intercepts",
    "href": "content/slides/04-panel_data.html#country-level-intercepts",
    "title": "Working with Panel Data",
    "section": "Country-level intercepts",
    "text": "Country-level intercepts\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1Year + \\beta_1Country + \\epsilon\n\\]"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-level-intercepts-1",
    "href": "content/slides/04-panel_data.html#country-level-intercepts-1",
    "title": "Working with Panel Data",
    "section": "Country-level intercepts",
    "text": "Country-level intercepts"
  },
  {
    "objectID": "content/slides/04-panel_data.html#random-effects-a-brief-introduction",
    "href": "content/slides/04-panel_data.html#random-effects-a-brief-introduction",
    "title": "Working with Panel Data",
    "section": "Random effects: a brief introduction",
    "text": "Random effects: a brief introduction\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1Year + \\epsilon\n\\]\nWhere:\n\\[\n\\epsilon \\sim \\mathcal{N}(0,\\sigma)\n\\]\nThis error captures all variance that is not explained by our IV: annual growth in average life expancies."
  },
  {
    "objectID": "content/slides/04-panel_data.html#focusing-on-that-error",
    "href": "content/slides/04-panel_data.html#focusing-on-that-error",
    "title": "Working with Panel Data",
    "section": "Focusing on that error",
    "text": "Focusing on that error\n\n\n\n\n\n\n\nThe standard deviation of the error term for our model is 8.7."
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-starting-points",
    "href": "content/slides/04-panel_data.html#country-specific-starting-points",
    "title": "Working with Panel Data",
    "section": "Country-specific starting points",
    "text": "Country-specific starting points"
  },
  {
    "objectID": "content/slides/04-panel_data.html#your-error-is-too-full",
    "href": "content/slides/04-panel_data.html#your-error-is-too-full",
    "title": "Working with Panel Data",
    "section": "Your error is too full",
    "text": "Your error is too full\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1 Year + (b_{country} + \\epsilon)\n\\]\nFor example:\n\\[\nAverage\\ life\\ expectancy_{Australia} = \\beta_0 + \\beta_1 Year + (b_{Australia} + \\epsilon)\n\\]"
  },
  {
    "objectID": "content/slides/04-panel_data.html#australia-specific-error",
    "href": "content/slides/04-panel_data.html#australia-specific-error",
    "title": "Working with Panel Data",
    "section": "Australia-specific error",
    "text": "Australia-specific error\n\n\n\n\n\n\n\nSome of the distance between the predicted values and observed values are due to universal random error and some is due to Australia-specific random error."
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-error",
    "href": "content/slides/04-panel_data.html#country-specific-error",
    "title": "Working with Panel Data",
    "section": "Country-specific error",
    "text": "Country-specific error\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1 Year + (b_{country} + \\epsilon)\n\\]\nWhere:\n\\[\nb_{country} \\sim \\mathcal{N}(0,\\tau)\n\\]\n\n\n\\(\\tau\\) is the Greek letter tau."
  },
  {
    "objectID": "content/slides/04-panel_data.html#what-can-we-do-with-this-new-power",
    "href": "content/slides/04-panel_data.html#what-can-we-do-with-this-new-power",
    "title": "Working with Panel Data",
    "section": "What can we do with this new power?",
    "text": "What can we do with this new power?\n\nGive each country its own starting point (intercept);\nGive each country its own slope;\nGiven each country its starting point and slope.\n\nCool!"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-starting-point",
    "href": "content/slides/04-panel_data.html#country-specific-starting-point",
    "title": "Working with Panel Data",
    "section": "Country-specific starting point",
    "text": "Country-specific starting point\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + b_{0,country} + \\beta_1 Year + \\epsilon\n\\]"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-starting-point-1",
    "href": "content/slides/04-panel_data.html#country-specific-starting-point-1",
    "title": "Working with Panel Data",
    "section": "Country-specific starting point",
    "text": "Country-specific starting point"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-starting-point-2",
    "href": "content/slides/04-panel_data.html#country-specific-starting-point-2",
    "title": "Working with Panel Data",
    "section": "Country-specific starting point",
    "text": "Country-specific starting point"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-slope",
    "href": "content/slides/04-panel_data.html#country-specific-slope",
    "title": "Working with Panel Data",
    "section": "Country-specific slope",
    "text": "Country-specific slope\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1 Year + b_{1,country} + \\epsilon\n\\]"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-slope-1",
    "href": "content/slides/04-panel_data.html#country-specific-slope-1",
    "title": "Working with Panel Data",
    "section": "Country-specific slope",
    "text": "Country-specific slope"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-slope-2",
    "href": "content/slides/04-panel_data.html#country-specific-slope-2",
    "title": "Working with Panel Data",
    "section": "Country-specific slope",
    "text": "Country-specific slope"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-starting-point-and-slope",
    "href": "content/slides/04-panel_data.html#country-specific-starting-point-and-slope",
    "title": "Working with Panel Data",
    "section": "Country-specific starting point and slope",
    "text": "Country-specific starting point and slope\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + b_{0,country} + \\beta_1 Year + b_{1,country} + \\epsilon\n\\]"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-starting-point-and-slope-1",
    "href": "content/slides/04-panel_data.html#country-specific-starting-point-and-slope-1",
    "title": "Working with Panel Data",
    "section": "Country-specific starting point and slope",
    "text": "Country-specific starting point and slope"
  },
  {
    "objectID": "content/slides/04-panel_data.html#country-specific-starting-point-and-slope-2",
    "href": "content/slides/04-panel_data.html#country-specific-starting-point-and-slope-2",
    "title": "Working with Panel Data",
    "section": "Country-specific starting point and slope",
    "text": "Country-specific starting point and slope"
  },
  {
    "objectID": "content/slides/04-panel_data.html#why-not-just-include-a-country-variable",
    "href": "content/slides/04-panel_data.html#why-not-just-include-a-country-variable",
    "title": "Working with Panel Data",
    "section": "Why not just include a country variable?",
    "text": "Why not just include a country variable?\nThis approach is good at capturing that different starting point, but falls down in capturing the uncertainty around those starting points.\n\nOver-contextualized;\nMaking inferences based on very little information."
  },
  {
    "objectID": "content/slides/04-panel_data.html#multi-level-modelling",
    "href": "content/slides/04-panel_data.html#multi-level-modelling",
    "title": "Working with Panel Data",
    "section": "Multi-level modelling",
    "text": "Multi-level modelling\nFocusing on country-specific starting points:\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + b_{0,country} + \\beta_1 Year + \\epsilon\n\\]\nIn R:\n\nm_multi &lt;- lme4::lmer(life_exp ~ year + (1 | country), data = full_df)"
  },
  {
    "objectID": "content/slides/04-panel_data.html#life-expectancy-over-time",
    "href": "content/slides/04-panel_data.html#life-expectancy-over-time",
    "title": "Working with Panel Data",
    "section": "Life expectancy over time",
    "text": "Life expectancy over time\n\n\n\n\n\n\nSimple\nMulti-level\n\n\n\n\n(Intercept)\n67.516\n67.568\n\n\n\n(0.253)\n(0.591)\n\n\nYear\n0.289\n0.290\n\n\n\n(0.022)\n(0.004)\n\n\nSD (Intercept country)\n\n8.584\n\n\nSD (Observations)\n\n1.503\n\n\nNum.Obs.\n4409\n4409"
  },
  {
    "objectID": "content/slides/04-panel_data.html#fixed-effects",
    "href": "content/slides/04-panel_data.html#fixed-effects",
    "title": "Working with Panel Data",
    "section": "Fixed effects",
    "text": "Fixed effects\nThe fixed effects describe the overall relationship within our data.\n\ntidy(m_multi, effects = \"fixed\")\n\n# A tibble: 2 × 5\n  effect term        estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)   67.6     0.591       114. \n2 fixed  year           0.290   0.00374      77.6\n\n\n\n\nThese can be interpreted exactly as we would any other linear regression coefficients."
  },
  {
    "objectID": "content/slides/04-panel_data.html#group-level-effects",
    "href": "content/slides/04-panel_data.html#group-level-effects",
    "title": "Working with Panel Data",
    "section": "Group-level effects",
    "text": "Group-level effects\nThe group-level effects tell us how this relationship differs between groups in our data (here: countries).\n\ntidy(m_multi, effects = \"ran_pars\")\n\n# A tibble: 2 × 4\n  effect   group    term            estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars country  sd__(Intercept)     8.58\n2 ran_pars Residual sd__Observation     1.50"
  },
  {
    "objectID": "content/slides/04-panel_data.html#group-level-effects-1",
    "href": "content/slides/04-panel_data.html#group-level-effects-1",
    "title": "Working with Panel Data",
    "section": "Group-level effects",
    "text": "Group-level effects\nAs we move from country to country, how much does the average life expectancy change on average?\n\n\n# A tibble: 1 × 4\n  effect   group   term            estimate\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars country sd__(Intercept)     8.58\n\n\nWhat is the remaining universal/global variance unexplained by our IV?\n\n\n# A tibble: 1 × 4\n  effect   group    term            estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars Residual sd__Observation     1.50"
  },
  {
    "objectID": "content/slides/04-panel_data.html#substantive-interpretation",
    "href": "content/slides/04-panel_data.html#substantive-interpretation",
    "title": "Working with Panel Data",
    "section": "Substantive interpretation",
    "text": "Substantive interpretation\nThe between country variance makes up a large proportion of our total variance:\n\\[\n\\frac{SD\\ (Intercept\\ country)}{Total\\ SD} = 85.1%\n\\]\nThis variance is also much larger than the estimated effect of our IV (0.29 years): which country you are in matters much more than which year you are in."
  },
  {
    "objectID": "content/slides/04-panel_data.html#model-performance",
    "href": "content/slides/04-panel_data.html#model-performance",
    "title": "Working with Panel Data",
    "section": "Model performance",
    "text": "Model performance"
  },
  {
    "objectID": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-3",
    "href": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-3",
    "title": "Working with Panel Data",
    "section": "Relationship between health and wealth",
    "text": "Relationship between health and wealth\n\n\n\n\n\n\n\nNot very linear!"
  },
  {
    "objectID": "content/slides/04-panel_data.html#log-transformation",
    "href": "content/slides/04-panel_data.html#log-transformation",
    "title": "Working with Panel Data",
    "section": "Log transformation",
    "text": "Log transformation"
  },
  {
    "objectID": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-over-time",
    "href": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-over-time",
    "title": "Working with Panel Data",
    "section": "Relationship between health and wealth over time",
    "text": "Relationship between health and wealth over time"
  },
  {
    "objectID": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-over-time-1",
    "href": "content/slides/04-panel_data.html#relationship-between-health-and-wealth-over-time-1",
    "title": "Working with Panel Data",
    "section": "Relationship between health and wealth over time",
    "text": "Relationship between health and wealth over time"
  },
  {
    "objectID": "content/slides/04-panel_data.html#linear-regression",
    "href": "content/slides/04-panel_data.html#linear-regression",
    "title": "Working with Panel Data",
    "section": "Linear regression",
    "text": "Linear regression\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1GDP\\ per\\ capita + \\beta_2Year + \\epsilon\n\\]"
  },
  {
    "objectID": "content/slides/04-panel_data.html#linear-regression-1",
    "href": "content/slides/04-panel_data.html#linear-regression-1",
    "title": "Working with Panel Data",
    "section": "Linear regression",
    "text": "Linear regression\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n31.197\n\n\n\n(0.431)\n\n\nGDP per capita (US$, logged)\n4.568\n\n\n\n(0.051)\n\n\nYear\n0.039\n\n\n\n(0.013)\n\n\nNum.Obs.\n4232"
  },
  {
    "objectID": "content/slides/04-panel_data.html#model-performance-1",
    "href": "content/slides/04-panel_data.html#model-performance-1",
    "title": "Working with Panel Data",
    "section": "Model performance",
    "text": "Model performance"
  },
  {
    "objectID": "content/slides/04-panel_data.html#accounting-for-country-specific-variance",
    "href": "content/slides/04-panel_data.html#accounting-for-country-specific-variance",
    "title": "Working with Panel Data",
    "section": "Accounting for country-specific variance",
    "text": "Accounting for country-specific variance\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n56.521\n\n\n\n(0.781)\n\n\nGDP per capita (US$, logged)\n1.376\n\n\n\n(0.077)\n\n\nYear\n0.217\n\n\n\n(0.006)\n\n\nSD (Intercept country)\n6.945\n\n\nSD (Observations)\n1.459"
  },
  {
    "objectID": "content/slides/04-panel_data.html#model-performance-2",
    "href": "content/slides/04-panel_data.html#model-performance-2",
    "title": "Working with Panel Data",
    "section": "Model performance",
    "text": "Model performance"
  },
  {
    "objectID": "content/slides/04-panel_data.html#what-did-we-gain",
    "href": "content/slides/04-panel_data.html#what-did-we-gain",
    "title": "Working with Panel Data",
    "section": "What did we gain?",
    "text": "What did we gain?\nA lot of the unexplained variance in the simple linear regression model is explained by country-specific differences:\n\\[\n\\frac{SD\\ (Intercept\\ country)}{Total\\ SD} = \\frac{6.945\\ years}{6.945\\ years + 1.459\\ years} = 0.82\n\\]"
  },
  {
    "objectID": "content/slides/04-panel_data.html#model-performance-across-time",
    "href": "content/slides/04-panel_data.html#model-performance-across-time",
    "title": "Working with Panel Data",
    "section": "Model performance across time",
    "text": "Model performance across time"
  },
  {
    "objectID": "content/slides/04-panel_data.html#model-performance-across-gdp-per-capita",
    "href": "content/slides/04-panel_data.html#model-performance-across-gdp-per-capita",
    "title": "Working with Panel Data",
    "section": "Model performance across GDP per capita",
    "text": "Model performance across GDP per capita"
  },
  {
    "objectID": "content/slides/04-panel_data.html#summary",
    "href": "content/slides/04-panel_data.html#summary",
    "title": "Working with Panel Data",
    "section": "Summary",
    "text": "Summary\nToday you learnt how to:\n\nAccount for the structure of your data;\nIsolate fixed- and group-effects in the relationship between your outcome and explanatory variables."
  },
  {
    "objectID": "content/slides/02-ols_basics.html#todays-class",
    "href": "content/slides/02-ols_basics.html#todays-class",
    "title": "OLS Basics",
    "section": "Today’s class",
    "text": "Today’s class\n\nFitting a regression model\nInterpreting statistical significance\nInterpreting substantive meaning\nMaking predictions using your model"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#packages-for-today",
    "href": "content/slides/02-ols_basics.html#packages-for-today",
    "title": "OLS Basics",
    "section": "Packages for today",
    "text": "Packages for today\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggdist)\nlibrary(poliscidata)\nlibrary(modelsummary)"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#regression-models",
    "href": "content/slides/02-ols_basics.html#regression-models",
    "title": "OLS Basics",
    "section": "Regression models",
    "text": "Regression models\nLet’s look at the relationship between an outcome of interest, \\(Y\\), and a predictor of that outcome, \\(X_1\\):\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\epsilon\n\\]"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#regression-models-1",
    "href": "content/slides/02-ols_basics.html#regression-models-1",
    "title": "OLS Basics",
    "section": "Regression models",
    "text": "Regression models\nLet’s look at the relationship between an outcome of interest, \\(Y\\), and a predictor of that outcome, \\(X_1\\):\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\epsilon\n\\]\nLet’s assume that we know the true relationship between \\(Y\\) and \\(X_1\\):\n\\[\nY = 10 + 20X_1 + \\epsilon\n\\]"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#solving-for-y",
    "href": "content/slides/02-ols_basics.html#solving-for-y",
    "title": "OLS Basics",
    "section": "Solving for \\(Y\\)",
    "text": "Solving for \\(Y\\)\n\\[\nY = 10 + 20X_1 + \\epsilon\n\\]\n\nThis equation has two unknown variables: \\(X_1\\) and \\(\\epsilon\\).\nYou need both to work out the value of \\(Y\\)."
  },
  {
    "objectID": "content/slides/02-ols_basics.html#random-error",
    "href": "content/slides/02-ols_basics.html#random-error",
    "title": "OLS Basics",
    "section": "Random error",
    "text": "Random error\n\nThe error term captures all of the random things that inevitably muddy the relationship between our outcomes of interest and our predictors in the real world.\nIt is a set of random values."
  },
  {
    "objectID": "content/slides/02-ols_basics.html#a-world-with-no-random-error",
    "href": "content/slides/02-ols_basics.html#a-world-with-no-random-error",
    "title": "OLS Basics",
    "section": "A world with no random error",
    "text": "A world with no random error"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#a-world-with-random-error",
    "href": "content/slides/02-ols_basics.html#a-world-with-random-error",
    "title": "OLS Basics",
    "section": "A world with random error",
    "text": "A world with random error"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error",
    "href": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error",
    "title": "OLS Basics",
    "section": "Looking closely at that random error",
    "text": "Looking closely at that random error\nWe can learn about the shape of our random error.\nFor example, let’s assume that this random error:\n\nIs normally distributed,\nHas a mean of zero,\nHas a standard deviation of 50."
  },
  {
    "objectID": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error-1",
    "href": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error-1",
    "title": "OLS Basics",
    "section": "Looking closely at that random error",
    "text": "Looking closely at that random error\n\nggplot(tibble(error = rnorm(1e6, mean = 0, sd = 50)), aes(x = error)) + \n  geom_density(fill = \"#A2E3C4\") + \n  theme_minimal()"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error-2",
    "href": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error-2",
    "title": "OLS Basics",
    "section": "Looking closely at that random error",
    "text": "Looking closely at that random error\nRandom error is always added to \\(10 + 20X_1\\) to produce \\(Y\\). Let’s simulate that process:\n\nerror &lt;- rnorm(1, mean = 0, sd = 50)\nerror\n\n[1] 24.46725\n\nx_1 &lt;- 1\nx_1\n\n[1] 1\n\ny &lt;- 10 + 20*x_1 + error\ny\n\n[1] 54.46725"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error-3",
    "href": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error-3",
    "title": "OLS Basics",
    "section": "Looking closely at that random error",
    "text": "Looking closely at that random error\nRandom error is always added to \\(10 + 20X_1\\) to produce \\(Y\\). Let’s simulate that process:\n\nerror &lt;- rnorm(1, mean = 0, sd = 50)\nerror\n\n[1] -35.64102\n\nx_1 &lt;- 2\nx_1\n\n[1] 2\n\ny &lt;- 10 + 20*x_1 + error\ny\n\n[1] 14.35898"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error-4",
    "href": "content/slides/02-ols_basics.html#looking-closely-at-that-random-error-4",
    "title": "OLS Basics",
    "section": "Looking closely at that random error",
    "text": "Looking closely at that random error\nRandom error is always added to \\(10 + 20X_1\\) to produce \\(Y\\). Let’s simulate that process:\n\nerror &lt;- rnorm(1, mean = 0, sd = 50)\nerror\n\n[1] 32.34872\n\nx_1 &lt;- 3\nx_1\n\n[1] 3\n\ny &lt;- 10 + 20*x_1 + error\ny\n\n[1] 102.3487"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#fitting-our-regression-model",
    "href": "content/slides/02-ols_basics.html#fitting-our-regression-model",
    "title": "OLS Basics",
    "section": "Fitting our regression model",
    "text": "Fitting our regression model\nAssume \\(X_1\\) is equal to all whole numbers between one and 100:\n\nx_1 &lt;- 1:100\n\nLet’s find the 100 corresponding values of \\(Y\\):\n\nerror &lt;- rnorm(100, mean = 0, sd = 50)\n\ndf &lt;- tibble(x_1 = x_1,\n             y = 10 + 20*x_1 + error)\n\nhead(df, n = 5)\n\n# A tibble: 5 × 2\n    x_1     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1  61.6\n2     2  53.3\n3     3 -14.4\n4     4 178. \n5     5  86.5"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#fitting-our-regression-model-1",
    "href": "content/slides/02-ols_basics.html#fitting-our-regression-model-1",
    "title": "OLS Basics",
    "section": "Fitting our regression model",
    "text": "Fitting our regression model\n\nggplot(df, aes(x = x_1, y = y)) + \n  geom_point() + \n  theme_minimal()"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#what-is-the-line-of-best-fit",
    "href": "content/slides/02-ols_basics.html#what-is-the-line-of-best-fit",
    "title": "OLS Basics",
    "section": "What is the line-of-best-fit?",
    "text": "What is the line-of-best-fit?\nIn other words, what is the line that minimizes the distance between itself and all of these points?\n\nm &lt;- lm(y ~ x_1, data = df)\n\nmodelsummary(m, statistic = NULL, coef_rename = c(x_1 = \"X1\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n14.331\n\n\nX1\n20.130\n\n\nNum.Obs.\n100\n\n\nR2\n0.992\n\n\nR2 Adj.\n0.992\n\n\nAIC\n1083.7\n\n\nBIC\n1091.5\n\n\nLog.Lik.\n−538.852\n\n\nF\n11796.835\n\n\nRMSE\n52.96"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#what-is-the-line-of-best-fit-1",
    "href": "content/slides/02-ols_basics.html#what-is-the-line-of-best-fit-1",
    "title": "OLS Basics",
    "section": "What is the line-of-best-fit?",
    "text": "What is the line-of-best-fit?\nOur fitted model is:\n\\[\nY = 14.331 + 20.130X_1 + \\epsilon\n\\]\nInstead of this:\n\\[\nY = 10 + 20X_1 + \\epsilon\n\\]\nWhy?"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#uncertainty-around-coefficients",
    "href": "content/slides/02-ols_basics.html#uncertainty-around-coefficients",
    "title": "OLS Basics",
    "section": "Uncertainty around coefficients",
    "text": "Uncertainty around coefficients\nWe have information about how uncertain we are of these coefficients:\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     14.3    10.8        1.33 1.87e-  1\n2 x_1             20.1     0.185    109.   6.09e-104"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#uncertainty-around-the-intercept",
    "href": "content/slides/02-ols_basics.html#uncertainty-around-the-intercept",
    "title": "OLS Basics",
    "section": "Uncertainty around the intercept",
    "text": "Uncertainty around the intercept\nOur best guess of the intercept:\n\nintercept_est &lt;- tidy(m) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nintercept_est\n\n[1] 14.33133\n\n\nOur level of uncertainty in that best guess:\n\nintercept_se &lt;- tidy(m) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(std.error)\n\nintercept_se\n\n[1] 10.78078"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#uncertainty-around-the-intercept-1",
    "href": "content/slides/02-ols_basics.html#uncertainty-around-the-intercept-1",
    "title": "OLS Basics",
    "section": "Uncertainty around the intercept",
    "text": "Uncertainty around the intercept"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#uncertainty-around-beta_1",
    "href": "content/slides/02-ols_basics.html#uncertainty-around-beta_1",
    "title": "OLS Basics",
    "section": "Uncertainty around \\(\\beta_1\\)",
    "text": "Uncertainty around \\(\\beta_1\\)\nOur best guess:\n\nbeta_1_est &lt;- tidy(m) |&gt; \n  filter(term == \"x_1\") |&gt; \n  pull(estimate)\n\nbeta_1_est\n\n[1] 20.13028\n\n\nOur uncertainty around this best guess:\n\nbeta_1_se &lt;- tidy(m) |&gt; \n  filter(term == \"x_1\") |&gt; \n  pull(std.error)\n\nbeta_1_se\n\n[1] 0.1853391"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#uncertainty-around-beta_1-1",
    "href": "content/slides/02-ols_basics.html#uncertainty-around-beta_1-1",
    "title": "OLS Basics",
    "section": "Uncertainty around \\(\\beta_1\\)",
    "text": "Uncertainty around \\(\\beta_1\\)"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#statistical-significance",
    "href": "content/slides/02-ols_basics.html#statistical-significance",
    "title": "OLS Basics",
    "section": "Statistical significance",
    "text": "Statistical significance\nTraditionally, we are required to accept that 95 percent of all alternative coefficient estimates are plausible.\n\nWhat estimates are included in this range?\n\n\nmodelplot(m) + \n  geom_vline(xintercept = 0, colour = \"red\")"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#p-values",
    "href": "content/slides/02-ols_basics.html#p-values",
    "title": "OLS Basics",
    "section": "P-values",
    "text": "P-values\nTells us how likely we would be to observe the coefficient estimate that we did if it were actually equal to zero.\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     14.3    10.8        1.33 1.87e-  1\n2 x_1             20.1     0.185    109.   6.09e-104"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#interpreting-regression-models-substantive-meaning",
    "href": "content/slides/02-ols_basics.html#interpreting-regression-models-substantive-meaning",
    "title": "OLS Basics",
    "section": "Interpreting regression models: substantive meaning",
    "text": "Interpreting regression models: substantive meaning\n\nRegression models cannot prove causality.\nThis can make them difficult or awkward to interpret."
  },
  {
    "objectID": "content/slides/02-ols_basics.html#obama-and-dog-owners",
    "href": "content/slides/02-ols_basics.html#obama-and-dog-owners",
    "title": "OLS Basics",
    "section": "Obama and dog owners",
    "text": "Obama and dog owners\nThe National Election Survey asked respondents both their feelings towards President Obama (rating between zero and 100, with higher values indicating more support) and whether or not they own a dog."
  },
  {
    "objectID": "content/slides/02-ols_basics.html#obama-and-dog-owners-1",
    "href": "content/slides/02-ols_basics.html#obama-and-dog-owners-1",
    "title": "OLS Basics",
    "section": "Obama and dog owners",
    "text": "Obama and dog owners\nLet’s fit a linear regression model against their responses to these two questions:\n\nm &lt;- lm(obama_therm ~ own_dog, data = nes)\n\nmodelsummary(m,\n             statistic = NULL,\n             stars = T,\n             coef_rename = c(\"own_dogYes\" = \"Owns a dog\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n74.305***\n\n\nOwns a dog\n−9.286***\n\n\nNum.Obs.\n1927\n\n\nR2\n0.023\n\n\nR2 Adj.\n0.022\n\n\nAIC\n18606.2\n\n\nBIC\n18622.8\n\n\nLog.Lik.\n−9300.077\n\n\nRMSE\n30.18\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#obama-and-dog-owners-2",
    "href": "content/slides/02-ols_basics.html#obama-and-dog-owners-2",
    "title": "OLS Basics",
    "section": "Obama and dog owners",
    "text": "Obama and dog owners\nOur regression model is as follows:\n\\[\nObama\\ thermometer = 74.305 - 9.286* Owns\\ a\\ dog + \\epsilon\n\\]\nWhat does this mean substantively?"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#obama-and-dog-owners-3",
    "href": "content/slides/02-ols_basics.html#obama-and-dog-owners-3",
    "title": "OLS Basics",
    "section": "Obama and dog owners",
    "text": "Obama and dog owners\nIt is tempting to state that:\n\nThe estimated effect of dog ownership on an individual’s feelings towards Obama is a decrease of 9.28 points, on average and holding all else constant.\n\nBut, this suggests an effect for which we have no proof!\n\nWe would be suggesting that if we gave someone a dog, their support for Obama would drop by 9.28 points.\nThat’s not actually what we have found!"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#obama-and-dog-owners-4",
    "href": "content/slides/02-ols_basics.html#obama-and-dog-owners-4",
    "title": "OLS Basics",
    "section": "Obama and dog owners",
    "text": "Obama and dog owners\nWe have observed that, on average, people who were surveyed who had a dog had lower opinions of Obama than those who did not own a dog.\n\nRegression models using observational data only allow us to make comparisons between our units of observation.\nHere, we can make comparisons between respondents to the NES. We cannot, however, use this model to make statements about changes to any individual respondent."
  },
  {
    "objectID": "content/slides/02-ols_basics.html#where-do-coefficient-estimates-come-from",
    "href": "content/slides/02-ols_basics.html#where-do-coefficient-estimates-come-from",
    "title": "OLS Basics",
    "section": "Where do coefficient estimates come from?",
    "text": "Where do coefficient estimates come from?\n\\[\nObama\\ thermometer = \\\\ 74.305 - 9.286 * Owns\\ a\\ dog + \\epsilon\n\\]\n\navg_responses &lt;- nes |&gt; \n  drop_na(own_dog) |&gt; \n  group_by(own_dog) |&gt; \n  summarise(avg_obama_therm = mean(obama_therm, \n                                   na.rm = T)) |&gt; \n  mutate(diff = avg_obama_therm - lag(avg_obama_therm))\n\navg_responses\n\n# A tibble: 2 × 3\n  own_dog avg_obama_therm  diff\n  &lt;fct&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 No                 74.3 NA   \n2 Yes                65.0 -9.29"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#making-predictions-using-our-model",
    "href": "content/slides/02-ols_basics.html#making-predictions-using-our-model",
    "title": "OLS Basics",
    "section": "Making predictions using our model",
    "text": "Making predictions using our model\nImagine that I pulled someone randomly from the US voting population and asked them their feelings towards President Obama on a 100-point scale. What would be your best guess of their response?"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#making-predictions-using-our-model-1",
    "href": "content/slides/02-ols_basics.html#making-predictions-using-our-model-1",
    "title": "OLS Basics",
    "section": "Making predictions using our model",
    "text": "Making predictions using our model\nThe NES pulled 5,916 people randomly from the US voting population and asked them this very question.\nWhat were their responses?\n\nnes |&gt; \n  select(caseid, obama_therm) |&gt; \n  slice_head(n = 10)\n\n   caseid obama_therm\n1     408          15\n2    3282         100\n3    1942          70\n4     118          30\n5    5533          70\n6    5880          45\n7    1651          50\n8    6687          60\n9    5903          15\n10    629         100"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#making-predictions-using-our-model-2",
    "href": "content/slides/02-ols_basics.html#making-predictions-using-our-model-2",
    "title": "OLS Basics",
    "section": "Making predictions using our model",
    "text": "Making predictions using our model\nImagine that the only information I provide to you is these 5,916 individuals’ responses.\n\nYour educated best guess may then be the average of their response:\n\n\nmean(nes$obama_therm, na.rm = T)\n\n[1] 60.74377"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#making-predictions-using-our-model-3",
    "href": "content/slides/02-ols_basics.html#making-predictions-using-our-model-3",
    "title": "OLS Basics",
    "section": "Making predictions using our model",
    "text": "Making predictions using our model\nImagine that the only information I provide to you is these 5,916 individuals’ responses.\n\nYour educated best guess may then be the average of their response:\n\n\nmean(nes$obama_therm, na.rm = T)\n\n[1] 60.74377\n\n\n\nIt’s just fancy averaging!\n\n\nlm(obama_therm ~ 1, data = nes)\n\n\nCall:\nlm(formula = obama_therm ~ 1, data = nes)\n\nCoefficients:\n(Intercept)  \n      60.74"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#making-predictions-using-our-model-4",
    "href": "content/slides/02-ols_basics.html#making-predictions-using-our-model-4",
    "title": "OLS Basics",
    "section": "Making predictions using our model",
    "text": "Making predictions using our model\nWhat other piece of information you would like to know about this random individual that might improve your guess?"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#making-predictions-using-our-model-5",
    "href": "content/slides/02-ols_basics.html#making-predictions-using-our-model-5",
    "title": "OLS Basics",
    "section": "Making predictions using our model",
    "text": "Making predictions using our model\nWhat other piece of information you would like to know about this random individual that might improve your guess?\nAre they a Democrat?\n\nnes |&gt; \n  select(caseid, obama_therm, dem) |&gt; \n  slice_head(n = 10)\n\n   caseid obama_therm dem\n1     408          15   0\n2    3282         100   1\n3    1942          70   0\n4     118          30   1\n5    5533          70   0\n6    5880          45   0\n7    1651          50   0\n8    6687          60   0\n9    5903          15   0\n10    629         100   1"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#making-predictions-using-our-model-6",
    "href": "content/slides/02-ols_basics.html#making-predictions-using-our-model-6",
    "title": "OLS Basics",
    "section": "Making predictions using our model",
    "text": "Making predictions using our model\n\nNow you can look at Democrats and non-Democrats separately:\n\n\nobama_therm_dem &lt;- nes |&gt; \n  drop_na(obama_therm, dem) |&gt; \n  group_by(dem) |&gt; \n  summarise(avg_obama_therm = mean(obama_therm, na.rm = T))\n\nobama_therm_dem\n\n# A tibble: 2 × 2\n    dem avg_obama_therm\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     0            44.2\n2     1            85.3"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#making-predictions-using-our-model-7",
    "href": "content/slides/02-ols_basics.html#making-predictions-using-our-model-7",
    "title": "OLS Basics",
    "section": "Making predictions using our model",
    "text": "Making predictions using our model\n\nNow you can look at Democrats and non-Democrats separately:\n\n\nobama_therm_dem\n\n# A tibble: 2 × 2\n    dem avg_obama_therm\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     0            44.2\n2     1            85.3\n\n\n\nlm(obama_therm ~ dem, data = nes) |&gt; \n  tidy() |&gt; \n  transmute(term, estimate, cum_est = estimate + lag(estimate))\n\n# A tibble: 2 × 3\n  term        estimate cum_est\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)     44.2    NA  \n2 dem             41.1    85.3"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#have-we-improved-our-guess",
    "href": "content/slides/02-ols_basics.html#have-we-improved-our-guess",
    "title": "OLS Basics",
    "section": "Have we improved our guess?",
    "text": "Have we improved our guess?\nHow accurately do we predict individuals’ feelings towards Obama?\n\n\n   caseid dem obama_therm pred_simple pred_party_id\n1     408   0          15    60.74377      44.24474\n2    3282   1         100    60.74377      85.30587\n3    1942   0          70    60.74377      44.24474\n4     118   1          30    60.74377      85.30587\n5    5533   0          70    60.74377      44.24474\n6    5880   0          45    60.74377      44.24474\n7    1651   0          50    60.74377      44.24474\n8    6687   0          60    60.74377      44.24474\n9    5903   0          15    60.74377      44.24474\n10    629   1         100    60.74377      85.30587\n11   1434   1          NA    60.74377      85.30587\n12   6380   0           0    60.74377      44.24474"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#how-does-each-approach-perform",
    "href": "content/slides/02-ols_basics.html#how-does-each-approach-perform",
    "title": "OLS Basics",
    "section": "How does each approach perform?",
    "text": "How does each approach perform?"
  },
  {
    "objectID": "content/slides/02-ols_basics.html#how-does-each-approach-perform-1",
    "href": "content/slides/02-ols_basics.html#how-does-each-approach-perform-1",
    "title": "OLS Basics",
    "section": "How does each approach perform?",
    "text": "How does each approach perform?\nWhat’s the sum of those distances?\n\npred |&gt; \n  mutate(resid_simple = pred_simple - obama_therm,\n         resid_party_id = pred_party_id - obama_therm) |&gt; \n  summarise(r_2_simple = scales::comma(sum(resid_simple^2, na.rm = T)),\n            r_2_party_id = scales::comma(sum(resid_party_id^2, na.rm = T)))\n\n  r_2_simple r_2_party_id\n1  6,582,137    4,351,352\n\n\n\n\n\n\n\n\nRemember: this is the \\(R^2\\) value."
  },
  {
    "objectID": "content/02-ols_basics.html#packages",
    "href": "content/02-ols_basics.html#packages",
    "title": "OLS Basics",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggdist)\nlibrary(poliscidata)\nlibrary(modelsummary)\n\n\nset.seed(1234)"
  },
  {
    "objectID": "content/02-ols_basics.html#regression-models",
    "href": "content/02-ols_basics.html#regression-models",
    "title": "OLS Basics",
    "section": "Regression models",
    "text": "Regression models\nThis week we are going to continue to look at how we can model the distribution of our outcome of interest using a single predictor.\nHere’s the general formula for a linear regression model with a single predictor:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\epsilon\n\\]\nLet’s work backwards to help build our intuition for this model. Let’s start with a modeled relationship between some outcome, \\(Y\\), and a predictor, \\(X_1\\):\n\\[\nY = 10 + 20X_1 + \\epsilon\n\\]\nThink of this as the true relationship between \\(Y\\) and \\(X_1\\).\n\n\n\n\n\n\nNote\n\n\n\nOut there in the real world we use modeling to attempt to find this true relationship. If only we had access to it!\n\n\n\nRandom error\nThis equation has two unknown variables: \\(X_1\\) and \\(\\epsilon\\). You need both to be able to calculate the corresponding values for \\(Y\\).\n\\(X_1\\) will in many ways be more intuitive for you. Generally, it represents some vector of values that representing something tangible: perhaps it’s a person’s age, a country’s GDP, or a binary value telling you whether or not a country is a democracy. The error term can be a bit more abstract, so let’s take a minute to look at it more closely.\n\n\n\n\n\n\nNote\n\n\n\nA quick note on notation! Generally, we will use capital letters (for example, \\(Y\\) and \\(X_1\\)) to denote vectors and lower case letters (for example, \\(y\\) and \\(x_1\\)) to denote single values (usually with another subscript telling us the observation to which that single value belongs).\nWhat is a vector? Here, I mean a series of values. In other words, it is a fancy term for some variable. The term comes from matrix algebra. A matrix with only one row or column is referred to as a vector. Think of it simply as one column (variable) or one row (unit of observation) of a dataframe.\nFor example, if our outcome of interest is whether or not an individual will vote in the next election, we might use data on whether or not survey respondents voted in the last election to study the predictors of voting. In this case, our vector \\(Y\\) will be a series of 1s and 0s describing whether or not each survey respondent voted in the last election.\n\n\nThe error term; on the other hand, is a set of random values. It captures all of the random things that inevitably muddy the relationship between our outcomes of interest and our predictors in the real world.\nWe can learn some useful things about the random error effecting our outcome of interest. For example, let’s say that the error in this true relationship is normally distributed with a mean of zero and a standard deviation of 50. In other words, there is some random noise in our data: \\(Y\\) does not always precisely equal \\(10 + 20X_1\\). Some random value will always be added to \\(10 + 20X_1\\). That value will be drawn randomly from this distribution:\n\nggplot(tibble(error = rnorm(1e6, mean = 0, sd = 50)), aes(x = error)) + \n  geom_density(fill = \"#A2E3C4\") + \n  theme_minimal()\n\n\n\n\nThe coloured area under that curve represents the probability that the corresponding error value (along the \\(x\\)-axis) will be drawn. This means that each error value randomly drawn is most likely to be a number around zero (where the curve peaks). It is very unlikely to be a number bigger than around 150 or smaller than around -150.\nNotably, this error value is completely independent of \\(X_1\\) and \\(Y\\) (you don’t need these values to work out your error value). To demonstrate, let’s draw some of those random error values from this distribution:\n\nrnorm(1, mean = 0, sd = 50)\n\n[1] -47.37963\n\n\n\nrnorm(1, mean = 0, sd = 50)\n\n[1] -84.53445\n\n\n\nrnorm(1, mean = 0, sd = 50)\n\n[1] -28.83643\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis semester we will talk a lot about the assumptions you are required to make when you fit a linear regression model. These include some assumptions about error, including its shape and uniformity."
  },
  {
    "objectID": "content/02-ols_basics.html#fitting-our-regression-model",
    "href": "content/02-ols_basics.html#fitting-our-regression-model",
    "title": "OLS Basics",
    "section": "Fitting our regression model",
    "text": "Fitting our regression model\nLet’s make \\(X_1\\) equal all the whole numbers between one and 100. We can take this vector and our random error and find 100 values of \\(Y\\):\n\nx_1 &lt;- 1:100\nerror &lt;- rnorm(100, mean = 0, sd = 50)\n\ndf &lt;- tibble(x_1 = x_1,\n             y = 10 + 20*x_1 + error)\n\ndf\n\n# A tibble: 100 × 2\n     x_1     y\n   &lt;int&gt; &lt;dbl&gt;\n 1     1 -17.0\n 2     2  78.2\n 3     3  44.3\n 4     4 184. \n 5     5  96.6\n 6     6 168. \n 7     7 174. \n 8     8 184. \n 9     9 139. \n10    10 205. \n# ℹ 90 more rows\n\n\nLet’s plot those:\n\nggplot(df, aes(x = x_1, y = y)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nWhat is the line-of-best-fit? In other words, what is the line that minimizes the distance between itself and all of these points?\n\nm &lt;- lm(y ~ x_1, data = df)\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x_1, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-135.372  -28.844   -1.649   31.502  109.694 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   8.0917     9.9769   0.811    0.419    \nx_1          20.0844     0.1715 117.098   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 49.51 on 98 degrees of freedom\nMultiple R-squared:  0.9929,    Adjusted R-squared:  0.9928 \nF-statistic: 1.371e+04 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nHmm, it’s different from the true relationship. It’s this:\n\\[\nY = 8.09 + 20.08X_1 + \\epsilon\n\\]\nInstead of this:\n\\[\nY = 10 + 20X_1 + \\epsilon\n\\]\nThis is despite the fact that we set everything up according to the true relationship between \\(Y\\) and \\(X_1\\)!\nWhy is it different? The long and short answer is: that pesky random error!\nRandom error will always exist. That’s okay. That’s why we built it into our true relationship. We have some statistical tools to help us deal out.\nLooking back at our model’s output we can see that we are provided with a lot of information about its uncertainty in those coefficient estimates.\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     8.09     9.98      0.811 4.19e-  1\n2 x_1            20.1      0.172   117.    4.05e-107\n\n\nIn addition to our coefficient estimates (estimate), we also have access to each estimate’s standard error (std.error), t-statistic (statistic), and p-value (p.value). Let’s focus on the standard error. We can use this to work out all of the plausible alternative coefficient estimates that our model could have found if that random error had been different. Let’s take a look at those plausible alternatives to see whether the true relationship (\\(\\beta_0 = 10\\) and \\(\\beta_1 = 20\\)) is sitting in there.\nLet’s start with the intercept. Centering our plausible set of intercept estimates on our model’s best guess - 8.09 - let’s simulate those alternative estimates using our uncertainty (the intercept coefficient’s standard error) and see where the “true” intercept estimate (represented by the black line on the graph below) sits within this context:\n\nintercept_est &lt;- tidy(m) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nintercept_est\n\n[1] 8.091687\n\nintercept_se &lt;- tidy(m) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(std.error)\n\nintercept_se\n\n[1] 9.976851\n\n\n\ntibble(intercept = rnorm(1e6, mean = intercept_est, sd = intercept_se)) |&gt; \n  ggplot(aes(x = intercept)) + \n  stat_halfeye(.width = c(0.025, 0.975), fill = \"#A2E3C4\") +\n  geom_vline(xintercept = 10) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI expect these plausible alternative coefficients to be normally distributed. For a simulated proof of why we can assume this, please check out the Regression notes from GVPT622.\n\n\nWe can do the same for our model’s best guess of \\(\\beta_1\\):\n\nbeta_1_est &lt;- tidy(m) |&gt; \n  filter(term == \"x_1\") |&gt; \n  pull(estimate)\n\nbeta_1_est\n\n[1] 20.08444\n\nbeta_1_se &lt;- tidy(m) |&gt; \n  filter(term == \"x_1\") |&gt; \n  pull(std.error)\n\nbeta_1_se\n\n[1] 0.1715183\n\n\n\ntibble(beta_1 = rnorm(1e6, mean = beta_1_est, sd = beta_1_se)) |&gt; \n  ggplot(aes(x = beta_1)) + \n  stat_halfeye(.width = c(0.025, 0.975), fill = \"#3095A2\") +\n  geom_vline(xintercept = 20) + \n  theme_minimal()\n\n\n\n\nOften in political science we are required to accept that the true estimates sit somewhere with 95 percent of these alternatives. Happily for us, the true values for the intercept (10) and \\(\\beta_1\\) (20) sit within this range (shown by the horizontal black lines on the graphs)."
  },
  {
    "objectID": "content/02-ols_basics.html#statistical-significance",
    "href": "content/02-ols_basics.html#statistical-significance",
    "title": "OLS Basics",
    "section": "Statistical significance",
    "text": "Statistical significance\nTraditionally, we are required to accept that 95 percent of all alternative coefficient estimates are plausible. Sadly for us, zero is included within this range of plausible values for the intercept coefficient. This coefficient would not; therefore, be considered to be statistically significant at the 95 percent confidence level.\nWe can learn that more quickly using the intercept coefficient’s p-value (which is less than 0.05, or the 5 percent risk we are willing to take on that we believe a null relationship):\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     8.09     9.98      0.811 4.19e-  1\n2 x_1            20.1      0.172   117.    4.05e-107\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe p-value tells us how likely we would be to observe the coefficient estimate that we did if it were actually equal to zero. It is calculated using the coefficient estimate’s t-statistic (statistic), which is a transformation of the coefficient estimate into its place along a standard distribution: the t-distribution.\n\n\nWhat if we had more certainty around our estimates? In other words, what if there was less error? Let’s reduce the spread of our random error by reducing its standard deviation from 50 to 5:\n\nerror_smaller &lt;- rnorm(100, mean = 0, sd = 5)\n\ndf_smaller &lt;- tibble(x_1 = x_1,\n                     y = 10 + 20*x_1 + error_smaller)\n\ndf_smaller\n\n# A tibble: 100 × 2\n     x_1     y\n   &lt;int&gt; &lt;dbl&gt;\n 1     1  28.1\n 2     2  39.1\n 3     3  74.7\n 4     4  89.9\n 5     5 106. \n 6     6 131. \n 7     7 155. \n 8     8 166. \n 9     9 181. \n10    10 213. \n# ℹ 90 more rows\n\n\nLet’s plot that:\n\nggplot(df_smaller, aes(x = x_1, y = y)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\nThose data points are much more tightly placed around the line-of-best-fit. Let’s find that line:\n\nm_smaller &lt;- lm(y ~ x_1, data = df_smaller)\n\nsummary(m_smaller)\n\n\nCall:\nlm(formula = y ~ x_1, data = df_smaller)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.351  -3.325  -0.249   3.777  10.456 \n\nCoefficients:\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)   9.2587     1.0180    9.095 1.12e-14 ***\nx_1          20.0112     0.0175 1143.402  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.052 on 98 degrees of freedom\nMultiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 \nF-statistic: 1.307e+06 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nOr:\n\\[\nY = 9.2587 + 20.0112X_1 + \\epsilon\n\\]\nThat’s closer to the true relationship! But - of course - there is still uncertainty in those estimates. Let’s look at the plausible range of intercept estimates:\n\nintercept_est_smaller &lt;- tidy(m_smaller) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(estimate)\n\nintercept_est_smaller\n\n[1] 9.258682\n\nintercept_se_smaller &lt;- tidy(m_smaller) |&gt; \n  filter(term == \"(Intercept)\") |&gt; \n  pull(std.error)\n\nintercept_se_smaller\n\n[1] 1.018022\n\n\n\ntibble(intercept = rnorm(1e6, mean = intercept_est_smaller, sd = intercept_se_smaller)) |&gt; \n  ggplot(aes(x = intercept)) + \n  stat_halfeye(.width = c(0.025, 0.975), fill = \"#A2E3C4\") +\n  geom_vline(xintercept = 10) + \n  theme_minimal()\n\n\n\n\nNo zero in sight! The range of values included within this plausible range is much smaller and it still includes the true intercept (10). By reducing our uncertainty, we gained more confident insight into where the true intercept coefficient lies.\n\n\n\n\n\n\nTip\n\n\n\nThis is an important point to think about: we lost the zero because the true intercept is different from zero. Sometimes you will theorize that changes to a predictor are associated with changes to your outcome of interest. Sometimes the truth is that there is no meaningful relationship between the two. We can learn a lot from strong null results. You should always aim to reduce your uncertainty, even if it leads to a stronger zero."
  },
  {
    "objectID": "content/02-ols_basics.html#interpreting-your-regression-models",
    "href": "content/02-ols_basics.html#interpreting-your-regression-models",
    "title": "OLS Basics",
    "section": "Interpreting your regression models",
    "text": "Interpreting your regression models\nWe use regression models to provide empirical evidence of the relationship between our outcome of interest and a set of predictors that we theorize to be important drivers of that outcome.\nRegression models cannot prove causality. This can make them difficult or awkward to interpret. You need to be very careful with the language you use to describe your model to avoid being misleading.\nLet’s step through interpreting your model results. To do this, we will look at the (highly precise) relationship between an individual’s feelings towards President Obama and their dog ownership.\nThe National Election Survey (NES) asked respondents both their feelings towards President Obama (rating between zero and 100, with higher values indicating more support) and whether or not they own a dog. Let’s fit a linear regression model against their responses to these two questions:\n\nm &lt;- lm(obama_therm ~ own_dog, data = nes)\n\nmodelsummary(m,\n             statistic = \"[{conf.low}, {conf.high}]\",\n             stars = T,\n             coef_rename = c(\"own_dogYes\" = \"Owns a dog\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n74.305***\n\n\n\n[72.505, 76.104]\n\n\nOwns a dog\n−9.286***\n\n\n\n[−12.005, −6.566]\n\n\nNum.Obs.\n1927\n\n\nR2\n0.023\n\n\nR2 Adj.\n0.022\n\n\nAIC\n18606.2\n\n\nBIC\n18622.8\n\n\nLog.Lik.\n−9300.077\n\n\nRMSE\n30.18\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nOur regression model is as follows:\n\\[\nObama\\ thermometer = 74.3047 - 9.2858*Owns\\ a\\ dog + \\epsilon\n\\]\nIt is tempting to state that the estimated effect of dog ownership on an individual’s feelings towards Obama is a decrease of 9.28 points, on average and holding all else constant. However, this suggests an effect for which we really have no proof. According to Gelman, Hill, and Vehtari (2020), an effect is usually thought of as “the change associated with some treatment, or intervention” (pg. 84). Here, we would be suggesting that if we gave someone a dog, their support for Obama would drop by 9.28 points. That’s not actually what we have found. Rather, we have observed that, on average, people who were surveyed who had a dog had lower opinions of Obama than those who did not own a dog.\nAnother way of thinking about this is to acknowledge that regression models using observational data only allow us to make comparisons between our units of observation. Here, we can make comparisons between respondents to the NES. We cannot, however, use this model to make statements about changes to any individual respondent. The NES did not give someone a dog and then ask them how their feelings towards Obama changed. We cannot suggest that we have insight into the effects of such an intervention (as cute as it would be).\nWhen you are interpreting a regression model, you should use comparative language. For example, we can say that, under the fitted model, the average difference in feelings towards President Obama between a person with a dog and a person without a dog is 9.28 points.\n\n\n\n\n\n\nTip\n\n\n\nThis care with your language will become even more important when we move on to looking at multiple predictors.\n\n\nRemember: regression is just fancy averaging. This binary predictor makes this point easy to illustrate. For example, let’s look at how we can interpret the intercept. Individuals who do not own a dog (own_dog \\(= 0\\)) are predicted to report feeling 74.3 points towards President Obama, on average and holding all else constant.\nWhere did this come from, you may ask? It’s just fancy averaging!\n\navg_responses &lt;- nes |&gt; \n  drop_na(own_dog) |&gt; \n  group_by(own_dog) |&gt; \n  summarise(avg_obama_therm = mean(obama_therm, na.rm = T))\n\navg_responses\n\n# A tibble: 2 × 2\n  own_dog avg_obama_therm\n  &lt;fct&gt;             &lt;dbl&gt;\n1 No                 74.3\n2 Yes                65.0\n\n\nThere it is! It’s just the average response provided by those who do not own a dog. How cool is that?!\nYou may also have noticed that the difference between the average response provided by dog owners and non-dog owners is roughly 9 points. Hmm…\n\nmutate(avg_responses, diff = avg_obama_therm - lag(avg_obama_therm))\n\n# A tibble: 2 × 3\n  own_dog avg_obama_therm  diff\n  &lt;fct&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 No                 74.3 NA   \n2 Yes                65.0 -9.29\n\n\nThat’s our coefficient estimate! It’s just the difference in the average response provided by dog owners and non-dog owners.\nThis is the beauty of linear regression: we can gain so much insight into the important predictors of the things that we really care about by applying simple and clear statistical processes to our data."
  },
  {
    "objectID": "content/02-ols_basics.html#making-predictions-using-your-regression-model",
    "href": "content/02-ols_basics.html#making-predictions-using-your-regression-model",
    "title": "OLS Basics",
    "section": "Making predictions using your regression model",
    "text": "Making predictions using your regression model\nThis leads us neatly to another great use for our regression model: predictions. We can use our model to predict our outcome of interest.\nFor example, imagine that I pulled someone randomly from the US voting population and asked them their feelings towards President Obama on a 100-point scale. What would be your best guess of their response?\nWe have access to the NES, which is a representative sample of the US voting population. In other words, they pulled 5,916 people randomly from the US voting population and asked them this very question. We can look at the first 10 respondents’ answers:\n\nhead(nes$obama_therm, n = 10)\n\n [1]  15 100  70  30  70  45  50  60  15 100\n\n\nImagine that the only information I provide to you is these 5,916 individuals’ responses. Your educated best guess may then be the average of their response:\n\nmean(nes$obama_therm, na.rm = T)\n\n[1] 60.74377\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nIt’s just fancy averaging!\n\nlm(obama_therm ~ 1, data = nes)\n\n\nCall:\nlm(formula = obama_therm ~ 1, data = nes)\n\nCoefficients:\n(Intercept)  \n      60.74  \n\n\n\n\n\nNow I might ask you what other piece of information you would like to know about this random individual that might improve your guess. You may respond that you would like to know whether or not the random individual identifies as a Democrat. You suspect that Democrats will have warmer feelings towards Obama than non-Democrats.\nYou adopt the same approach (your best guess is the average response), but you can now look at Democrats and non-Democrats separately.\n\nobama_therm_dem &lt;- nes |&gt; \n  group_by(dem) |&gt; \n  summarise(avg_obama_therm = mean(obama_therm, na.rm = T))\n\nobama_therm_dem\n\n# A tibble: 3 × 2\n    dem avg_obama_therm\n  &lt;dbl&gt;           &lt;dbl&gt;\n1     0            44.2\n2     1            85.3\n3    NA            65.7\n\n\nNow your best guess may be 85.31 if the random person identifies as a Democrat and 44.24 if they do not.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIt’s just fancy averaging!\n\nlm(obama_therm ~ dem, data = nes)\n\n\nCall:\nlm(formula = obama_therm ~ dem, data = nes)\n\nCoefficients:\n(Intercept)          dem  \n      44.24        41.06  \n\n\nRemember, the coefficient estimate for dem is the estimated difference between individuals who do not identify as Democrats (dem \\(= 0\\)) and individuals who identify as Democrats (dem \\(= 1\\)). In other words, add those two values together and you will get the average response provided by Democrats.\n\n\n\nHave you improved your guess? How might we evaluate this? One approach would be to look at how far our predicted values were from the observed values.\nUsing our most simple approach, we will always predict the average feeling thermometer value provided by respondents to the NES. Using our slightly more sophisticated approach, we will predict the average response provided by Democrats if the individual is a Democrat, or the average response provided by non-Democrats if they do not identify as a Democrat. Let’s map those predictions against the actual responses provided by NES participants:\n\npred &lt;- nes |&gt; \n  transmute(dem, \n            obama_therm,\n            pred_simple = mean(nes$obama_therm, na.rm = T)) |&gt; \n  left_join(obama_therm_dem) |&gt; \n  rename(pred_party_id = avg_obama_therm)\n\npred |&gt; \n  select(dem, obama_therm, pred_simple, pred_party_id) |&gt; \n  head()\n\n  dem obama_therm pred_simple pred_party_id\n1   0          15    60.74377      44.24474\n2   1         100    60.74377      85.30587\n3   0          70    60.74377      44.24474\n4   1          30    60.74377      85.30587\n5   0          70    60.74377      44.24474\n6   0          45    60.74377      44.24474\n\n\nHere, we have stored our simple prediction in the pred_simple variable and our approach that accounts for party identification in the pred_party_id column.\nHow does each approach perform?\n\npred |&gt; \n  mutate(resid_simple = pred_simple - obama_therm,\n         resid_party_id = pred_party_id - obama_therm) |&gt; \n  summarise(r_2_simple = sum(resid_simple^2, na.rm = T),\n            r_2_party_id = sum(resid_party_id^2, na.rm = T))\n\n  r_2_simple r_2_party_id\n1    6582137      4364116\n\n\nThe simple approach predicts values further from the observed values than our approach that accounts for party ID. Great! It is worthwhile asking our random individual to which party they belong.\nSoon, we will look at including more than one predictor in our models. This will help us improve our predictive power even further."
  },
  {
    "objectID": "content/04-model_specification.html",
    "href": "content/04-model_specification.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "You will need to install a new package that I built for this class. Because it is not published on CRAN, you need to download it from Github directly. To do this, you also need devtools.\nRun the following to install these packages:\n\ninstall.packages(\"devtools\")\n\ndevtools::install_github(\"hgoers/polisciols\")\n\nYou then need to load all the relevant packages into your current R session:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(janitor)\nlibrary(ggdist)\nlibrary(polisciols)"
  },
  {
    "objectID": "content/04-model_specification.html#set-up",
    "href": "content/04-model_specification.html#set-up",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "You will need to install a new package that I built for this class. Because it is not published on CRAN, you need to download it from Github directly. To do this, you also need devtools.\nRun the following to install these packages:\n\ninstall.packages(\"devtools\")\n\ndevtools::install_github(\"hgoers/polisciols\")\n\nYou then need to load all the relevant packages into your current R session:\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(janitor)\nlibrary(ggdist)\nlibrary(polisciols)"
  },
  {
    "objectID": "content/04-model_specification.html#the-economic-benefits-of-justice",
    "href": "content/04-model_specification.html#the-economic-benefits-of-justice",
    "title": "Multiple Linear Regression",
    "section": "The economic benefits of justice",
    "text": "The economic benefits of justice\nAppel and Loyle (2012) (two wonderful UMD alumni) explore the determinants of foreign direct investment (FDI) flows into and out of post-conflict states. States that are emerging from civil war often have an acute need for foreign and stable sources of capital. However, multinational corporations and other foreign commercial actors are likely to view post-conflict states as high risk countries in which to invest: the risk of a return to violence and instability is often high in the immediate aftermath of a civil war. Understanding this, leaders of post-conflict states often attempt to decrease this perceived risk.\nAppel and Loyle argue that leaders can successfully do this by establishing post-conflict justice (PCJ) institutions. These institutions impose both domestic and reputational costs on post-conflict leaders. These costs allow leaders to signal their commitment to minimizing the risk of a return to violence and instability to foreign commercial actors. This is a great paper and I highly encourage you to take a look at their argument in detail.\nTheir argument focuses on leaders’ attempts to change foreign commercial actors’ perceptions of the risk of a return to violence. We cannot directly observe these commercial actors’ perceptions. However, we can observe the outcome of these perceptions: investment. If Appel and Loyle’s argument is correct, we should observe higher levels of FDI investment coming into post-conflict states that establish PCJ institutions compared to those that do not, on average and holding all else constant.\nAppel and Loyle find strong evidence of this. We will replicate and modify this empirical work. In doing so, we will become more familiar with the underlying mechanics of multiple linear regression and strengthen our ability to interpret these models.\nLet’s get started!"
  },
  {
    "objectID": "content/04-model_specification.html#net-fdi-inflows-to-post-conflict-states",
    "href": "content/04-model_specification.html#net-fdi-inflows-to-post-conflict-states",
    "title": "Multiple Linear Regression",
    "section": "Net FDI inflows to post-conflict states",
    "text": "Net FDI inflows to post-conflict states\nAppel and Loyle provide us with data on 95 different post-conflict states. These include all states that had internal armed conflicts, including internationalized conflicts, that resulted in at least 25 battle-related deaths and were settled between 1970 and 2001.\nWe can access these data through polisciols::ebj:\n\nhead(ebj)\n\n   id ccode      country_name              pcj net_fdi_inflows gdp_per_capita\n1  71    41             Haiti  No institutions         -9.8000       1182.498\n2  71    41             Haiti  No institutions          6.6000       1088.680\n3 154    52 Trinidad & Tobago  No institutions        510.1589       7742.736\n4 102    70            Mexico  No institutions       -340.6904       6894.704\n5 102    70            Mexico  No institutions       6460.7998       7780.053\n6  67    90         Guatemala PCJ institutions        431.3800       3061.873\n           gdp gdp_per_capita_growth ex_rate_fluc cap_account_openness labor\n1   8407079981            -2.1324685     0.000000           -0.7681904  68.5\n2   8055393763           -14.8832922     1.776603           -0.0871520  67.9\n3   9542938026             1.9370972     0.000000           -1.1305820  55.2\n4 628418000000            -7.8634830     1.978028            1.1804080  59.8\n5 730752000000             5.2345648     1.129686            1.1804080  61.3\n6  31339424077             0.6277104     1.046489            1.2642760  63.1\n  f_life_exp polity2 pol_constraints conflict_duration     damage\n1   55.02233       7            0.00                 1   0.000000\n2   56.09750      -7            0.00                 1  12.855902\n3   72.61483       9            0.84                 1  -2.787351\n4   74.67104       4            0.39                 1   0.000000\n5   75.23493       6            0.39                 1  -8.949999\n6   67.46067       8            0.43                31 -43.714600\n  peace_agreement    victory      cold_war\n1    No agreement    Victory Post-Cold War\n2    No agreement    Victory Post-Cold War\n3    No agreement    Victory Post-Cold War\n4 Peace agreement No victory Post-Cold War\n5    No agreement No victory Post-Cold War\n6 Peace agreement No victory Post-Cold War\n\n\nThey focus on the 10-year period immediately following the conflict’s conclusion. This is the period in which we would expect foreign commercial actors to perceive the risk of a return to violence and instability to be greatest and; therefore, the period in which leaders’ attempts to quash these perceptions to be most relevant.\nTherefore, each row in this data set represents a single post-conflict state. The data is generally a summary of the 10-years post-conflict. For example net_fdi_inflows provides us with the total net FDI inflows each post-conflict state received in that 10-year period.\n\n\n\n\n\n\nTip\n\n\n\nYou can learn more about each variable using the following command:\n\n?ebj\n\n\n\nTheir outcome of interest is net FDI inflows. Remember, if their argument is correct we would expect post-conflict states that have established a PCJ institution to have higher net FDI inflows than than states that did not. Let’s take a look at those net inflows:\n\nsummary(ebj$net_fdi_inflows)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-1858.914    -0.572    38.290   759.146   408.250 24836.787 \n\n\n\nggplot(ebj, aes(x = net_fdi_inflows)) + \n  geom_histogram() + \n  theme_minimal() + \n  scale_x_continuous(labels = scales::dollar)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere appears to be a clear outlier: a net FDI inflow of $24,836.79 million for Russia. You can see that this is pulling up the average net FDI inflows well above the median inflow across our group of post-conflict states. We will keep this in because this is not the focus of today’s session, but I would encourage you to explore whether these findings are sensitive to its inclusion.\n\n\nWe learn from this that some states have greater FDI outflows than inflows (resulting in negative net FDI inflows). Indonesia had the greatest negative inflow, with net inflows of -$1,858.91 million.\nOther states received greater foreign investments than they invested elsewhere, resulting in positive net FDI inflows. In fact, on average, post-conflict states received more inflows than outflows."
  },
  {
    "objectID": "content/04-model_specification.html#pcj-institutions",
    "href": "content/04-model_specification.html#pcj-institutions",
    "title": "Multiple Linear Regression",
    "section": "PCJ institutions",
    "text": "PCJ institutions\nAppel and Loyle focus on whether states that have PCJ institutions successfully attract greater net FDI inflows than those that do not. Therefore, their main explanatory variable is the existance of PCJ institutions.\nThis variable, pcj, indicates whether the the state established a PCJ institution within five years following the end of the conflict. What proportion of states did this?\n\ntabyl(ebj, pcj)\n\n              pcj  n   percent\n  No institutions 77 0.8105263\n PCJ institutions 18 0.1894737\n\n\nThe majority of post-conflict states did not have PCJ institutions."
  },
  {
    "objectID": "content/04-model_specification.html#relationship-between-pcj-institutions-and-net-fdi-inflows",
    "href": "content/04-model_specification.html#relationship-between-pcj-institutions-and-net-fdi-inflows",
    "title": "Multiple Linear Regression",
    "section": "Relationship between PCJ institutions and net FDI inflows",
    "text": "Relationship between PCJ institutions and net FDI inflows\nOf the states that do have a PCJ institution, do they tend to receive higher net FDI inflows than their less reconciliatory counterparts?\n\nebj |&gt; \n  group_by(pcj) |&gt; \n  summarise(avg_net_fdi = mean(net_fdi_inflows))\n\n# A tibble: 2 × 2\n  pcj              avg_net_fdi\n  &lt;fct&gt;                  &lt;dbl&gt;\n1 No institutions         425.\n2 PCJ institutions       2189.\n\n\nYes! States that established a PCJ institution received higher net FDI inflows, on average, than those states that did not. This difference looks large! It’s $1,763.52 million in net inflows!\nBut what are the chances this difference is simply the product of chance? We can formally test this question using linear regression:\n\nm &lt;- lm(net_fdi_inflows ~ pcj, data = ebj)\n\nmodelsummary(m, \n             coef_rename = c(\"pcjPCJ institutions\" = \"PCJ institutions established\"),\n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n425.006\n\n\n\n(323.874)\n\n\nPCJ institutions established\n1763.517*\n\n\n\n(744.049)\n\n\nNum.Obs.\n95\n\n\nR2\n0.057\n\n\nR2 Adj.\n0.047\n\n\nAIC\n1784.5\n\n\nBIC\n1792.2\n\n\nLog.Lik.\n−889.253\n\n\nF\n5.618\n\n\nRMSE\n2811.91\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nAccording to this model, what is the average predicted net FDI inflows for countries that did not establish PCJ institution?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPost-conflict states that did not establish a PCJ institution received, on average, net FDI inflows of $425.01 million in the 10-year period after conflict.\nTo get this, I looked at the intercept coefficient which tells us the average predicted value of our outcome variable when all explanatory variables are equal to zero or their baseline category.\n\n\n\nAccording to this model, what is the average predicted net FDI inflows for countries that did establish PCJ institution?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn contrast, states that did establish PCJ institution received, on average, net FDI inflows of $2,188.52 million.\nIn other words:\n\\[\nAverage\\ net\\ FDI\\ inflows = \\beta_0 + \\beta_1 Institution\\ established + \\epsilon\n\\]\nWhen an institution was established (i.e. \\(Institution\\ established = 1\\)):\n\\[\nAverage\\ net\\ FDI\\ inflows = 425.01 + 1763.52 * 1 + \\epsilon \\\\\n\\]\n\\[\nAverage\\ net\\ FDI\\ inflows = 2188.52\n\\]\n\n\n\n\nSubstantive significance\nThis finding is substantively significant. These countries are recovering from conflict: their economies are really weak. Leaders are often very keen to find stable and reliable sources of funding to promote and strengthen their battered economies. This average difference of $1,763.52 million is; therefore, likely to incentivize this policy.\nFurther, if we look at the range of plausible values of the coefficient on the existence of a PCJ institution (i.e. the confidence interval) we can see states that did not establish a commission plausibly receive no net FDI inflows in this post-conflict period. In fact, it is plausible that investment leaves their economies: this net inflow can be negative. On the other hand, states that do establish a commission enjoy, on average, billions in net FDI inflows. For an economy struggling to establish indigenous production in a post-conflict setting, this can be critical to their long-term economic development. Again, this is further proof of the substantive significance of this relationship.\n\n\n\n\n\n\nTip\n\n\n\nVisualizing the full range of plausible coefficients can be a great way to communicate your findings:\n\nplot_predictions(m, condition = \"pcj\") + \n  geom_hline(yintercept = 0, colour = \"grey\") + \n  labs(x = NULL,\n       y = \"Net FDI inflows (USD million)\") + \n  scale_y_continuous(labels = scales::dollar) + \n  theme_minimal()"
  },
  {
    "objectID": "content/04-model_specification.html#but-what-about-other-factors-that-shape-net-fdi-inflows",
    "href": "content/04-model_specification.html#but-what-about-other-factors-that-shape-net-fdi-inflows",
    "title": "Multiple Linear Regression",
    "section": "But what about other factors that shape net FDI inflows?",
    "text": "But what about other factors that shape net FDI inflows?\nOne of Appel and Loyle’s major contributions is their critique of approaches to estimating net FDI inflows that focus only on economic factors. They argue that there are several political factors that are significant determinants of other countries’ and foreign firms’ willingness to invest in these war-torn countries.\nThis critique is very valid, but it suggests that there are many different things influencing this outcome of interest, including economic factors. We have only looked at the political! The economists might turn around and accuse us of doing the very thing Appel and Loyle accused them of!\nLet’s add some of those economic factors into our model. We will start with an intuitive one: individuals’ economic wealth (measured as GDP per capita). I expect that foreign firms will be more willing to invest larger sums of money into economies with richer citizens. These citizens will be more willing and able to purchase the goods and services provided by those firms.\nTherefore, I hypothesize that the greater a state’s GDP per capita, the larger its net FDI inflows. I expect this to be the case regardless of whether the state has established a PCJ institution. Let’s test this claim:\n\nm &lt;- lm(net_fdi_inflows ~ pcj + gdp_per_capita, data = ebj)\n\nmodelsummary(m, \n             coef_rename = c(\"pcjPCJ institutions\" = \"PCJ institutions established\",\n                             \"gdp_per_capita\" = \"GDP per capita (current USD)\"),\n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−319.173\n\n\n\n(396.052)\n\n\nPCJ institutions established\n1673.288*\n\n\n\n(714.015)\n\n\nGDP per capita (current USD)\n0.318**\n\n\n\n(0.105)\n\n\nNum.Obs.\n95\n\n\nR2\n0.142\n\n\nR2 Adj.\n0.124\n\n\nAIC\n1777.5\n\n\nBIC\n1787.7\n\n\nLog.Lik.\n−884.742\n\n\nF\n7.638\n\n\nRMSE\n2681.52\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nWe continue to find a positive and statistically significant relationship between net FDI inflows and the establishment of a PCJ institution. This model also accounts for the association between the state’s GDP per capita and those inflows.\nThe intercept here is not informative on its own. It tells us the estimated average net FDI inflows for states that do not have PCJ institutions and in which citizens had a GDP per capita of $0. Although the majority of states did not establish an institution, there are no states in the world that have a GDP per capita of $0.\nLet’s instead focus on the other coefficients. We find that states that established a PCJ institution received, on average, net FDI inflows of $1,673.29 million more than states that did not in the 10-year period after conflict. This is a slightly smaller estimated difference than we found in the model that did not account for individuals’ average wealth, but it remains large.\nWe also find that an increase in the GDP per capita of a state of $1,000 is associated with an increase of $318.45 million in net FDI inflow. This is consistent with our expectations that, holding all else constant, a country with a richer population is a more attractive investment destination than a country that has poorer citizens.\n\nUsing this richer model\nWe now have a richer understanding of the determinants of net FDI inflows to post-conflict countries. We have accounted for both economic and political determinants of those flows. Although it is often useful to look at the estimated relationship of each of those variables individually (as we did just above), we often learn more by looking at the whole model in context.\nAs usual, one of the easiest ways to communicate this is through a visualization. Let’s look at the predicted net FDI inflows for post-conflict countries that established and did not establish PCJ institutions across a range of plausible GDP per capita values:\n\nplot_predictions(m, condition = c(\"gdp_per_capita\", \"pcj\")) + \n  labs(x = \"GDP per capita (USD)\",\n       y = \"Net FDI inflows (USD million)\") + \n  scale_y_continuous(labels = scales::dollar) + \n  scale_x_continuous(labels = scales::dollar) + \n  theme_minimal()\n\n\n\n\nWe can see the positive relationship between GDP per capita and net FDI inflows and that across any given value of GDP per capita states that have established a PCJ institution start and stay at a higher predicted inflow compared to those that did not.\n\n\nThe full model\nAppel and Loyle control for many more economic and political factors shaping net FDI inflows. Remember, they are arguing that the extensive literature that looks at the determinants of FDI flows to post-conflict states failed to account for this important political factor. However, that same literature did a very good job of identifying the economic factors, including countries’ economic development, size, and growth rates, that shape this outcome. They don’t dispute that these factors are also important, they just argue that we should also think about the role PCJ institutions play in shaping foreign firms’ beliefs about the risk of returning to violence.\nSo, let’s account for these other factors:\n\nm &lt;- lm(net_fdi_inflows ~ pcj + gdp_per_capita + gdp + gdp_per_capita_growth + \n          cap_account_openness + ex_rate_fluc + labor + f_life_exp + \n          pol_constraints + polity2 + damage + conflict_duration + peace_agreement + \n          victory + cold_war, \n        data = ebj)\n\nmodelsummary(m, \n             coef_rename = c(\"pcjPCJ institutions\" = \"PCJ institutions established\",\n                             \"gdp_per_capita\" = \"GDP per capita (current USD)\",\n                             \"gdp\" = \"GDP (current USD)\",\n                             \"gdp_per_capita_growth\" = \"GDP per capita growth rate (%)\",\n                             \"cap_account_openness\" = \"Capital account openness\",\n                             \"ex_rate_fluc\" = \"Exchange rate fluctuation\",\n                             \"labor\" = \"Labor force participation (%)\",\n                             \"f_life_exp\" = \"Average female life expectancy (years)\",\n                             \"pol_constraints\" = \"Political constraints\",\n                             \"polity2\" = \"Regime type (Polity score)\",\n                             \"damage\" = \"Pre-conflict GDP lost\",\n                             \"conflict_duration\" = \"Conflict duration (years)\",\n                             \"peace_agreementPeace agreement\" = \"Peace agreement\",\n                             \"victoryVictory\" = \"Decisive victory\",\n                             \"cold_warCold War\" = \"Cold War\"),\n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−1278.322\n\n\n\n(2852.294)\n\n\nPCJ institutions established\n1960.282**\n\n\n\n(702.992)\n\n\nGDP per capita (current USD)\n−0.111\n\n\n\n(0.133)\n\n\nGDP (current USD)\n0.000***\n\n\n\n(0.000)\n\n\nGDP per capita growth rate (%)\n37.400\n\n\n\n(23.239)\n\n\nCapital account openness\n198.823\n\n\n\n(201.590)\n\n\nExchange rate fluctuation\n−42.516**\n\n\n\n(13.888)\n\n\nLabor force participation (%)\n9.844\n\n\n\n(25.528)\n\n\nAverage female life expectancy (years)\n3.475\n\n\n\n(32.993)\n\n\nPolitical constraints\n2557.954+\n\n\n\n(1459.599)\n\n\nRegime type (Polity score)\n−90.169\n\n\n\n(56.309)\n\n\nPre-conflict GDP lost\n28.379**\n\n\n\n(10.242)\n\n\nConflict duration (years)\n0.811\n\n\n\n(35.543)\n\n\nPeace agreement\n−1215.137\n\n\n\n(793.826)\n\n\nDecisive victory\n−33.969\n\n\n\n(650.725)\n\n\nCold War\n81.531\n\n\n\n(654.092)\n\n\nNum.Obs.\n95\n\n\nR2\n0.514\n\n\nR2 Adj.\n0.422\n\n\nAIC\n1749.5\n\n\nBIC\n1792.9\n\n\nLog.Lik.\n−857.752\n\n\nRMSE\n2018.34\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nEven when we account for all the political, economic, and conflict-related factors that the literature previously identified to be important, we still find that the existence of PCJ institution substantively and statistically significantly shapes the net FDI inflows of post-conflict states."
  },
  {
    "objectID": "content/04-model_specification.html#including-categorical-variables-with-multiple-categories",
    "href": "content/04-model_specification.html#including-categorical-variables-with-multiple-categories",
    "title": "Multiple Linear Regression",
    "section": "Including categorical variables with multiple categories",
    "text": "Including categorical variables with multiple categories\nAppel and Loyle look at a number of political factors driving net FDI inflows to post-conflict states. They include a measure of regime type: the country’s Polity score. This score measures a state’s regime type along a 21-point scale from -10 (perfect autocracy) to 10 (perfect democracy). Broadly speaking, political scientists have usefully broken this spectrum down into three regime types: democracies, hybrid regimes, and autocracies.\nLet’s modify their measure of regime type to reflect these broad categories, instead of treating it as a continuous variable:\n\nebj &lt;- ebj |&gt; \n  mutate(regime_type = case_when(polity2 &gt; 5 ~ \"Democracy\",\n                                 polity2 &lt; -5 ~ \"Autocracy\",\n                                 TRUE ~ \"Hybrid regime\"),\n         regime_type = factor(regime_type, levels = c(\"Autocracy\",\n                                                      \"Hybrid regime\",\n                                                      \"Democracy\")))\n\nI have a theoretical reason to do this. I suspect that there is not a clear linear relationship between investors’ confidence in a post-conflict state and its regime type when we treat regime type as a continuous spectrum moving linearly from autocracies to democracies. In other words, I don’t think that moving one Polity score away from being an autocracy to being a democracy would have a consistent effect on investor confidence (and; therefore, net FDI inflows). Appel and Loyle’s model agrees with me: the regime type variable is not statistically significant.\nRather, I suspect that strong democracies and strong autocracies provide the political stability required to comfort foreign investors. These investors believe that the strong control democrats and autocrats have over their citizens and institutions reduces the risk that the country will re-enter into conflict. However, hybrid regimes do not tend to have this level of control. Investors are; therefore, less likely to invest in post-conflict countries with hybrid regimes.\nLet’s test this!\nWe now have a categorical variable with three categories: democracy, hybrid regime, and autocracy. We have thus far largely dealt with binary categorical variables (voted or not, Southern or not, female or not). How do we use and interpret multiple categorical variables in regression analysis?\nHappily, the intuition remains the same as with our binary categorical variables. We hold one category out as our baseline category and then compare the associated effects of the other categories to this one.\nLet’s step through that using a stripped back version of our model:\n\nm &lt;- lm(net_fdi_inflows ~ pcj + regime_type, data = ebj)\n\nmodelsummary(m, \n             coef_rename = c(\"pcjPCJ institutions\" = \"PCJ institutions established\",\n                             \"regime_typeDemocracy\" = \"Democracy\",\n                             \"regime_typeHybrid regime\" = \"Hybrid\"),\n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n214.555\n\n\n\n(497.984)\n\n\nPCJ institutions established\n1816.169*\n\n\n\n(764.093)\n\n\nHybrid\n553.893\n\n\n\n(667.644)\n\n\nDemocracy\n−148.124\n\n\n\n(809.623)\n\n\nNum.Obs.\n95\n\n\nR2\n0.068\n\n\nR2 Adj.\n0.037\n\n\nAIC\n1787.4\n\n\nBIC\n1800.1\n\n\nLog.Lik.\n−888.688\n\n\nF\n2.217\n\n\nRMSE\n2795.25\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nYou’ll note that autocracies are missing from our regression table. This is because they are being held out as our baseline category. Their effect on net FDI inflows is captured by the intercept coefficient.\n\n\n\n\n\n\nTip\n\n\n\nWe often say that the intercept coefficient represents the predicted average value of our outcome of interest when all independent variables are set to zero. It might be useful for you to think of your baseline category as taking on the value zero. For example, we can think of autocracy = 0.\n\n\nOur model suggests that autocracies (regime_type = \"Autocracy\") that have not established a commission (pcj = \"No institutions\") have a predicted average net FDI inflow of $214.55 million.\nThe coefficients on democracies and hybrid regimes need to be interpreted in relation to autocracies (their baseline category). From our model, we can see that the coefficient for democracies is negative and the coefficient for hybrid regimes is positive. That means that, on average, democracies tend to receive less net FDI inflows than autocracies and hybrid regimes tend to receive more net FDI inflows than autocracies.\n\nPredictions with multiple categorical variables\nUsing our model, what do we predict to be the net FDI inflows for democracies, autocracies, and hybrid regimes that either have a PCJ institutions or do not?\nFirst, let’s create a table with each possible combination of these two variables of interest:\n\nnew_data &lt;- tibble(pcj = factor(c(\"No institutions\", \n                                  \"PCJ institutions\"))) |&gt; \n  cross_join(\n    tibble(regime_type = factor(c(\"Autocracy\", \"Democracy\", \"Hybrid regime\")))\n  )\n\nnew_data\n\n# A tibble: 6 × 2\n  pcj              regime_type  \n  &lt;fct&gt;            &lt;fct&gt;        \n1 No institutions  Autocracy    \n2 No institutions  Democracy    \n3 No institutions  Hybrid regime\n4 PCJ institutions Autocracy    \n5 PCJ institutions Democracy    \n6 PCJ institutions Hybrid regime\n\n\nThen we can use our model to predict what we expect a hypothetical state with each of these combinations of characteristics to receive in net FDI inflows:\n\npred &lt;- augment(m, newdata = new_data)\n\npred\n\n# A tibble: 6 × 3\n  pcj              regime_type   .fitted\n  &lt;fct&gt;            &lt;fct&gt;           &lt;dbl&gt;\n1 No institutions  Autocracy       215. \n2 No institutions  Democracy        66.4\n3 No institutions  Hybrid regime   768. \n4 PCJ institutions Autocracy      2031. \n5 PCJ institutions Democracy      1883. \n6 PCJ institutions Hybrid regime  2585. \n\n\nThat’s a bit unweildy. Let’s visualize it!\n\nggplot(pred, aes(x = .fitted, y = pcj, colour = regime_type)) + \n  geom_point(size = 5) + \n  theme_minimal() + \n  labs(x = \"Predicted net FDI inflows (USD, million)\",\n       y = NULL, \n       colour = \"Regime type\")\n\n\n\n\nWe can clearly see that states that established PCJ institutions received, on average, larger net FDI inflows than states that did not, no matter their regime type. Further and completely counter to my hypothesis, hybrid regimes have, on average, the highest net FDI inflows compared to democracies and autocracies even when we account for whether the state has a PCJ institution.\n\n\n\n\n\n\nNote\n\n\n\nI’m not too worried: none of these coefficients are anywhere close to being statistically significant. I suspect that there is a more complex relationship underlying commercial actors’ beliefs about the stability of hybrid regimes, democracies, and autocracies and its effect on net investment flows. But I hope this serves as a good illustration of how we can use multiple categorical variables in our analyses."
  },
  {
    "objectID": "content/06-binary_outcomes.html",
    "href": "content/06-binary_outcomes.html",
    "title": "Binary outcomes",
    "section": "",
    "text": "Warning\n\n\n\nTHIS IS A DRAFT.\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(scales)\nlibrary(broom)\nlibrary(marginaleffects)\n\nset.seed(1234)"
  },
  {
    "objectID": "content/06-binary_outcomes.html#introduction-to-binary-outcomes",
    "href": "content/06-binary_outcomes.html#introduction-to-binary-outcomes",
    "title": "Binary outcomes",
    "section": "Introduction to binary outcomes",
    "text": "Introduction to binary outcomes\nWe often want to better understand binary outcomes: voted, or not; at war, or not; signed a treaty, or not.\nImagine that if you are over six feet tall, there is a 80% chance that you will score a three-point field goal on any given attempt. In other words, people over six feet tall score a three-pointer 95% of the time they attempt one. Imagine also that if you are under six feet tall, your luck is reversed. There is only a 5% chance that you score a three-pointer when you attempt one.\nLet’s make some data to reflect this world. In 2021 (the latest data I get from a very cursory Google search)1, the tallest player in the WNBA is Brittney Griner who is 6.9 feet (206 cm) tall. The shortest player, Leilani Mitchell, is 5.5 feet (165 cm) tall. We will use these women as our plausible range of players.\nLet’s take 1,000 hypothetical randomly-selected players ranging from 165 cms to 206 cms tall and ask them to take a shot from the three-point score line. We will record whether or not they score.\n\nplayers_height &lt;- sample(165:206, size = 1000, replace = T)\n\nRemember, if they are taller than 6 feet (or 183 cms), they will score 95% of the time. If they are less than 6 feet, they will only score 5% of the time.\n\nthree_pointers &lt;- tibble(\n  height = players_height,\n  scored = if_else(height &lt; 183,\n                   sample(0:1, 1000, replace = T, prob = c(0.95, 0.05)),\n                   sample(0:1, 1000, replace = T, prob = c(0.05, 0.95)))\n)\n\nLet’s look at how they went:\n\nggplot(three_pointers, aes(x = height, y = scored)) + \n  geom_vline(xintercept = 182, colour = \"grey\") + \n  geom_jitter(alpha = 0.5,\n              height = 0.1) + \n  theme_minimal() + \n  labs(x = \"Player's height (cm)\",\n       y = \"Scored three-pointer\") + \n  scale_y_continuous(n.breaks = 2)\n\n\n\n\nSo, it looks like some of our taller plays (those above the grey line) were unlucky and missed. Happily some of our shorter players beat the odds!\nIf I asked you to predict whether or not a player would score a three-pointer and you knew this fact that taller players scored 95% of the time whilst shorter players only scored 5% of the time, you might follow up and ask how tall the player in question is. It seems to be an important predictor of her ability to score!\nIn other words, if I wanted to build a model predicting the likelihood any given player would score a three-pointer, I would include in that model the player’s height as a variable.\nLet’s do that!"
  },
  {
    "objectID": "content/06-binary_outcomes.html#linear-probability-model-lmp",
    "href": "content/06-binary_outcomes.html#linear-probability-model-lmp",
    "title": "Binary outcomes",
    "section": "Linear probability model (LMP)",
    "text": "Linear probability model (LMP)\nLet’s start simply by drawing a straight line between these two clusters of points. As with all linear regression models, we want to draw the line that minimizes the distance between itself and all of the observed points.\n\nggplot(three_pointers, aes(x = height, y = scored)) + \n  geom_jitter(alpha = 0.5,\n              height = 0.1) + \n  geom_smooth(method = \"lm\", se = F) +\n  theme_minimal() + \n  labs(x = \"Player's height (cm)\",\n       y = \"Probability of scoring\") + \n  scale_y_continuous(label = percent, limits = c(0, 1))\n\n\n\n\n\nm_lpm &lt;- lm(scored ~ height, data = three_pointers)\n\nmodelsummary(m_lpm, coef_rename = c(\"height\" = \"Height (cm)\"), stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−5.269***\n\n\n\n(0.153)\n\n\nHeight (cm)\n0.032***\n\n\n\n(0.001)\n\n\nNum.Obs.\n1000\n\n\nR2\n0.596\n\n\nR2 Adj.\n0.595\n\n\nAIC\n507.4\n\n\nBIC\n522.1\n\n\nLog.Lik.\n−250.693\n\n\nF\n1471.377\n\n\nRMSE\n0.31\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nAlthough this looks a lot like our familiar linear regression model, we interpret its estimates differently. Here, instead of estimating the value of the outcome variable, we are estimating the probability that the outcome variable will be equal to one (or that our player scores).\nSo, we have found that players that are one centimeter taller than their teammates are, on average, 3% more like to score a three-pointer than those shorter team mates.\nOur model predicts that a player who is 170 cms tall will score 9% of the time. It predicts that a player who is 10 cms taller will score 41% of the time. Finally, it predicts that a player who is another 10 cms taller will score 72% of the time. More succintly:\n\nplot_predictions(m_lpm, condition = \"height\") +\n  theme_minimal() + \n  labs(x = \"Player's height (cm)\",\n       y = \"Probability of scoring\") + \n  scale_y_continuous(label = percent, limits = c(0, 1))\n\n\n\n\nHmm, this doesn’t really reflect the hypothetical world we set up very well…\nWe said that if you were above a cut off point (6 feet) then you had an 95% chance of scoring. If you were below this height threshold, you only had a 5% chance of scoring. But this LPM assumes a linear (constant) increase across all heights.\nAlso, this model predicts probabilities of scoring that are more than 100% of the time and less than 0% if you are less than 170cm tall. That’s very dire odds for some current players…\nHappily, we can do better (…than economists)."
  },
  {
    "objectID": "content/06-binary_outcomes.html#latent-variable-approach",
    "href": "content/06-binary_outcomes.html#latent-variable-approach",
    "title": "Binary outcomes",
    "section": "Latent variable approach",
    "text": "Latent variable approach\nWhen we are dealing with binary outcomes, we can only observe 1 or 0, success or failure. This makes this whole prediction thing tricky!\nFor example, lots of political pundits will attempt to predict the outcome of this year’s US Presidential Election. They will say that they predict that either President Biden or Mr Trump will win.\nThose who build complex models to help them in this will likely put some percentage against their prediction. For example, someone might say that they predict that President Biden will hold office with an 85% chance. What they are saying is that, if you were able to run 100 different versions of the 2024 US Presidential Election, their model suggests that President Biden wins 85 of those 100 elections. That means that their model also predicts that Mr Trump will win 15 of those 100 elections.\nWe can only observe the outcome: Biden or Trump. We cannot observe this underlying probability: 85% Biden and 15% Trump. But we assume that underlying continuous relationship exists! This is logic behind the latent variable approach.\nGoing back to our basketball players: the latent variable approach assumes that some continuous linear relationship exists between our observed outcome, scored or did not score, and our independent variable: height. This continuous relationship is driven by an unobserved variable: \\(Z\\). Formally:\n\\[\nz_i = X_i\\beta + \\epsilon_i\n\\]\nThis is a linear model! In fact, it’s the linear model you have been working with for two semesters. Great!\nBut as usual, the interesting (and tricky) part of this model is that error term: \\(\\epsilon\\). What shape does it take? In a linear regression, we assume that it is normally distributed and centered at zero. When we have a binary outcome, we assume that it takes on a different shape. Which shape? Well, you get to pick your poison: logit or probit?\n\nLogistic regression\nLogistic regression is a favourite among international relations scholars. It has lots of very useful features.\nFirst, it is bounded by 0 and 1 (just like our outcome). Second, it allows for a varying impact of a change in \\(X\\) on the probability that our outcome is a success.\nFormally, the inverse logistic function is:\n\\[\nPr(Y = 1 | X) = logit^{-1}(X) = \\frac{e^X}{1 + e^X}\n\\]\nBut don’t worry too much about that. Let’s instead look at its shape:\n\ntibble(x = seq(-10, 10, by = 0.5),\n       y = plogis(x)) |&gt; \n  ggplot(aes(x = x, y = y)) + \n  geom_line() + \n  theme_minimal()\n\n\n\n\nThis is great! The outcome (probability of success) is bounded between 0 and 1 as all probabilities are. Further, our cut-off point (in this general demonstration, 0) is clearly defined.\nLet’s fit a logistic regression model against our basketball players’ scoring efforts:\n\nggplot(three_pointers, aes(x = height, y = scored)) + \n  geom_jitter(alpha = 0.5,\n              height = 0.1) + \n  geom_smooth(method = \"glm\", se = F, method.args = list(family = binomial(link = \"logit\"))) + \n  theme_minimal() + \n  labs(x = \"Player's height (cm)\",\n       y = \"Probability of scoring\") + \n  scale_y_continuous(label = percent, limits = c(0, 1))\n\n\n\n\nWe can fit this model using the following:\n\nm_lr &lt;- glm(scored ~ height, data = three_pointers, family = binomial(link = \"logit\"))\n\nmodelsummary(m_lr,\n             coef_rename = c(\"height\" = \"Height (cm)\"),\n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−50.744***\n\n\n\n(3.203)\n\n\nHeight (cm)\n0.279***\n\n\n\n(0.018)\n\n\nNum.Obs.\n1000\n\n\nAIC\n549.1\n\n\nBIC\n558.9\n\n\nLog.Lik.\n−272.543\n\n\nF\n252.357\n\n\nRMSE\n0.26\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\nInterpreting logistic regression coefficients\nThe coefficients presented above are log odds ratios. We can easily interpret their statistical significance and their sign. For example, we learn that as a player’s height increases, so too does her probability of scoring a three-pointer. We also know that the probability that we would pull a random sample from a population in which a player’s height has no bearing on her ability to score a three pointer is very small. Sadly, that’s about all that we can easily get from the regression table provided above.\nRemember that logit (and probit) models are simply transformed linear models. We take our linear model of the relationship between scoring a three pointer and a player’s height and reshape it to better predict probabilities (bound between 0 and 1, with varying effects of changes of height on probability of success). Similarly, we can reshape these coefficients to make them easier to interpret.\n\nOdds ratios\nThe regression coefficient provided above is a log odds ratio. Log-transformed variables are really hard to interpret. So, let’s get rid of that log. The opposite operation to taking the logarithm of a number is to take the exponential of it. To demonstrate:\n\ntibble(x = 1:5,\n       log_x = log(x),\n       exp_log_x = exp(log_x))\n\n# A tibble: 5 × 3\n      x log_x exp_log_x\n  &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1     1 0             1\n2     2 0.693         2\n3     3 1.10          3\n4     4 1.39          4\n5     5 1.61          5\n\n\nSo, exponentiating the log odd ratio will get us the odds ratio. The odds ratio is much easier to interpret. If the probability of success of an outcome is \\(p\\) and, therefore, the probability of failure is \\(1-p\\), then , the the odds of success is \\(\\frac{p}{1-p}\\). Now, dividing two odds by each other gives you their odds ratio.\nFor example, when the odds ratio is 1, the odds of success are the same as the odds of failure (\\(\\frac{0.5}{0.5} = 1\\)). When the odds ratio is greater than 1, the odds of success are greater than the odds of failure (for example, \\(\\frac{0.8}{0.2} = 4\\)). Where the odds ratio of success is four, the odds of success are four times higher than the odds of failure.\nGetting back to our model, the odds ratio that a player will score is:\n\\[\ne^{0.302} = 1.35\n\\]\nTherefore, if we increase a player’s height by one centimeter, the odds that she will score increases by 0.35 (or \\(1 - 1.35\\)).\nHappily, modelsummary::modelsummary(exponentiate = T) will help us out here:\n\nmodelsummary(m_lr,\n             exponentiate = T, \n             coef_rename = c(\"height\" = \"Height (cm)\"),\n             stars = T)\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n0.000***\n\n\n\n(0.000)\n\n\nHeight (cm)\n1.321***\n\n\n\n(0.023)\n\n\nNum.Obs.\n1000\n\n\nAIC\n549.1\n\n\nBIC\n558.9\n\n\nLog.Lik.\n−272.543\n\n\nF\n252.357\n\n\nRMSE\n0.26\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nAnd we can also get these results programmatically using broom::tidy():\n\ntidy(m_lr, exponentiate = T)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 9.16e-23    3.20       -15.8 1.62e-56\n2 height      1.32e+ 0    0.0175      15.9 7.95e-57\n\n\n\n\nPredicted probabilities\nOdds ratios are certainly easier to interpret than log odds ratios, but they are still a little awkward. It is easier again to discuss the effects of changing our independent variables of interest in terms of probabilities, rather than odds.\nRemember, the odds ratio is simply the odds of success divided by the odds of failure:\n\\[\nOR = \\frac{\\frac{p}{1-p}}{\\frac{1-p}{p}}\n\\]\nWe are interested in getting the odds of success (\\(p\\)). To get this, we do the following:\n\\[\np = \\frac{OR}{1+OR}\n\\]\nOr, more generally:\n\\[\nPr(Y = 1 | X) = \\frac{e^{X\\beta}}{1 + e^{X\\beta}}\n\\]\nOr, more specifically:\n\\[\nPr(scored = 1 | height) = \\frac{e^{−50.744 + 0.279height + \\epsilon}}{1 + e^{−50.744 + 0.279height + \\epsilon}}\n\\]\nImportantly, we need to know our player’s height to predict her probability of success. This makes sense! The slope of our model is not constant:\n\n\n\n\n\nWhen a player is 170 cm tall, our model predicts that she will score a three-pointer 3.5 percent of the time.\nHow did I get that?\n\\[\nPr(scored = 1 | height) = \\frac{e^{−50.744 + 0.279height + \\epsilon}}{1 + e^{−50.744 + 0.279height + \\epsilon}} = \\frac{e^{−50.744 + 0.279*170}}{1 + e^{−50.744 + 0.279*170}} = \\frac{.036}{1.036} = 0.035\n\\]\nWhen a player is 180 cm tall, our model predicts that she will score a three-pointer 27.5 percent of the time.\nWhen a player is 190 cm tall, our model predicts that she will score a three-pointer 88.6 percent of the time.\nNotice that we are increasing the player’s height by the same amount each time, but the increase in her probability of success from this constant increase is different. Here is the predicted probability of success across our range of players’ heights:\n\nplot_predictions(m_lr, condition = \"height\") + \n  theme_minimal() + \n  labs(x = \"Player's height (cm)\",\n       y = \"Probability of scoring\") + \n  scale_y_continuous(label = percent, limits = c(0, 1))\n\n\n\n\n\n\n\nInterpreting the intercept\nAs usual, the intercept should be interpreted as the expected value when all independent variables are set to 0. This is simple to interpret in terms of the probability of success. Remember:\n\\[\nPr(Y = 1 | X) = \\frac{e^X}{1 + e^X}\n\\]\nWhen height = 0:\n\\[\nPr(Y = 1 | X) = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}} = \\frac{e^{−50.744}}{1 + e^{−50.744}} = A\\ very\\ small\\ number!\n\\]"
  },
  {
    "objectID": "content/06-binary_outcomes.html#footnotes",
    "href": "content/06-binary_outcomes.html#footnotes",
    "title": "Binary outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’m sorry… it’s just not as good as AFL…↩︎"
  },
  {
    "objectID": "content/01-introduction.html",
    "href": "content/01-introduction.html",
    "title": "Linear Regression: A Refresher",
    "section": "",
    "text": "library(tidyverse)\nlibrary(poliscidata)\nlibrary(janitor)\nlibrary(scales)\nlibrary(wbstats)\nlibrary(broom)\nlibrary(ggdist)"
  },
  {
    "objectID": "content/01-introduction.html#packages",
    "href": "content/01-introduction.html#packages",
    "title": "Linear Regression: A Refresher",
    "section": "",
    "text": "library(tidyverse)\nlibrary(poliscidata)\nlibrary(janitor)\nlibrary(scales)\nlibrary(wbstats)\nlibrary(broom)\nlibrary(ggdist)"
  },
  {
    "objectID": "content/01-introduction.html#introduction",
    "href": "content/01-introduction.html#introduction",
    "title": "Linear Regression: A Refresher",
    "section": "Introduction",
    "text": "Introduction\nYou have a brilliant idea describing the relationship between an outcome of interest and a variable that you think is driving interesting changes to that outcome. You have a very clever theory describing the relationship between these two phenomena. Now you want to provide empirical support for that theory.\nThis quick refresher will outline the steps required to fit a linear regression model against two continuous variables: a dependent variable (our outcome of interest) and an independent variable (the thing you think is driving changes to that outcome of interest).\n\n\n\n\n\n\nNote\n\n\n\nFor a more in-depth set of notes on each of these steps, please refer to the GVPT622: Quantitative Methods for Political Science notes."
  },
  {
    "objectID": "content/01-introduction.html#do-richer-countries-enjoy-better-health-outcomes",
    "href": "content/01-introduction.html#do-richer-countries-enjoy-better-health-outcomes",
    "title": "Linear Regression: A Refresher",
    "section": "Do richer countries enjoy better health outcomes?",
    "text": "Do richer countries enjoy better health outcomes?\nLet’s return to a familiar question: what is the relationship between a country’s health and its wealth?\nHere, our outcome of interest is the average health of countries’ citizens. We want to determine what factors are important determinants of a country’s citizens’ health. We theorize that the level of wealth each country’s citizens hold is an important determinant of their overall health.\nSimply put, our goal is to produce the best model we can of how our outcome of interest occurs. You might hear people refer to this process - how an outcome of interest occurs - as the data generation process. This is just a rather sterile way of asking how our outcome of interest comes to be. For example, what factors drove the level of health of countries last year? How did they shape those health outcomes?\nWe can use a good model of our outcome of interest to achieve two worthwhile goals. First, this model provides us with explanatory traction. This, generally speaking, is your raison d’être as an academic. Your research goals should be to identify and explain the mechanisms driving changes to some important outcome of interest. To do that, you need to identify what factors are the important determinants of that outcome of interest. What gets people out to vote? What drives countries to war with one another? Why do some countries experience democratic backsliding? You need to find and then explain what factors are the most important drivers of these outcomes. You can use these statistical tools to do just that.\nSecond, this model provides us with predictive power. If we know the factors that are driving (or at least strongly associated with) changes to our outcome of interest and we know how they are shaping that outcome, we can predict how changes to those factors will effect our outcome of interest. For example, if you know that large spikes in the price of food staples are strongly associated with the onset of civil unrest, you can alert policymakers to the increased risk of that unrest if such spikes occur. You can also use your model to inform policymakers of the expected effects of various policies or interventions that they may be considering.\nHaving completed GVPT622, you now have the knowledge and skills to test empirically the relationship between two variables: a dependent and an independent variable. Let’s refresh those."
  },
  {
    "objectID": "content/01-introduction.html#step-1-collect-your-data",
    "href": "content/01-introduction.html#step-1-collect-your-data",
    "title": "Linear Regression: A Refresher",
    "section": "Step 1: Collect your data",
    "text": "Step 1: Collect your data\nDo richer countries enjoy better health outcomes? To answer this question, we need data on the health and wealth of each country’s citizens. We will use historical data to learn about the relationship between a country’s health and wealth. We will then extrapolate from that historical relationship to a generalized understanding of the relationship.\nTo do this, we need an observable and measurable factor that appropriately represents each of these concepts. I propose to follow the approach applied by the Gapminder project. We will use the average life expectancy of each country’s citizens as a measure of the health of its citizens. Countries with higher average life expectancy are assumed to have higher levels of health. We will use each country’s gross domestic product (GDP) per capita as our proxy measure of its citizens’ average wealth. Countries with higher GDP per capita are assumed to have wealthier citizens.\nWe might expect that as the wealth of country’s citizens increases, so too does their overall health. Our measurable hypothesis is; therefore, as follows:\n\nThe higher a country’s GDP per capita, the longer its average life expectancy will be.\n\n\n\n\n\n\n\nTip\n\n\n\nAll hypotheses must be testable. In other words, we must be able to use data to prove whether or not our hypothesis is true or false.\n\n\nThe World Bank provides reliable data on both of these variables. We can use the wbstats R package to pull these data from the World Bank API all from within our R script.\n\ngapminder_df &lt;- wb_data(\n  indicator = c(\"SP.DYN.LE00.IN\", \"NY.GDP.PCAP.CD\"),\n  start_date = 2021,\n  end_date = 2021\n) |&gt; \n  rename(\n    life_exp = SP.DYN.LE00.IN,\n    gdp_per_cap = NY.GDP.PCAP.CD\n  ) |&gt; \n  mutate(log_gdp_per_cap = log(gdp_per_cap)) \n\ngapminder_df\n\n# A tibble: 217 × 7\n   iso2c iso3c country               date gdp_per_cap life_exp log_gdp_per_cap\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n 1 AW    ABW   Aruba                 2021      29128.     74.6           10.3 \n 2 AF    AFG   Afghanistan           2021        356.     62.0            5.87\n 3 AO    AGO   Angola                2021       1927.     61.6            7.56\n 4 AL    ALB   Albania               2021       6377.     76.5            8.76\n 5 AD    AND   Andorra               2021      42072.     NA             10.6 \n 6 AE    ARE   United Arab Emirates  2021      44332.     78.7           10.7 \n 7 AR    ARG   Argentina             2021      10651.     75.4            9.27\n 8 AM    ARM   Armenia               2021       4973.     72.0            8.51\n 9 AS    ASM   American Samoa        2021      16654.     NA              9.72\n10 AG    ATG   Antigua and Barbuda   2021      17179.     78.5            9.75\n# ℹ 207 more rows"
  },
  {
    "objectID": "content/01-introduction.html#step-2-know-your-data",
    "href": "content/01-introduction.html#step-2-know-your-data",
    "title": "Linear Regression: A Refresher",
    "section": "Step 2: Know your data",
    "text": "Step 2: Know your data\nOnce you have data for your variables of interest you should take a good look at them. We now have access to data on 217 countries’ average life expectancy and GDP per capita.\n\nIndividual variables\nFirst, take a look at each variable individually. Some useful questions with which to start include:\n\nWhat kind of data are they (continuous or categorical)?\nHow are they distributed (normally or are they skewed)?\nAre there any unusual data points? If so, why are they unusual?\nAre any observations missing? Is this missingness random or systematic?\n\nOne of the easiest ways to answer these and other important questions is by visualizing your variables. We can use the many great plot functions in ggplot to do this.\nLet’s start with our outcome of interest: a country’s average life expectancy.\n\nggplot(gapminder_df, aes(x = life_exp)) + \n  geom_histogram() + \n  theme_minimal() + \n  labs(\n    x = \"Average life expectancy (in years)\",\n    y = \"Count\"\n  )\n\n\n\n\nThis is what we are trying to model.\nWe suspect that the average wealth of a country’s citizens is associated with their overall level of health. Let’s look at the distribution of countries’ GDP per capita:\n\nggplot(gapminder_df, aes(x = gdp_per_cap)) + \n  geom_histogram() + \n  theme_minimal() + \n  labs(\n    x = \"GDP per capita (in current USD)\",\n    y = \"Count\"\n  ) + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\n\n\nThe relationship between these two variables\nNow you can take a look at the relationship between your two variables of interest.\nSome good questions with which to start:\n\nHow do the two variables move with each other? As one goes up, does the other also go up, go down, or stay roughly constant?\nWhat is the shape of this relationship? Is it constant (linear)?\nAre there any noticable clusters or groups of observations?\nAre there any unusual observations? Ones sitting out on their own?\n\nLet’s visualize the relationship between each country’s average life expectancy and its GDP per capita:\n\nggplot(gapminder_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"GDP per capita (USD current)\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThis relationship is positive: as a country’s wealth increases, so too does its health. However, this relationship is not linear. An increase of $1,000 GDP per capita tends to be associated with a large increase in the country’s average life expectancy when the country has relatively low GDP per capita compared to the change in a richer country’s average life expectancy that tends to be associated with that same $1,000 increase in GDP per capita.\nWe can model non-linear relationships; however, these models can be difficult to interpret. Happily the relationship between a country’s average life expectancy and its logged GDP per capita is linear:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThis transformation means that we can now appropriately fit a linear model to these two variables: logged GDP per capita and average life expectancy."
  },
  {
    "objectID": "content/01-introduction.html#step-3-fit-your-linear-model",
    "href": "content/01-introduction.html#step-3-fit-your-linear-model",
    "title": "Linear Regression: A Refresher",
    "section": "Step 3: Fit your linear model",
    "text": "Step 3: Fit your linear model\nWe can now fit a linear regression model to our data to better capture and generalize this relationship.\n\nAn Ordinary Least Squares (OLS) regression finds the straight line that minimizes the distance between itself and all of the data points.\n\nWe can visualize that relationship using geom_smooth() from ggplot:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nWe can fit that model using lm():\n\nm &lt;- lm(life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nm\n\n\nCall:\nlm(formula = life_exp ~ log_gdp_per_cap, data = gapminder_df)\n\nCoefficients:\n    (Intercept)  log_gdp_per_cap  \n         33.217            4.343  \n\n\nThis gives us an estimated linear relationship between a country’s health and wealth. Formally:\n\\[\nLife\\ expectancy = \\beta_0 + \\beta_{1} Logged\\ GDP\\ per\\ capita + \\epsilon\n\\]\nOur model estimates the following relationship:\n\\[\nLife\\ expectancy = 33.427 + 4.316 Logged\\ GDP\\ per\\ capita + \\epsilon\n\\]\nWe can use this model to do all kinds of amazing things, including (hopefully) providing empirical support for our theories and making predictions about some outcome of interest."
  },
  {
    "objectID": "content/01-introduction.html#step-4-interpret-your-model",
    "href": "content/01-introduction.html#step-4-interpret-your-model",
    "title": "Linear Regression: A Refresher",
    "section": "Step 4: Interpret your model",
    "text": "Step 4: Interpret your model\nWhat does this model tell us about the estimated relationship between a country’s life expectancy and its logged GDP per capita?\nFirst, let’s look at the regression coefficient for a country’s logged GDP per capita. We found that every one unit increase in a country’s logged GDP per capita is associated with a 4.343 year increase in its citizens’ average life expectancy, on average.\nSecond, let’s look at that intercept coefficient. A country with a logged GDP per capita of zero is expected to have an average life expectancy of 33.217 years, on average. This is not a very useful piece of information because there are no countries with zero (logged) GDP per capita. Therefore, this intercept coefficient is more usefully thought of as a statistical artifact that grounds our model.\n\n\n\n\n\n\nWarning\n\n\n\nLinear regression models cannot detect causal relationships. You cannot use this model to determine whether changes to your dependent variable are caused by changes to your independent variable. Here, we cannot use this model as evidence of changes to a country’s GDP per capita causing changes in its average life expectancy. Therefore, you need to be careful when interpreting linear regression models. Use words like “associated with” instead of “causes”."
  },
  {
    "objectID": "content/01-introduction.html#step-5-evaluate-your-model",
    "href": "content/01-introduction.html#step-5-evaluate-your-model",
    "title": "Linear Regression: A Refresher",
    "section": "Step 5: Evaluate your model",
    "text": "Step 5: Evaluate your model\nHow well does our model fit our data? This is a complicated question without a single answer. As we step through this you should think critically about what questions you should ask of your data and your model that would help convince you that you have found the underlying relationship.\nThe first thing that is worth noting is that in the previous step we moved away from focusing on modelling our outcome of interest: each country’s average life expectancy. Instead, we wanted to understand the relationship between that outcome and our independent variable (each country’s GDP per capita). Let’s step back for a second and look again at modelling the outcome of interest.\nTo do this, we can see what our model predicts each country’s average life expectancy to be given its GDP per capita. To work this out, we simply need to plug each country’s GDP per capita into our model:\n\\[\nLife\\ expectancy = 33.427 + 4.316 Logged\\ GDP\\ per\\ capita + \\epsilon\n\\]\nHappily, broom::augment() does this for us:\n\nm_pred &lt;- augment(m)\nm_pred\n\n# A tibble: 201 × 9\n   .rownames life_exp log_gdp_per_cap .fitted .resid    .hat .sigma  .cooksd\n   &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n 1 1             74.6           10.3     77.9 -3.24  0.00956   4.24 0.00285 \n 2 2             62.0            5.87    58.7  3.25  0.0255    4.24 0.00793 \n 3 3             61.6            7.56    66.1 -4.43  0.00887   4.23 0.00494 \n 4 4             76.5            8.76    71.3  5.20  0.00500   4.23 0.00380 \n 5 6             78.7           10.7     79.7 -0.978 0.0127    4.24 0.000348\n 6 7             75.4            9.27    73.5  1.90  0.00536   4.24 0.000542\n 7 8             72.0            8.51    70.2  1.86  0.00526   4.24 0.000511\n 8 10            78.5            9.75    75.6  2.93  0.00677   4.24 0.00164 \n 9 11            83.3           11.0     81.1  2.25  0.0156    4.24 0.00226 \n10 12            81.2           10.9     80.5  0.733 0.0144    4.24 0.000221\n# ℹ 191 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\naugment() takes each of our 217 countries’ logged GDP per capita (log_gdp_per_cap), plugs it into our model (written above), and tells us the corresponding predicted average life expectancy (.fitted).\n\n\n\n\n\n\nNote\n\n\n\nNote that some data are missing for some countries. That is why we have less than 217 predicted values.\n\n\nLet’s take a look at both our predicted average life expectancy and the actual average life expectancy of each country in our dataset:\n\nggplot(m_pred) + \n  geom_histogram(aes(x = life_exp)) + \n  geom_histogram(aes(x = .fitted), fill = \"blue\", alpha = 0.5) +\n  theme_minimal() + \n  labs(x = \"Average life expectancy (in years)\",\n       y = \"Count\")\n\n\n\n\nThe distribution of the predicted average life expectancy is provided in blue. The actual average life expectancy of each county is provided in gray.\nA density curve might also help here:\n\nggplot(m_pred) + \n  geom_density(aes(x = life_exp)) + \n  geom_density(aes(x = .fitted), colour = \"blue\") +\n  theme_minimal() + \n  labs(x = \"Average life expectancy (in years)\",\n       y = \"Count\")\n\n\n\n\nWe’re looking pretty good! Ultimately; however, we want to find a generalized relationship between our outcome of interest and the independent variable that we think is an important driver of that outcome. Rather than focusing on building a model that perfectly predicts the average life expectancy of countries given their GDP per capita in 2021, we want to learn something about the general shape of this relationship. A linear regression model helps us do this.\nTo illustrate, let’s go back to our model:\n\\[\nLife\\ expectancy = 33.427 + 4.316 Logged\\ GDP\\ per\\ capita + \\epsilon\n\\]\nLet’s plot out the predicted average life expectancy for hypothetical countries with any logged GDP per capita within a plausible range:\n\naugment(m, newdata = tibble(log_gdp_per_cap = seq(0, 12, by = 0.01))) |&gt; \n  ggplot(aes(x = log_gdp_per_cap, y = .fitted)) + \n  geom_line() + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita (in current USD)\",\n       y = \"Predicted average life expectancy (in years)\")\n\n\n\n\nHere, we have used historical data that describe the actual average life expectancy of countries globally and their actual GDP per capita to build our understanding of an important factor associated with the overall health of a country’s citizens. In doing this, we have gained explanatory traction: there is a strong, positive association between a country’s GDP per capita and its average life expectancy. Countries with higher GDP per capita tend to have longer average life expectancy. We have also gained predictive power: for any given (logged) GDP per capita, we can predict what that country’s average life expectancy would be. Further, we can predict the change in a country’s average life expectancy that may result from a change in its GDP per capita.\nAll of this knowledge that we have gained rests on the strength of our model. Have we included all of the important drivers of a country’s average life expectancy? Have we used the right models of the relationship between those variables and average life expectancy? How much confidence do we have in our model’s ability to explain these relationships and to use those inputs to predict average life expectancy?\nOLS regression is one approach to finding this generalized relationship. Remember, an OLS regression finds the straight line that minimizes the distance between itself and all of the data points. To illustrate, let’s look back at our fitted model within the context of the data points used to generate it:\n\nggplot(gapminder_df, aes(x = log_gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = F) + \n  theme_minimal() + \n  labs(x = \"Logged GDP per capita\",\n       y = \"Average life expectancy (years)\") + \n  scale_x_continuous(labels = label_dollar())\n\n\n\n\nThis is a strong and clean relationship: I can clearly see that countries with higher logged GDP per capita tend to have longer average life expectancies. This is incorporated into our model, which formalizes this positive relationship.\nThere are many different ways we can think about error, but before you apply any more complicated statistical tests you should take a look at your model in the context of your data. This really is the best way to determine whether your model is capturing the underlying relationship between your variables.\n\nError for each data point\nAn OLS regression finds the straight line that minimizes the distance between itself and all of the data points. We can look at how far the predicted value produced by our model is from each data point. This distance is called the residual. As above, we can use broom::augment() to find it:\n\naugment(m) |&gt; \n  select(life_exp, .fitted, .resid)\n\n# A tibble: 201 × 3\n   life_exp .fitted .resid\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     74.6    77.9 -3.24 \n 2     62.0    58.7  3.25 \n 3     61.6    66.1 -4.43 \n 4     76.5    71.3  5.20 \n 5     78.7    79.7 -0.978\n 6     75.4    73.5  1.90 \n 7     72.0    70.2  1.86 \n 8     78.5    75.6  2.93 \n 9     83.3    81.1  2.25 \n10     81.2    80.5  0.733\n# ℹ 191 more rows\n\n\nLet’s plot those differences to make them easier to digest:\n\nggplot(augment(m), aes(x = .resid)) + \n  geom_density() + \n  geom_vline(xintercept = 0) + \n  theme_minimal()\n\n\n\n\nIf our model perfectly predicted each country’s life expectancy, we would see no difference between the predicted and observed values. There would just be a very tall straight line at zero on the graph above.\n\n\n\n\n\n\nNote\n\n\n\nThis is not necessarily the goal. Random error is fine: the world is a complicated and chaotic place. However, we can use these residuals to evaluate our model. For example, you may notice that residuals for certain countries or groupings of countries are larger than the rest. This may prompt you to re-examine your data collection process (perhaps something strange went on) or to include another variable in your model that captures these differences.\n\n\nOur model hasn’t predicted life expectancy perfectly. Although most predictions are within a couple of years of the country’s observed average life expectancy, there are some that are very different (up to 10 or 15 years!). Where the model has got it wrong, it has tended to overestimate life expectancy (note that the peak of the density curve sits above zero).\n\n\nModel-wide tools\nSometimes we need a measure of the model’s overall accuracy. Here, I will refer you back to our notes from GVPT622: Relationships Between Two Variables.\n\n\n\n\n\n\nTip\n\n\n\nYou should be familiar with these tests: your colleagues will refer to them and reviewers may look at them. However, there is increasing recognition among political scientists of their limits. You will be far better off asking specific and critical questions of your data and your models than you will be by relying on many of these tests, which can sometimes be misleading.\nFor example, most of these tests are sensitive to the quantity of data you use. If you throw an extraordinary amount of data into your models, you will likely get a very good looking F-statistic and T-statistic, even if you have not fully uncovered the underlying relationship in your data. Our access to and ability to use very large amounts of data is only increasing. Issues with some of these tests will become more acute."
  },
  {
    "objectID": "content/01-introduction.html#step-6-evaluate-your-coefficients",
    "href": "content/01-introduction.html#step-6-evaluate-your-coefficients",
    "title": "Linear Regression: A Refresher",
    "section": "Step 6: Evaluate your coefficients",
    "text": "Step 6: Evaluate your coefficients\nAt this stage, we have a model that fits our data well. We now want to ask whether the relationship we have uncovered is statistically significantly different from no relationship. In other words, is this all just random noise?\nAssume that we have a pure random sample of our population. If we were to pull a different pure random sample from our population we would get a different set of coefficients. That’s totally fine! However, we need to work out what these different coefficients could plausibly be. Once we have done that, we can determine whether or not they include zero (or no relationship).\n\n\n\n\n\n\nNote\n\n\n\nFor the long version of this, return to the regression notes from GVPT622.\n\n\nWe use the coefficients we produced using our data as our best guesses:\n\ntidy(m) |&gt; \n  select(term:estimate)\n\n# A tibble: 2 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)        33.2 \n2 log_gdp_per_cap     4.34\n\n\nWe can then use the standard deviations around these best guesses to work out how spread out around this best guess alternative plausible coefficients would sit.\n\n\n\n\n\n\nTip\n\n\n\nThe standard deviation, (\\(s\\)), is calculated using two pieces of information: how well our line of best fit fits our observed data; and how much information (or data) we used to fit our model.\n\n\nYou can find each coefficient’s standard deviation using broom::tidy():\n\ntidy(m) |&gt; \n  select(term:std.error)\n\n# A tibble: 2 × 3\n  term            estimate std.error\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        33.2      1.82 \n2 log_gdp_per_cap     4.34     0.203\n\n\nWe can now build out the plausible set of alternative coefficients:\n\ntibble(\n  x = rnorm(1e6, \n            mean = tidy(m) |&gt; filter(term == \"log_gdp_per_cap\") |&gt; pull(estimate),\n            sd = tidy(m) |&gt; filter(term == \"log_gdp_per_cap\") |&gt; pull(std.error))\n) |&gt; \n  ggplot(aes(x = x)) + \n  stat_halfeye(.width = c(.025, 0.975)) + \n  theme_minimal()\n\n\n\n\nIf we were to build an infinite number of linear regression models from an infinite number of pure random samples from the world (think: multiverse) we would get a series of coefficients that follow the distribution plotted above. Does this include zero (or no relationship)?\n\n\n\n\n\n\nNote\n\n\n\nRemember, regression coefficients tell us the expected change in our dependent variable changes for each one-unit increase in our independent variable, on average. If there is no association between our variables, there will be no change in our dependent variable and our regression coefficient will be zero.\n\n\nMost of the time you will be required to demonstrate that at least 95 percent of these plausible alternative coefficients do not include zero to claim statistical significance. In the graph above, this 95 percent is provided by the bar at the bottom of the density curve.\nTo work out whether 95 percent of these plausible alternative coefficients include zero we need to calculate the probability that we would observe the coefficient we found (here \\(\\beta_1 =\\) 4.343) if it were actually equal to zero. This probability is referred to as the p-value. We calculate it by finding our coefficient’s T-statistic.\n\n\n\n\n\n\nTip\n\n\n\nThe T-distribution is standard and centered at zero. We can transform our coefficient to its T-statistic and place it within this T-distribution to determine how likely we are to observe this or a more extreme value.\n\n\nbroom::tidy() provides both the T-statistic and p-value for our coefficients:\n\ntidy(m)\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        33.2      1.82       18.2 3.01e-44\n2 log_gdp_per_cap     4.34     0.203      21.4 1.67e-53\n\n\nThe coefficient we observed is statistically significant: there is a very, very, very small chance (far less than 5% certainly) we would be able to pull a perfect random sample from our population and fit this model if there truly was no relationship between a country’s average life expectancy and its GDP per capita."
  },
  {
    "objectID": "content/05-interactions.html",
    "href": "content/05-interactions.html",
    "title": "Interactions",
    "section": "",
    "text": "You will need to install a new package to follow along:\n\ninstall.packages(\"car\")\n\nYou then need to load all relevant packages into your R session:\n\nlibrary(tidyverse)\nlibrary(polisciols)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(broom)\nlibrary(car)"
  },
  {
    "objectID": "content/05-interactions.html#set-up",
    "href": "content/05-interactions.html#set-up",
    "title": "Interactions",
    "section": "",
    "text": "You will need to install a new package to follow along:\n\ninstall.packages(\"car\")\n\nYou then need to load all relevant packages into your R session:\n\nlibrary(tidyverse)\nlibrary(polisciols)\nlibrary(modelsummary)\nlibrary(marginaleffects)\nlibrary(broom)\nlibrary(car)"
  },
  {
    "objectID": "content/05-interactions.html#exporting-mass-destruction",
    "href": "content/05-interactions.html#exporting-mass-destruction",
    "title": "Interactions",
    "section": "Exporting mass destruction",
    "text": "Exporting mass destruction\nTrade between states is not only the product of economic considerations. States often promote or restrict trade with other states for political and security reasons. International political economists have identified a wide range of political factors that shape international trade, including regime type (democracies tend to trade more with each other than with other regimes), instability (states tend to trade less with states they believe to be at a high risk of conflict), and alliances (trade tends to “follow the flag”).\nThese theories of the political determinants of trade flows are often agnostic to the good being traded. But this is not necessarily the case. It is reasonable to expect that a state will be more mindful of who it supplies javelin missiles to than to whom it sells typewriters. Fuhrmann (2008) explores this distinction explicitly in his paper Exporting Mass Destruction? The Determinants of Dual-Use Trade.\nHe seeks to identify the strategic determinants of trade in dual-use commodities. These are commodities that can be used to produce Weapons of Mass Destruction (WMDs) or in legitimate civilian applications. Unlike javelin missiles, the purpose to which importing states will put these commodities is not clear. How do exporters balance the benefits of trade against the risk that they may enable WMD manufacturing?\nTo explore this question, Fuhrmann analyzes exports of dual-use commodities from the US to all other countries in the post-Cold War period between 1991 and 2001. Whilst he explores both economic and political determinants, his focus is on whether or not the US and its trade partner share the same foreign policy goals.\nBefore we dive into his full model, let’s set up our expectations. We should expect that states will be more reticent to sell these sensitive commodities to their enemies. Who knows to what nefarious purpose they may put them to?\nFuhrmann uses the degree to which states share similar foreign policy platforms to identify friends and enemies. This variable, pol_dissimiliarity is binary and takes on a value of 1 if the country enacts foreign policies that are very dissimilar to those of the US. In other words, pol_dissimilarity = 1 when they are enemies.\n\n\n\n\n\n\nNote\n\n\n\nFor those who are interested, he asks whether the average compatibility between the country’s and the United States’ foreign policy between 1991 and 2001 was in the lowest 10th percentile of all countries’ similarities with the United States.\n\n\nSo, we should expect that the US traded less in these sensitive commodities with its enemies than with its friends. Let’s take a look:\n\nx_mass_destruction |&gt; \n  group_by(pol_dissimilarity) |&gt; \n  summarise(avg_x_dual_use = scales::dollar(mean(x_dual_use)))\n\n# A tibble: 2 × 2\n  pol_dissimilarity avg_x_dual_use\n              &lt;dbl&gt; &lt;chr&gt;         \n1                 0 $1,046,814,259\n2                 1 $1,710,910,642\n\n\nHmm, the US appears to have provided its enemies with more dual-use commodities than its friends! That’s not very strategic!\nBut wait, let’s not write off US foreign policy just yet. Dual-use commodities are sensitive because they can be used to produce or support WMDs. But not all countries that import these commodities pursue WMDs. Many need these commodities for civilian applications.\nSo, it may in fact be the case that the US is only reticent to sell dual-use commodities to enemies that it knows are pursuing or have access to WMDs. Let’s include this characteristic in our summary:\n\nx_mass_destruction |&gt; \n  group_by(pol_dissimilarity, wmd_pursuit) |&gt; \n  summarise(avg_x_dual_use = scales::dollar(mean(x_dual_use))) \n\n# A tibble: 4 × 3\n# Groups:   pol_dissimilarity [2]\n  pol_dissimilarity wmd_pursuit avg_x_dual_use\n              &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;         \n1                 0           0 $924,468,836  \n2                 0           1 $3,004,341,034\n3                 1           0 $1,811,301,875\n4                 1           1 $104,650,927  \n\n\nAh! There we go! The US is happy to trade in dual-use commodities with enemies when they are not pursuing WMDs. Further, the US is happy to trade in dual-use commodities with its friends that are pursuing WMDs. I guess their interests must be really aligned.\nThis simple summary suggests that the relationship between trade in dual-use commodities and foreign policy is not as straight forward as we first thought. The US appears to be less willing to trade in these sensitive commodities with enemies if they are pursuing WMDs. This condition is critical to understanding this relationship.\nIn other words, we cannot say anything about the relationship between trade in dual-use commodities and foreign policy without first knowing whether the country in question is pursuing WMDs. If someone were to ask you “do you think the US avoids exporting dual-use commodities to its enemies?”, you should reply “it depends!”.\nWe can explore this more complex relationship formally using interactions. Below, we will replicate Fuhrmann’s analysis. In doing do, we will strengthen our understanding of multiple linear regression and introduce the knowledge required to interpret interaction effects.\nLet’s get started!"
  },
  {
    "objectID": "content/05-interactions.html#dual-use-trade",
    "href": "content/05-interactions.html#dual-use-trade",
    "title": "Interactions",
    "section": "Dual-use trade",
    "text": "Dual-use trade\nFuhrmann draws on existing literature to build an understanding of the determinants of trade in dual-use commodities. Let’s build his baseline model of US exports of dual-use commodities:\n\nm &lt;- lm(log(x_dual_use) ~ defence_ally + democracy + conflict + pol_dissimilarity + \n          wmd_pursuit + wmd_acquisition + log_gdp + log_population + \n          log_distance + log_x_all,\n        data = x_mass_destruction)\n\nmodelsummary(m, \n             stars = T,\n             coef_rename = c(\n               \"defence_ally\" = \"Military alliance\",\n               \"democracy\" = \"Democracy\",\n               \"conflict\" = \"Conflict\",\n               \"pol_dissimilarity\" = \"Politically dissimilar\",\n               \"wmd_pursuit\" = \"WMD pursuit\",\n               \"wmd_acquisition\" = \"WMD acquisition\",\n               \"log_gdp\" = \"Average GDP (USD logged)\",\n               \"log_population\" = \"Average population (logged)\",\n               \"log_distance\" = \"Distance between the United States and the country (logged)\",\n               \"log_x_all\" = \"Total imports from the US (USD logged)\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−3.085\n\n\n\n(3.173)\n\n\nMilitary alliance\n0.244\n\n\n\n(0.545)\n\n\nDemocracy\n0.837\n\n\n\n(0.526)\n\n\nConflict\n−3.548***\n\n\n\n(0.928)\n\n\nPolitically dissimilar\n1.862**\n\n\n\n(0.604)\n\n\nWMD pursuit\n1.706*\n\n\n\n(0.771)\n\n\nWMD acquisition\n0.644\n\n\n\n(0.704)\n\n\nAverage GDP (USD logged)\n0.289\n\n\n\n(0.210)\n\n\nAverage population (logged)\n0.052\n\n\n\n(0.173)\n\n\nDistance between the United States and the country (logged)\n−0.033\n\n\n\n(0.177)\n\n\nTotal imports from the US (USD logged)\n0.616***\n\n\n\n(0.147)\n\n\nNum.Obs.\n151\n\n\nR2\n0.658\n\n\nR2 Adj.\n0.634\n\n\nAIC\n6080.3\n\n\nBIC\n6116.5\n\n\nLog.Lik.\n−317.663\n\n\nF\n26.973\n\n\nRMSE\n1.98\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\nHis argument focuses the level of political similarity between the US and its trade partner. Let’s look at the value of dual-use exports this model predicts friends and foes to receive:\n\nplot_predictions(m, condition = \"pol_dissimilarity\") + \n  theme_minimal() + \n  labs(x = \"Politically dissimilar\",\n       y = \"Exports in dual-use goods (logged USD)\")\n\n\n\n\nAs above, we see that countries with strongly dissimilar foreign policy agendas to the US receive the most dual-use exports from it. This runs counter to our expectations.\nThis model assumes that the association between exports in dual-use commodities and the level of compatibility between the trade partner’s and the United States’ foreign policy is constant across all trade partners regardless of their desire or capacity to produce WMDs. However, as Fuhrmann argues, this should not be the case: the US should be much more reticent to export dual-use commodities to states that are pursuing foreign policies that are not aligned with the US’s interests and seek to produce WMDs.\nWe can capture this by introducing into our model an interaction between dissimilarities and whether a state is pursuing acquiring nuclear, chemical, or biological weapons:\n\nm &lt;- lm(log(x_dual_use) ~ defence_ally + democracy + conflict + pol_dissimilarity + \n          wmd_pursuit + wmd_acquisition + wmd_pursuit * pol_dissimilarity + log_gdp + \n          log_population + log_distance + log_x_all,\n        data = x_mass_destruction)\n\nmodelsummary(m, \n             stars = T,\n             coef_rename = c(\n               \"defence_ally\" = \"Military alliance\",\n               \"democracy\" = \"Democracy\",\n               \"conflict\" = \"Conflict\",\n               \"pol_dissimilarity\" = \"Politically dissimilar\",\n               \"wmd_pursuit\" = \"WMD pursuit\",\n               \"wmd_acquisition\" = \"WMD acquisition\",\n               \"log_gdp\" = \"Average GDP (USD logged)\",\n               \"log_population\" = \"Average population (logged)\",\n               \"log_distance\" = \"Distance between the United States and the country (logged)\",\n               \"log_x_all\" = \"Total imports from the US (USD logged)\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n−2.823\n\n\n\n(3.149)\n\n\nMilitary alliance\n0.276\n\n\n\n(0.540)\n\n\nDemocracy\n0.872+\n\n\n\n(0.522)\n\n\nConflict\n−3.560***\n\n\n\n(0.919)\n\n\nPolitically dissimilar\n2.163***\n\n\n\n(0.620)\n\n\nWMD pursuit\n2.198**\n\n\n\n(0.808)\n\n\nWMD acquisition\n0.574\n\n\n\n(0.698)\n\n\nAverage GDP (USD logged)\n0.273\n\n\n\n(0.208)\n\n\nAverage population (logged)\n0.055\n\n\n\n(0.172)\n\n\nDistance between the United States and the country (logged)\n−0.040\n\n\n\n(0.175)\n\n\nTotal imports from the US (USD logged)\n0.620***\n\n\n\n(0.145)\n\n\nPolitically dissimilar:WMD pursuit\n−4.203+\n\n\n\n(2.247)\n\n\nNum.Obs.\n151\n\n\nR2\n0.667\n\n\nR2 Adj.\n0.640\n\n\nAIC\n6078.6\n\n\nBIC\n6117.8\n\n\nLog.Lik.\n−315.787\n\n\nF\n25.276\n\n\nRMSE\n1.96\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo include an interaction in your lm() function, multiply the two variables together. Above, I included an interaction between pol_dissimilarity and wmd_pursuit by including in addition to the two variables on their own pol_dissimilarity * wmd_pursuit.\nYou must also include the constituative variables on their own. Above I included:\n\nlm(log(x_dual_use) ~ ... + pol_dissimilarity + wmd_pursuit + wmd_pursuit * pol_dissimilarity, \n   data = x_mass_destruction)\n\n\n\n\nplot_predictions(m, condition = c(\"pol_dissimilarity\", \"wmd_pursuit\")) + \n  theme_minimal() + \n  labs(colour = \"Pursues WMDs\" ,\n       x = \"Politically dissimilar\",\n       y = \"Exports in dual-use goods (logged USD)\")\n\n\n\n\nNow we can see support for Fuhrmann’s claim that the US is much more reticent to export dual-use goods to countries pursuing antithetical foreign policy that are also seeking to develop WMDs. We can also see that the US is happy to export dual-use goods to its friends that are pursuing WMDs. Without the interaction, we would not be able to see this strategic dynamic at play.\n\nInterpreting interaction coefficients\nYou need to be very careful when interpreting these coefficients. Let’s go through some of the important ones together.\nOne way to think about interactions is to look at the different regression lines for trade partners that are politically dissimilar and for those that are similar to the US.\nLet’s start with the general regression function:\n\\[\nDual\\ use\\ exports = −2.82 + ... + 2.16 * pol\\_dissimilarity + 2.20 * wmd\\_pursuit - 4.20 pol\\_dissimilarity * wmd\\_pursuit + \\epsilon\n\\]\nNow, let’s look at this function for states that share the US’s foreign policy goals (pol_dissimilarity = 0):\n\\[\nDual\\ use\\ exports = −2.82 + ... + 2.16 * 0 + 2.20 * wmd\\_pursuit - 4.20 * 0 * wmd\\_pursuit + \\epsilon\n\\]\n\\[\nDual\\ use\\ exports = −2.82 + ... + 2.20 * wmd\\_pursuit + \\epsilon\n\\]\nAnd for states that are dissimilar to the US:\n\\[\nDual\\ use\\ exports = −2.82 + ... + 2.16 * 1 + 2.20 * wmd\\_pursuit - 4.20 * 1 * wmd\\_pursuit + \\epsilon\n\\]\n\\[\nDual\\ use\\ exports = -0.66 - 2 * wmd\\_pursuit + \\epsilon\n\\]\nSo, among states that share the US’s political goals, pursuing WMDs actually increases their dual-use exports from the US relative to those like-minded states that do not pursue WMDs. Presumably, these states demand greater quantities of these goods (to support their WMD programs) than those who are not pursuing WMDs and; therefore, only use these goods for civilian purposes. We predict that the value of dual-use exports for these countries is -0.66 logged dollars, compared to −2.82 logged dollars among like-minded states that are not pursuing these weapons.\nWe can contrast this with our expectations among states that do not share the US’s political goals. When these states are pursuing WMDs, they receive -2.66 logged dollars of exports in these useful commodities from the US. As expected, this is less than that received by dissimilar states that are not pursuing these weapons (who we predict will receive -0.66 logged dollars in exports).\nLet’s make this a little more tangible. Let’s find the average value of dual-use exports our model predicts friends and foes pursuring and not pursuing WMDs to receive:\n\naugment(m) |&gt; \n  group_by(pol_dissimilarity, wmd_pursuit) |&gt; \n  summarise(pred_x_dual_use = scales::dollar(exp(mean(.fitted))))\n\n# A tibble: 4 × 3\n# Groups:   pol_dissimilarity [2]\n  pol_dissimilarity wmd_pursuit pred_x_dual_use\n              &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;          \n1                 0           0 $42,857,009    \n2                 0           1 $401,105,513   \n3                 1           0 $464,421,049   \n4                 1           1 $104,650,927   \n\n\nOverall, the predicted effect of political dissimilarity is equal to:\n\\[\n\\beta_{pol\\_dissimilarity} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit} * wmd\\_pursuit = 2.16 - 4.20*wmd\\_pursuit\n\\]\nIn other words: the effect on dual-use exports from the US of political dissimilarity depends on whether or not the trade partner is pursuing WMDs. Just as we theorized!\nThis also means that \\(\\beta_{pol\\_dissimilarity}\\) represents the predicted effect of political dissimilarity when the trade partner is not pursuing WMDs.\nAlso, the predicted effect of pursuing WMDs is equal to:\n\\[\n\\beta_{wmd\\_pursuit} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit} * pol\\_dissimilarity = 2.20 - 4.20*pol\\_dissimilarity\n\\]\nAgain, the effect on dual-use exports from the US of pursuing WMDs depends on whether or not the trade partner is a friend or foe of the US.\nThe coefficient of the interaction term (\\(\\beta_{pol\\_dissimilarity*wmd\\_pursuit}\\)) is the effect of the effect. On the effect on dual-use exports from the US of political dissimilarity, it is the effect of the trade partner pursuing WMDs. On the effect on dual-use exports from the US of pursuing WMDs, it is the effect of being a friend or foe of the US."
  },
  {
    "objectID": "content/05-interactions.html#are-these-effects-significant",
    "href": "content/05-interactions.html#are-these-effects-significant",
    "title": "Interactions",
    "section": "Are these effects significant?",
    "text": "Are these effects significant?\nRemember, the predicted effect of political dissimilarity is equal to:\n\\[\n\\beta_{pol\\_dissimilarity} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit} * wmd\\_pursuit\n\\]\nWhen the trade partner is not pursuing WMDs, the effect of political dissimilarity is equal to:\n\\[\n\\beta_{pol\\_dissimilarity} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit} * 0 = \\beta_{pol\\_dissimilarity}\n\\]\nHere, \\(\\beta_{pol\\_dissimilarity}\\) is meaningful on its own. We can, therefore, use our familiar t-test (communicated through the p-value) to tell us whether the predicted effect of political dissimilarity on US dual-use exports to friends is significantly different from no effect.\nIs the predicted effect of political dissimilarity on US dual-use exports among trade partners that pursue WMDs significant? To test this, we need to ask whether the joint effect is significantly different from zero:\n\\[\n\\beta_{pol\\_dissimilarity} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit}\n\\]\nIn other words, you need to ask whether:\n\\[\n\\beta_{pol\\_dissimilarity} = \\beta_{pol\\_dissimilarity*wmd\\_pursuit} = 0\n\\]\nWe can use an F-test to formally test whether our coefficients are significantly different from zero.\n\nlinearHypothesis(m, c(\"pol_dissimilarity = 0\", \"pol_dissimilarity:wmd_pursuit = 0\"))\n\nLinear hypothesis test\n\nHypothesis:\npol_dissimilarity = 0\npol_dissimilarity:wmd_pursuit = 0\n\nModel 1: restricted model\nModel 2: log(x_dual_use) ~ defence_ally + democracy + conflict + pol_dissimilarity + \n    wmd_pursuit + wmd_acquisition + wmd_pursuit * pol_dissimilarity + \n    log_gdp + log_population + log_distance + log_x_all\n\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    141 634.29                                \n2    139 579.41  2    54.878 6.5826 0.001856 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe F-statistic for the joint hypothesis is 6.5826. The likelihood that we would get an F-statistic this large if, in fact, it was equal to zero is 0.001856. Therefore, we can reject the null hypothesis that both of these coefficients are equal to zero.\n\n\n\n\n\n\nTip\n\n\n\nIf you look back at the regression table, you will see that \\(\\beta_{pol\\_dissimilarity*wmd\\_pursuit}\\) does not on its own meet this standard of statistical significance. This is why it is so important carefully interpret interactions.\n\n\nTherefore, we can say that the predicted effect of political dissimilarity on US dual-use exports among trade partners that pursue WMDs (\\(\\beta_{pol\\_dissimilarity} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit} = 2.16 − 4.20\\)) is statistically significant.\nIs the predicted effect of pursuing WMDs among friends statistically significant?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nRemember, the predicted effect of pursuing WMDs is equal to:\n\\[\n\\beta_{wmd\\_pursuit} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit} * pol\\_dissimilarity\n\\]\nWhen the trade partner is a friend, the effect of pursuing WMDs is equal to:\n\\[\n\\beta_{wmd\\_pursuit} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit} * 0 = \\beta_{wmd\\_pursuit}\n\\]\nSo we can look at the statistical significance of that single coefficient. The p-value on \\(\\beta_{wmd\\_pursuit}\\) is 0.007. This is statistically significant at the 0.05-level.\n\n\n\nIs the predicted effect of pursuing WMDs among foes statistically significant?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWhen the trade partner is a foe, the effect of pursuing WMDs is equal to:\n\\[\n\\beta_{wmd\\_pursuit} + \\beta_{pol\\_dissimilarity*wmd\\_pursuit}\n\\]\nSo we need to ask whether:\n\\[\n\\beta_{wmd\\_pursuit} = \\beta_{pol\\_dissimilarity*wmd\\_pursuit} = 0\n\\]\nWe can use an F-test to formally test whether our coefficients are significantly different from zero:\n\nlinearHypothesis(m, c(\"wmd_pursuit = 0\", \"pol_dissimilarity:wmd_pursuit = 0\"))\n\nLinear hypothesis test\n\nHypothesis:\nwmd_pursuit = 0\npol_dissimilarity:wmd_pursuit = 0\n\nModel 1: restricted model\nModel 2: log(x_dual_use) ~ defence_ally + democracy + conflict + pol_dissimilarity + \n    wmd_pursuit + wmd_acquisition + wmd_pursuit * pol_dissimilarity + \n    log_gdp + log_population + log_distance + log_x_all\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1    141 614.78                              \n2    139 579.41  2    35.373 4.2429 0.01627 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, we are very unlikely to see such a large F-statistic of 4.2429 (in fact, there is only a 1.63% chance) if it were zero. Therefore, we can state that the the effect of pursuing WMDs on dual-use exports from the US among its foes is statistically significant."
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#what-problem-are-we-solving",
    "href": "content/slides/03_consistency_bias.html#what-problem-are-we-solving",
    "title": "Consistency and bias",
    "section": "What problem are we solving?",
    "text": "What problem are we solving?\nBuilding confidence in your inference from a finite number of random samples."
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#data-and-packages",
    "href": "content/slides/03_consistency_bias.html#data-and-packages",
    "title": "Consistency and bias",
    "section": "Data and packages",
    "text": "Data and packages\n\nlibrary(tidyverse)\nlibrary(poliscidata)\nlibrary(modelsummary)\nlibrary(broom)\nlibrary(ggdist)\n\nBecause we are working with randomness:\n\nset.seed(222)"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#consistency",
    "href": "content/slides/03_consistency_bias.html#consistency",
    "title": "Consistency and bias",
    "section": "Consistency",
    "text": "Consistency\nRefers to the probability that each random sample from our population will produce a similar set of estimates of our regression coefficients."
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#consistency-1",
    "href": "content/slides/03_consistency_bias.html#consistency-1",
    "title": "Consistency and bias",
    "section": "Consistency",
    "text": "Consistency\nWhat is the relationship between an individual’s feelings towards President Obama and their party affiliation?\n\npoliscidata::nes |&gt; \n  select(caseid, obama_therm, dem) |&gt; \n  glimpse()\n\nRows: 5,916\nColumns: 3\n$ caseid      &lt;dbl&gt; 408, 3282, 1942, 118, 5533, 5880, 1651, 6687, 5903, 629, 1…\n$ obama_therm &lt;dbl&gt; 15, 100, 70, 30, 70, 45, 50, 60, 15, 100, NA, 0, 45, 30, 4…\n$ dem         &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#consistency-2",
    "href": "content/slides/03_consistency_bias.html#consistency-2",
    "title": "Consistency and bias",
    "section": "Consistency",
    "text": "Consistency\nWhat is the relationship between an individual’s feelings towards President Obama and their party affiliation?\n\nm &lt;- lm(obama_therm ~ dem, data = nes)\n\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n44.245***\n\n\nDemocrat\n41.061***\n\n\nNum.Obs.\n5474\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\nIndividuals who identify as Democrats have, on average, 41.1 points warmer feelings towards President Obama than individuals who do not identify as Democrats."
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#random-sample-from-our-population",
    "href": "content/slides/03_consistency_bias.html#random-sample-from-our-population",
    "title": "Consistency and bias",
    "section": "Random sample from our “population”",
    "text": "Random sample from our “population”\nLet’s start with a random sample of 100 respondents:\n\nnes_100 &lt;- nes |&gt; \n  sample_n(100) |&gt; \n  select(caseid, obama_therm, dem)\n\nglimpse(nes_100)\n\nRows: 100\nColumns: 3\n$ caseid      &lt;dbl&gt; 405, 5672, 372, 3522, 3882, 1437, 6178, 1148, 5379, 5953, …\n$ obama_therm &lt;dbl&gt; 100, 85, 40, 100, 100, 100, NA, 60, 50, 100, 70, 100, NA, …\n$ dem         &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0…"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#learning-from-this-sample",
    "href": "content/slides/03_consistency_bias.html#learning-from-this-sample",
    "title": "Consistency and bias",
    "section": "Learning from this sample",
    "text": "Learning from this sample\n\nm &lt;- lm(obama_therm ~ dem, data = nes_100)\n\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n47.268***\n\n\nDemocrat\n44.192***\n\n\nNum.Obs.\n93\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#acknowledging-randomness",
    "href": "content/slides/03_consistency_bias.html#acknowledging-randomness",
    "title": "Consistency and bias",
    "section": "Acknowledging randomness",
    "text": "Acknowledging randomness\nLet’s take a different random sample of 100 respondents:\n\nnes_100 &lt;- nes |&gt; \n  sample_n(100) |&gt; \n  select(caseid, obama_therm, dem)\n\nglimpse(nes_100)\n\nRows: 100\nColumns: 3\n$ caseid      &lt;dbl&gt; 6111, 148, 1913, 6246, 6640, 3983, 6779, 3650, 1452, 4943,…\n$ obama_therm &lt;dbl&gt; 85, 100, 80, NA, 50, 100, NA, 70, 100, 50, 70, 70, 60, 85,…\n$ dem         &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0…"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#acknowledging-randomness-1",
    "href": "content/slides/03_consistency_bias.html#acknowledging-randomness-1",
    "title": "Consistency and bias",
    "section": "Acknowledging randomness",
    "text": "Acknowledging randomness\nLet’s take a different random sample of 100 respondents:\n\nm &lt;- lm(obama_therm ~ dem, data = nes_100)\n\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n39.057***\n\n\nDemocrat\n42.548***\n\n\nNum.Obs.\n96\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#acknowledging-randomness-2",
    "href": "content/slides/03_consistency_bias.html#acknowledging-randomness-2",
    "title": "Consistency and bias",
    "section": "Acknowledging randomness",
    "text": "Acknowledging randomness\nLet’s take 1,000 different random samples of 100 respondents."
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#building-confidence-in-our-one-random-sample",
    "href": "content/slides/03_consistency_bias.html#building-confidence-in-our-one-random-sample",
    "title": "Consistency and bias",
    "section": "Building confidence in our one random sample",
    "text": "Building confidence in our one random sample\nWe can only take a finite number of random samples from our population.\n\nHow can we increase our confidence that the estimated coefficients produced by these random samples is close to the truth?"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#building-confidence-in-our-one-random-sample-1",
    "href": "content/slides/03_consistency_bias.html#building-confidence-in-our-one-random-sample-1",
    "title": "Consistency and bias",
    "section": "Building confidence in our one random sample",
    "text": "Building confidence in our one random sample\nWe can do this by increasing our sample size. The larger the sample size, the more consistent the estimates.\n\nLet’s look at 1,000 different random samples of 300 respondents.\nAnd 1,000 different random samples of 1,000 respondents.\n\nDo we get more consistent estimates?"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#building-confidence-in-our-one-random-sample-2",
    "href": "content/slides/03_consistency_bias.html#building-confidence-in-our-one-random-sample-2",
    "title": "Consistency and bias",
    "section": "Building confidence in our one random sample",
    "text": "Building confidence in our one random sample"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#bias",
    "href": "content/slides/03_consistency_bias.html#bias",
    "title": "Consistency and bias",
    "section": "Bias",
    "text": "Bias\nA biased coefficient estimate will systematically be higher or lower than the true value."
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#what-if-we-only-sampled-from-males",
    "href": "content/slides/03_consistency_bias.html#what-if-we-only-sampled-from-males",
    "title": "Consistency and bias",
    "section": "What if we only sampled from males?",
    "text": "What if we only sampled from males?\nWhat happens to our understanding of the relationship between an individual’s feelings towards Obama and their party affiliation?\n\nnes_men &lt;- nes |&gt; \n  filter(gender == \"Male\") |&gt; \n  select(caseid, obama_therm, dem, gender)\n\nglimpse(nes_men)\n\nRows: 2,847\nColumns: 4\n$ caseid      &lt;dbl&gt; 408, 3282, 1942, 118, 5533, 5880, 1651, 6687, 5903, 629, 1…\n$ obama_therm &lt;dbl&gt; 15, 100, 70, 30, 70, 45, 50, 60, 15, 100, NA, 0, 45, 30, 4…\n$ dem         &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ gender      &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male…"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#a-consistent-but-biased-estimate",
    "href": "content/slides/03_consistency_bias.html#a-consistent-but-biased-estimate",
    "title": "Consistency and bias",
    "section": "A consistent but biased estimate",
    "text": "A consistent but biased estimate\nLet’s take a random sample of 1,000 individuals from this male-only pool.\n\n\n\n\n\n\nMales only\n\n\n\n\n(Intercept)\n43.575***\n\n\nDemocrat\n41.641***\n\n\nNum.Obs.\n925\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#a-consistent-but-biased-estimate-1",
    "href": "content/slides/03_consistency_bias.html#a-consistent-but-biased-estimate-1",
    "title": "Consistency and bias",
    "section": "A consistent but biased estimate",
    "text": "A consistent but biased estimate\n\n\n\n\n\n\nMales only\nAll respondents\n'True' relationship\n\n\n\n\n(Intercept)\n43.575***\n43.102***\n44.245***\n\n\nDemocrat\n41.641***\n41.505***\n41.061***\n\n\nNum.Obs.\n925\n934\n5474\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "content/slides/03_consistency_bias.html#summary",
    "href": "content/slides/03_consistency_bias.html#summary",
    "title": "Consistency and bias",
    "section": "Summary",
    "text": "Summary\n\nCannot take infinite samples from our population\nCan use our understanding of uncertainty to increase our confidence in a single or finite number of random samples from our population (consistency)\nNeed to ensure that we are not excluding groups of observations from the population from which we draw those random samples (bias)\nWe aim to have consistent and unbiased estimates of our coefficients"
  },
  {
    "objectID": "content/04-panel_data.html#set-up",
    "href": "content/04-panel_data.html#set-up",
    "title": "Panel Data",
    "section": "Set up",
    "text": "Set up\nYou will need to install the following new packages:\n\ninstall.packages(c(\"lme4\", \"broom.mixed\", \"ggh4x\", \"marginaleffects\"))\n\nThen load them and some other friends into your session:\n\nlibrary(tidyverse)\nlibrary(wbstats)\nlibrary(countrycode)\nlibrary(modelsummary)\nlibrary(ggh4x)\nlibrary(lme4)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(ggdist)\nlibrary(marginaleffects)\n\nset.seed(333)"
  },
  {
    "objectID": "content/04-panel_data.html#introducing-panel-data",
    "href": "content/04-panel_data.html#introducing-panel-data",
    "title": "Panel Data",
    "section": "Introducing panel data",
    "text": "Introducing panel data\nA lot of political science research (especially in international relations and comparative politics) attempts to resolve questions that deal with many actors’ behavior over time. These data have dependencies between them: a country’s GDP this year is, in part, the product of its GDP last year; whether or not a country is at war this year is going to be influenced by whether or not it was at war last year; individuals’ votes in this year’s US Presidential Election will be shaped by who they voted for in the last election.\nThese types of data go by many names, but are often referred to as panel data or time-series cross sectional (TSCS) data. You can get a much richer picture of the patterns that exist within your population of interest by looking at how those patterns form and change over time. However, there is no such thing as a free lunch. Adding time and hierarchy to your analysis adds complexity. This week, I will introduce you to this complexity in (what I hope is) a digestible way.\n\n\n\n\n\n\nNote\n\n\n\nWe will focus on country-panel data (i.e. Australia in 2022, Australia in 2023, Australia in 2024) but this logic and these methods can be applied to almost any panel data."
  },
  {
    "objectID": "content/04-panel_data.html#relationship-between-health-and-wealth",
    "href": "content/04-panel_data.html#relationship-between-health-and-wealth",
    "title": "Panel Data",
    "section": "Relationship between health and wealth",
    "text": "Relationship between health and wealth\nToday we are going to explore a familiar question in a new way. What is the relationship between health and wealth? Previously, we used data from only one year to find that, on average, as a country’s GDP per capita increases, so too does the average life expectancy of its citizens. We are going to extend this analysis to explore this trend over time and between countries. We will identify whether this positive relationship is consistent across all years between 2000 and 2020, and between all countries in the world.\nFirst, we will collect data on each country’s GDP per capita from 2000:\n\ngdp_df &lt;- wb_data(\"NY.GDP.PCAP.CD\", start_date = 2000, end_date = 2020, return_wide = F) |&gt; \n  transmute(iso3c, country, year = date, gdp_per_cap = value)\n\nNext, we will collect data on countries’ average life expectancy over this same time period:\n\nlife_exp_df &lt;- wb_data(\"SP.DYN.LE00.IN\", start_date = 2000, end_date = 2020, return_wide = F) |&gt; \n  select(iso3c, country, year = date, life_exp = value)\n\nThen we can build our data set:\n\nfull_df &lt;- gdp_df |&gt; \n  left_join(life_exp_df, by = join_by(iso3c, country, year)) |&gt; \n  mutate(region = countrycode(country, \"country.name\", \"region\"),\n         year = year - 2000) |&gt; \n  relocate(region, .after = country)\nfull_df\n\n# A tibble: 4,557 × 6\n   iso3c country     region      year gdp_per_cap life_exp\n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 AFG   Afghanistan South Asia    20        512.     62.6\n 2 AFG   Afghanistan South Asia    19        498.     63.6\n 3 AFG   Afghanistan South Asia    18        492.     63.1\n 4 AFG   Afghanistan South Asia    17        526.     63.0\n 5 AFG   Afghanistan South Asia    16        523.     63.1\n 6 AFG   Afghanistan South Asia    15        567.     62.7\n 7 AFG   Afghanistan South Asia    14        627.     62.5\n 8 AFG   Afghanistan South Asia    13        639.     62.4\n 9 AFG   Afghanistan South Asia    12        653.     61.9\n10 AFG   Afghanistan South Asia    11        609.     61.4\n# ℹ 4,547 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nI am baselining the years (0 represents 2000, 1 represents 2001, etc.). This will give us a meaningful intercept.\n\n\nNow we have data on all 217 countries’ GDP per capita and life expectancy across 20 years. Therefore, our observations are clustered by country (we have 20 observations per country). These countries are, themselves, clustered within 7 regions."
  },
  {
    "objectID": "content/04-panel_data.html#why-are-we-doing-this",
    "href": "content/04-panel_data.html#why-are-we-doing-this",
    "title": "Panel Data",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?\nBefore we dive into some of the additional work you will need to do when working with time-series data, I should answer a very valid question: “why on earth are we doing this?”. In other words, what problem are we solving?\nThere are two answers to this question. First, the goal of your analysis is to undercover a general relationship between your outcome of interest and all of the things you theorize to be important drivers of that outcome of interest. You use this empirical analysis to provide evidence of that theorized relationship occurring out in the wild. When your theory applies to actors’ behavior over time, you should aim to demonstrate this to be the case. You should use time-series data.\nSecond, once you have decided to provide this evidence of your theory over time, your statistical tools need to also adapt. Your observations are no longer independent of each other. You are observing behavior by the same actor over time (i.e. Australia’s trade with the US in 2020, in 2021, in 2022, etc.). The actor’s behavior in the year prior influences their behavior in the current year. For example, US consumers that have established links with Australian manufacturers have to decide to change to a new manufacturer from another country (expending some amount of effort), or continue to buy products from that Australian manufacturer this year. Either way, the fact that they traded with Australia in the year prior impacts their behavior this year. Contrast this with our previous set ups. For example, we have use the National Election Survey to looked at the US public’s opinions on various things. These individuals are selected at random. They are strangers to one another. It is appropriate to assume that they do not influence each other’s opinions or responses to the survey. We discussed this assumption of independence of errors at length in the model specification week. If we violate this assumption (which we most certainly do when dealing with the same observation over time), we need to adjust our statistical model or tool accordingly. So, let’s get to it."
  },
  {
    "objectID": "content/04-panel_data.html#starting-simple",
    "href": "content/04-panel_data.html#starting-simple",
    "title": "Panel Data",
    "section": "Starting simple",
    "text": "Starting simple\nTo get a better sense of why we need to think seriously about time in our analyses, we will start by looking only at each country’s average life expectancy. We will explore the relationship between this and GDP per capita later.\nWe know that, generally speaking, people are increasingly living longer:\n\nggplot(full_df, aes(x = (year + 2000), y = life_exp, group = country)) + \n  geom_line(alpha = 0.25) + \n  theme_minimal() + \n  theme(panel.grid = element_blank()) + \n  labs(x = \"Year\",\n       y = \"Average Life Expectancy (in years)\")\n\n\n\n\nA useful summary of these data is the global average life expectancy over time:\n\nfull_df |&gt;\n  group_by(year) |&gt; \n  summarise(life_exp = mean(life_exp, na.rm = T)) |&gt; \n  ggplot(aes(x = (year + 2000), y = life_exp)) + \n  geom_line() + \n  theme_minimal() + \n  theme(panel.grid = element_blank()) + \n  labs(x = \"Year\",\n       y = \"Average Life Expectancy (in years)\")\n\n\n\n\nThis growth is linear and positive (except for that drop off in 2020)! Therefore, we can model it over time using our good friend linear regression:\n\nm &lt;- lm(life_exp ~ year, data = full_df)\n\nmodelsummary(m, coef_rename = c(year = \"Year\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n67.516\n\n\n\n(0.253)\n\n\nYear\n0.289\n\n\n\n(0.022)\n\n\nNum.Obs.\n4409\n\n\nR2\n0.039\n\n\nR2 Adj.\n0.039\n\n\nAIC\n31593.5\n\n\nBIC\n31612.6\n\n\nLog.Lik.\n−15793.726\n\n\nF\n178.675\n\n\nRMSE\n8.70\n\n\n\n\n\n\n\nThis model tells us that, in 2000 (when year = 0), the average life expectancy across countries globally was 67.52 years. This global average increases by an average of 0.29 years (or 3.5 months) every year thereafter.\nTo put this in familiar terms, we have fit the following model:\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1 Year + \\epsilon\n\\]"
  },
  {
    "objectID": "content/04-panel_data.html#accounting-for-the-structure-of-our-data",
    "href": "content/04-panel_data.html#accounting-for-the-structure-of-our-data",
    "title": "Panel Data",
    "section": "Accounting for the structure of our data",
    "text": "Accounting for the structure of our data\nThis model accounts for time in a very straightforward and direct way. However, it ignores the structure of the data: it does not account for regional- or country-specific trends buried within. We lose a lot of interesting information when we summarize our data in this way.\nTo illustrate, let’s take a look at how our model performs at the country-level. I will randomly select two countries from every region to illustrate.\n\ncountries &lt;- full_df |&gt; \n  group_by(region) |&gt; \n  sample_n(2) |&gt; \n  pull(country)\n\nWhat average life expectancy does our very simple model predict for these countries across our time period?\n\naugment(m, newdata = full_df) |&gt; \n  filter(country %in% countries) |&gt; \n  ggplot(aes(x = year)) + \n  geom_point(aes(y = life_exp)) + \n  geom_line(aes(y = .fitted)) + \n  facet_nested_wrap(~ region + country) + \n  theme_minimal()\n\n\n\n\nAs expected, the model predicts the same average life expectancy for every country in every region, despite clear differences between them. For example, countries in North America have a much higher average life expectancy than countries in Sub-Saharan Africa. It would be great if we could account for some of these differences. What if we could customize this starting point (the intercept) for each region? In other words, what if the starting point for our model was the average life expectancy in each region, instead of the global average?\n\nRegion-level intercepts\nLet’s ask our model to adjust its starting point to the average life expectancy in countries within the same region. We can do that by including each country’s region as a variable in our model:\n\nm_region &lt;- lm(life_exp ~ year + region, data = full_df)\n\nmodelsummary(m_region, coef_rename = c(year = \"Year\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n69.191\n\n\n\n(0.238)\n\n\nYear\n0.290\n\n\n\n(0.013)\n\n\nregionEurope & Central Asia\n4.174\n\n\n\n(0.252)\n\n\nregionLatin America & Caribbean\n1.392\n\n\n\n(0.269)\n\n\nregionMiddle East & North Africa\n1.595\n\n\n\n(0.321)\n\n\nregionNorth America\n7.610\n\n\n\n(0.697)\n\n\nregionSouth Asia\n−3.961\n\n\n\n(0.455)\n\n\nregionSub-Saharan Africa\n−13.840\n\n\n\n(0.259)\n\n\nNum.Obs.\n4409\n\n\nR2\n0.643\n\n\nR2 Adj.\n0.643\n\n\nAIC\n27236.2\n\n\nBIC\n27293.7\n\n\nLog.Lik.\n−13609.087\n\n\nF\n1133.649\n\n\nRMSE\n5.30\n\n\n\n\n\n\n\nIn 2000 (our baseline year, where year = 0), the estimated average life expectancy among countries in East Asia and the Pacific (our baseline region) was 69.19 years. All regions other than South Asia and Sub-Saharan Africa had a higher average life expectancy than East Asia and the Pacific in 2000. North America had the highest average life expectancy with 76.8 years.\n\n\n\n\n\n\nNote\n\n\n\nRemember, the intercept represents the predicted average life expectancy when all of our variables are equal to zero. When we have a categorical variable, we hold one category out as a baseline. The coefficients on the other categories are the predicted average change in the outcome relative to this baseline category.\n\n\nLet’s look at how our model performs across our sample of countries:\n\naugment(m_region, newdata = full_df) |&gt; \n  filter(country %in% countries) |&gt; \n  ggplot(aes(x = year)) + \n  geom_point(aes(y = life_exp)) + \n  geom_line(aes(y = .fitted)) + \n  ggh4x::facet_nested_wrap(~ region + country) + \n  theme_minimal()\n\n\n\n\nThis is better! You can see that the starting point for each country looks more realistic than the global average. It reflects the average across the region.\nHowever, we can improve on this. Using this model, each country within the same region shares a starting point, but there can be a lot of variance within a single region. For example, look at the difference between Djibouti and Egypt. Djibouti’s average life expectancy in 2020 is below where Egypt started in 2000.\n\n\nCountry-level intercepts\nLet’s ask our model to adjust the starting point to reflect the average life expectancy for each country in 2000:\n\nm_country &lt;- lm(life_exp ~ year + country, data = full_df)\n\nI won’t print the regression table for obvious reasons…\nLet’s look at how our model performs after this adjustment:\n\naugment(m_country) |&gt; \n  filter(country %in% countries) |&gt; \n  left_join(\n    distinct(full_df, country, region)\n  ) |&gt; \n  ggplot(aes(x = year)) + \n  geom_point(aes(y = life_exp)) + \n  geom_line(aes(y = .fitted)) + \n  ggh4x::facet_nested_wrap(~ region + country) + \n  theme_minimal()\n\n\n\n\nThis is much better! We have a much more accurate model that accounts for what is going on at the country level."
  },
  {
    "objectID": "content/04-panel_data.html#random-effects-a-brief-introduction",
    "href": "content/04-panel_data.html#random-effects-a-brief-introduction",
    "title": "Panel Data",
    "section": "Random effects: a brief introduction",
    "text": "Random effects: a brief introduction\nWhat is going on? To start, we fit this very simple (and familiar) linear regression model:\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1 Year + \\epsilon\n\\]\nYou’ll remember that we assume that the error term (\\(\\epsilon\\)) is normally distributed, centered at zero, and has some standard deviation (\\(\\sigma\\)). Formally, we can write this as:\n\\[\n\\epsilon \\sim \\mathcal{N}(0,\\sigma)\n\\]\nThe standard deviation for our error term in our model above is:\n\nglance(m) |&gt; \n  pull(sigma)\n\n[1] 8.701056\n\n\nSo, we are assuming that the difference between each country’s actual average life expectancy and the average life expectancy our model predicts for that country is a random number of years pulled from this distribution:\n\nerror_m &lt;- tibble(residual = rnorm(1e6, mean = 0, sd = glance(m)$sigma))\n\nggplot(error_m, aes(x = residual)) + \n  stat_halfeye()\n\n\n\n\nAlthough the average of this error is zero, it has a very wide range. Some of those residuals can be as large as 25 years! This captures all of the variance in countries’ average life expectancy not picked up by annual growth rates (our single independent variable: year).\nWhat if we could open up all of that unexplained variance and take a good look at it?\n\nCountry-specific starting points\nLet’s focus on the error around the intercept, \\(\\beta_0\\). As we saw above, there is a lot of variation in that \\(\\beta_0\\). It’s the average life expectancy across all countries in 2000 (our baseline year). Averages are very useful summaries, but they come at a cost of information. Let’s see what we lost:\n\nfull_df |&gt; \n  filter(year == 0) |&gt; \n  ggplot(aes(x = life_exp)) + \n  stat_halfeye() + \n  theme_minimal() + \n  labs(x = \"Average life expectancy (years)\",\n       y = NULL)\n\n\n\n\nWe reduced all of these values to just one summary value: the average (represented by that lonely dot). This is often very useful! Remember, the goal of statistical analysis is to learn general patterns in our data. But when your data are nested (i.e. within countries within regions), we need to be careful not to miss important factors that influence the relationship we are interested in learning about.\nTo illustrate, let’s assume that there are country-specific difference in the relationship between life expectancy and time. That’s why we see such a large difference between, say, Canada’s and Myanmar’s average life expectancies at any given point in time. Both countries’ populations had average life expectancies that grew each year. But they started at different, country-specific spots.\nWhen we do not account for the nested structure of our data, we miss these differences. But that doesn’t mean they don’t exist. It just means that they are hidden from us, sitting in the error term.\n\n\nYour error is too full\nThe error term above accounts for all of the variation in average life expectancy that is not captured by the baseline average life expectancy (captured by the intercept) and average annual growth rate across all countries (captured by the coefficient on year). This includes a whole bunch of country-specific variance (represented below as \\(b_{country}\\)):\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1 Year + (b_{country} + \\epsilon)\n\\]\nNow, we can (and should!) account for that \\(b_{country}\\).\nFor example, imagine that we have the following model for Australia’s average life expectancy over time:\n\\[\nAverage\\ life\\ expectancy_{Australia} = \\beta_0 + \\beta_1 Year + (b_{Australia} + \\epsilon)\n\\]\nThis model suggests that there is a general relationship between any country’s average life expectancy and time. This is appropriate! Generally, all countries’ average life expectancies are increasing over time. This is due to advancements in medicine and other quality of life boosters that transcend borders. However, we do not always observe a country’s average life expectancy perfectly following this relationship. There is some variance. All good and familiar so far.\nPreviously, we have relied on our random error term (\\(\\epsilon\\)) to capture this variance. However, the model written above suggests that there is also a country-specific effect that serves as an additional source of variance from that global relationship. \\(b_{Australia}\\) captures the variance that we observe just because Australia is Australia.\nMore generally, we can model the following:\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1 Year + (b_{country} + \\epsilon)\n\\]\nNow, we can make some assumptions about the shape of that country-specific variance, just as we do for the other random variance. We assume that the country-specific variance from the general pattern is normally distributed, centered at zero, and has some standard deviation (\\(\\tau\\)).\nFormally, we can write this as:\n\\[\nb_{country} \\sim \\mathcal{N}(0,\\tau)\n\\]\nThese variances from the general trend are still random. In fact, the only difference between this model and our more simple (and familiar) linear regression is this country-specific variance (\\(b_{country}\\)). On average, this difference is zero! But often this varies by country and that variance is, on average, \\(\\tau\\).\nAbove we accounted for changes in the starting point for each country. In other words, we adjusted the intercept to reflect each country’s average life expectancy in year 0. Explicitly, we modeled the country-specific error as:\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + b_{0,country} + \\beta_1 Year + \\epsilon\n\\]\n\n\n\n\n\n\nTip\n\n\n\nSkip this if you aren’t comfy with the content so far!\nWe can also model the error term as:\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + \\beta_1 Year + b_{1,country} + \\epsilon\n\\]\nThis will customize the slope of our linear regression to reflect country-specific growth rates. How cool! For example, Ethiopia’s growth rate is much larger than Canada’s. This is how we account for this difference.\nFurther, we can model that country-specific error as follows:\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + b_{0,country} + \\beta_1 Year + b_{1,country} + \\epsilon\n\\]\nNow we have a custom starting point and slope for each country! Multi-level modelling is awesome.\n\n\n\n\nWhy not just include a country variable as we did above?\nTo capture the country-specific starting points of the relationship between average life expectancy and time, we simply added the country as a variable into our model. This approach is good at capturing that different starting point, but falls down in capturing the uncertainty around those starting points.\nThis is because this approach is over-contextualized: it ignores what each country has in common. The multi-level modelling approach introduced above explicitly parses out what countries have in common, and how their unique quirks make them deviate from this common relationship.\nFurther, we often don’t have a lot of information on each observation. Here, we are fitting several regression lines using only 20 data points (one for each year observed for each country). The multi-level modelling approach allows us to build our understanding of the relationship across all available data.\nHowever, the above approach introduces the intuition behind accounting for the structure of your data in a way I find more intuitive than the multi-level approach. You can carry that logic with you through the following explanation."
  },
  {
    "objectID": "content/04-panel_data.html#multi-level-modelling",
    "href": "content/04-panel_data.html#multi-level-modelling",
    "title": "Panel Data",
    "section": "Multi-level modelling",
    "text": "Multi-level modelling\nLet’s get started modelling the general relationship and the country-specific effects. We are going to fit the following model:\n\\[\nAverage\\ life\\ expectancy = \\beta_0 + b_{0, country} + \\beta_1 Year + \\epsilon\n\\]\n\n\n\n\n\n\nTip\n\n\n\nThis is sometimes referred to as a random-intercept model because we create different starting points for each group in our data.\n\n\nIn R, we use the lme4::lme() function:\n\nm_multi &lt;- lmer(life_exp ~ year + (1 | country), data = full_df)\n\nmodelsummary(m_multi, coef_rename = c(year = \"Year\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n67.568\n\n\n\n(0.591)\n\n\nYear\n0.290\n\n\n\n(0.004)\n\n\nSD (Intercept country)\n8.584\n\n\nSD (Observations)\n1.503\n\n\nNum.Obs.\n4409\n\n\nR2 Marg.\n0.039\n\n\nR2 Cond.\n0.971\n\n\nAIC\n17498.1\n\n\nBIC\n17523.6\n\n\nICC\n1.0\n\n\nRMSE\n1.47\n\n\n\n\n\n\n\nIf we look back at the results of the simple model we fit at the start, we will see very similar coefficients to those captured here. The estimated average life expectancy in 2000 across all countries globally was 67.52 years. This estimated average is increasing by an average of 0.29 years annually.\nThe magic is happening a little under the hood in our errors. They are reported in those two new lines: SD (Intercept country) and SD (Observations).\nOur model has separated out the global relationship (not affected by country-specific factors) and the country-specific features.\nThe fixed effects describe the overall relationship within our data. This is similar to your standard linear regression model fit across all observations. In other words, this is the global average and global growth in average life expectancy. We can isolate these components:\n\ntidy(m_multi, effects = \"fixed\")\n\n# A tibble: 2 × 5\n  effect term        estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  (Intercept)   67.6     0.591       114. \n2 fixed  year           0.290   0.00374      77.6\n\n\nThese can be interpreted exactly as we would any other linear regression coefficients.\nAdditionally, we have some very rich information about how this relationship differs between countries stored in the group-level effects (our group here is countries):\n\ntidy(m_multi, effects = \"ran_pars\")\n\n# A tibble: 2 × 4\n  effect   group    term            estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 ran_pars country  sd__(Intercept)     8.58\n2 ran_pars Residual sd__Observation     1.50\n\n\nThis gives us the estimated variance for average life expectancy across countries: 8.58. As we move from country to country, how much does the average life expectancy change on average? 8.58 years!\nThis difference is much larger than changes to average life expectancy associated with our main independent variable: year. In terms of substantive meaning, which country you are looking at matters more than which years you are moving between.\nWe also have an estimate of the remaining unexplained variation in life expectancy: 1.5 years. Together with the country-specific variance, we see that countries tend to randomly deviate from the general pattern by 10.09 years.\nThis is great because we have now captured the country-specific component of our error (changes in average life expectancy not captured by the global average in our baseline year and annual changes). And it’s substantial! It makes up 85% of the total error in our model.\nHow does our model perform?\n\naugment(m_multi) |&gt; \n  filter(country %in% countries) |&gt; \n  left_join(\n    distinct(full_df, country, region)\n  ) |&gt; \n  ggplot(aes(x = year)) + \n  geom_point(aes(y = life_exp)) + \n  geom_line(aes(y = .fitted)) + \n  ggh4x::facet_nested_wrap(~ region + country) + \n  theme_minimal()\n\n\n\n\nLooks good!"
  },
  {
    "objectID": "content/04-panel_data.html#relationship-between-health-and-wealth-1",
    "href": "content/04-panel_data.html#relationship-between-health-and-wealth-1",
    "title": "Panel Data",
    "section": "Relationship between health and wealth",
    "text": "Relationship between health and wealth\nOkay, so we now have a good understanding of how each country’s average life expectancy grows over time. We have accounted for both time and the nested structure of our data.\nBut now we want to explore a more interesting relationship: that between health and wealth. We know from previous classes that a country’s average life expectancy tends to grow with its GDP. How can we formalize that whilst accounting for differences in the relationship over time and between countries?\nFirst, let’s look at the relationship between health and wealth across all countries and years:\n\nggplot(full_df, aes(x = gdp_per_cap, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"GDP (USD)\",\n       y = \"Average life expectancy (years)\")\n\n\n\n\nOkay, that’s certainly a positive and strong relationship but it’s not very linear. Happily, we can transform our variables to expose a more linear relationship:\n\nfull_df &lt;- full_df |&gt; \n  mutate(log_gdp = log(gdp_per_cap))\n\nggplot(full_df, aes(x = log_gdp, y = life_exp)) + \n  geom_point() + \n  theme_minimal() + \n  labs(x = \"GDP (logged USD)\",\n       y = \"Average life expectancy (years)\")\n\n\n\n\nGreat! We can detect a linear and positive relationship lurking in that cloud.\n\n\n\n\n\n\nSuggested reading\n\n\n\nIf you want to learn more about log transformations, please read:\n\nBailey, Michael A. 2020. Real Stats: Using Econometrics for Political Science and Public Policy. Chapter 7.\n\nIf you want to learn more about interpreting linear regression models that use log transformed DVs and/or IVs, please read:\n\nThis UCLA Statistical Methods and Data Analytics FAQ.\nThis StackExchange thread.\n\n\n\nEach point in this cloud represents one country’s logged GDP per capita and average life expectancy in one year. This visualization does not tell us much about that fact. Let’s adjust it:\n\nggplot(full_df, aes(x = log_gdp, y = life_exp, group = country)) + \n  geom_point(alpha = 0.25) + \n  geom_line(alpha = 0.25) + \n  theme_minimal() + \n  labs(x = \"GDP (logged USD)\",\n       y = \"Average life expectancy (years)\")\n\n\n\n\nLet’s isolate our sample of countries:\n\nfull_df |&gt; \n  filter(country %in% countries) |&gt; \n  ggplot(aes(x = log_gdp, y = life_exp)) + \n  geom_line() + \n  geom_point() + \n  ggh4x::facet_nested_wrap(~ region + country) + \n  theme_minimal() + \n  labs(x = \"GDP per capita (logged USD)\",\n       y = \"Average life expectancy (years)\")\n\n\n\n\nOh gosh. Okay, so we now have a better understanding of the structure of our data but it’s still a mess. We can see some fairly linear and positive relationships running around, but they are start and end at various different places. We will need to account for this diversity by fitting a multi-level model with country groupings.\nLet’s start by running a normal linear regression. This will be a our baseline:\n\nm_linear &lt;- lm(life_exp ~ log_gdp + year, data = full_df)\n\nmodelsummary(m_linear, \n             coef_rename = c(log_gdp = \"GDP per capita (US$, logged)\", \n                             year = \"Year\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n31.197\n\n\n\n(0.431)\n\n\nGDP per capita (US$, logged)\n4.568\n\n\n\n(0.051)\n\n\nYear\n0.039\n\n\n\n(0.013)\n\n\nNum.Obs.\n4232\n\n\nR2\n0.670\n\n\nR2 Adj.\n0.670\n\n\nAIC\n25756.4\n\n\nBIC\n25781.8\n\n\nLog.Lik.\n−12874.221\n\n\nRMSE\n5.07\n\n\n\n\n\n\n\nBecause we logged our variable of GDP per capita, we need to take one additional step before we can interpret the coefficient on GDP per capita in a way that is meaningful to humans. The predicted effect on a country’s average life expectancy of a $1,000 increase in its GDP per capita is 31.6 years.\nHow did I get this? First, I get the coefficient for a country’s logged GDP per capita:\n\ncoef_gdp_linear &lt;- tidy(m_linear) |&gt; \n  filter(term == \"log_gdp\") |&gt; \n  pull(estimate)\n\ncoef_gdp_linear\n\n[1] 4.567743\n\n\nI then translate a GDP per capita increase of $1,000 into a logged GDP Per capita increase:\n\nlog(1000)\n\n[1] 6.907755\n\n\nI then find the predicted increase in a country’s life expectancy resulting from a $1,000 (or a 6.908 logged dollar) increase in a country’s GDP per capita, holding the yearly effects constant:\n\ncoef_gdp_linear * log(1000)\n\n[1] 31.55285\n\n\nHolding a country’s GDP per capita constant, this average life expectancy increases by an average of 0.04 years annually.\nOur model estimates these effects across all countries. But we know each country has a slightly different relationship between health and wealth. So, we should model this difference using multilevel modelling. As above, we are going to fit different intercepts for each country.\n\nm_multi &lt;- lmer(life_exp ~ log_gdp + year + (1 | country), \n                data = full_df)\n\nmodelsummary(m_multi, \n             coef_rename = c(log_gdp = \"GDP per capita (US$, logged)\", \n                             year = \"Year\"))\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n56.521\n\n\n\n(0.781)\n\n\nGDP per capita (US$, logged)\n1.376\n\n\n\n(0.077)\n\n\nYear\n0.217\n\n\n\n(0.006)\n\n\nSD (Intercept country)\n6.945\n\n\nSD (Observations)\n1.459\n\n\nNum.Obs.\n4232\n\n\nR2 Marg.\n0.131\n\n\nR2 Cond.\n0.963\n\n\nAIC\n16500.3\n\n\nBIC\n16532.0\n\n\nICC\n1.0\n\n\nRMSE\n1.42\n\n\n\n\n\n\n\nWhat did we find? The model has isolated the global (population) effects. Holding annual growth constant, a country’s average life expectancy increases by an average of 9.5 years for every $1,000 increase in its GDP per capita. Holding that growth in GDP per capita constant, a country’s average life expectancy increases by an average of 0.22 years annually.\nWe also have the country-specific effects. We find that life expectancy varies by 6.945 years between countries. This country-level variance accounts for 83% of the total residual variation of the model. How did I get that?\n\\[\n\\frac{SD\\ (Intercept\\ country)}{Total\\ SD} = \\frac{SD\\ (Intercept\\ country)}{SD\\ (Intercept\\ country) + SD\\ (Observations)} = \\frac{6.945\\ years}{6.945\\ years + 1.459\\ years}\n\\]\nLet’s look at how our model performs against our sample of countries for their GDP per capita:\n\naugment(m_multi) |&gt; \n  filter(country %in% countries) |&gt; \n  left_join(\n    distinct(full_df, country, region)\n  ) |&gt; \n  ggplot(aes(x = log_gdp)) + \n  geom_point(aes(y = life_exp)) + \n  geom_line(aes(y = .fitted), colour = \"red\") + \n  ggh4x::facet_nested_wrap(~ region + country) + \n  theme_minimal() + \n  labs(x = \"Year\", \n       y = \"Average life expectancy (years, logged)\")\n\n\n\n\nAnd over time:\n\naugment(m_multi) |&gt; \n  filter(country %in% countries) |&gt; \n  left_join(\n    distinct(full_df, country, region)\n  ) |&gt; \n  ggplot(aes(x = year)) + \n  geom_point(aes(y = life_exp)) + \n  geom_line(aes(y = .fitted), colour = \"red\") + \n  ggh4x::facet_nested_wrap(~ region + country) + \n  theme_minimal() + \n  labs(x = \"Year\", \n       y = \"Average life expectancy (years, logged)\")\n\n\n\n\nPretty good! Let’s look at what we gained by accounting for these country-level differences:\n\naugment(m_linear) |&gt; \n  select(.rownames, life_exp, log_gdp, year, .fitted_linear = .fitted) |&gt; \n  left_join(\n    augment(m_multi) |&gt;\n      select(.rownames, life_exp, log_gdp, year, .fitted_multi = .fitted)\n  ) |&gt; \n  left_join(\n    full_df |&gt; \n      select(country, region, year) |&gt; \n      rownames_to_column(var = \".rownames\")\n  ) |&gt; \n  filter(country %in% countries) |&gt; \n  ggplot(aes(x = year)) + \n  geom_point(aes(y = life_exp)) + \n  geom_line(aes(y = .fitted_linear), colour = \"blue\") + \n  geom_line(aes(y = .fitted_multi), colour = \"red\") + \n  ggh4x::facet_nested_wrap(~ region + country) + \n  theme_minimal() + \n  labs(x = \"Year\", \n       y = \"Average life expectancy (years)\")\n\n\n\n\nIt looks much better across all countries and years!"
  },
  {
    "objectID": "content/04-panel_data.html#further-reading",
    "href": "content/04-panel_data.html#further-reading",
    "title": "Panel Data",
    "section": "Further reading",
    "text": "Further reading\n\nThis lab session draws heavily on Andrew Heiss’s brilliant blog post A guide to working with country-year panel data and Bayesian multilevel models (2021).\nI also highly recommend Michael Clark’s Mixed Models with R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Advanced Quantitative Methods for Political Science\n        ",
    "section": "",
    "text": "Advanced Quantitative Methods for Political Science\n        \n        \n            An introduction to multivariate analysis.\n        \n        \n            Spring 2024Department of Government and PoliticsUniversity of Maryland, College Park\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\n\nProfessor\n\n   Dr Michael Hanmer\n   mhanmer@umd.edu\n\n\n\nTeaching Assistant\n\n   Harriet Goers\n   Chincoteague Building\n   hgoers@umd.edu\n   hgoers\n\n\n\n\n\nCourse details\n\n   January 24 - May 9\n   Thursday, 9:30 - 12:15 PM\n   Tydings Building, Room 1111\n\n\n\nLab details\n\n   TBD\n   TBD\n\n\n\nOffice hours\n\n   2-3 PM, Mondays\n   Zoom\n\n\n\n\nContacting me\nE-mail is the best ways to get in contact with me. I will try to respond to all course-related e-mails within 24 hours."
  }
]